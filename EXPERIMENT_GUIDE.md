# ğŸš€ HÆ¯á»šNG DáºªN CHáº Y EXPERIMENTS TRÃŠN SERVER

## ğŸ“‹ Tá»•ng quan

Cháº¡y táº¥t cáº£ 60 experiments cho LLMDTA:
- **3 datasets**: davis, kiba, metz
- **4 settings**: warm, novel-drug, novel-prot, novel-pair
- **5 folds**: 0, 1, 2, 3, 4
- **Total**: 60 runs vá»›i `--epochs 200 --batch_size 16`

---

## ğŸ› ï¸ Chuáº©n bá»‹ (Setup trÃªn Server)

### 1. Clone repository vÃ  download data

```bash
# Clone repo
git clone https://github.com/glucose20/Temp.git
cd Temp

# Download dataset tá»« Kaggle (hoáº·c copy tá»« local)
# CÃ i kagglehub náº¿u cáº§n
pip install kagglehub

# Download vÃ  giáº£i nÃ©n data
python -c "
import kagglehub
import shutil
import os

# Download pretrained features
path = kagglehub.dataset_download('christang0002/llmdta')
pretrain_dir = f'{path}/pretrain-feature/pretrained-feature'

# Copy to project
for dataset in ['davis', 'kiba', 'metz']:
    src = os.path.join(pretrain_dir, dataset)
    dst = f'./data/{dataset}'
    if os.path.exists(dst):
        shutil.rmtree(dst)
    shutil.copytree(src, dst)
    print(f'Copied {dataset}')
"

# Giáº£i nÃ©n 5-fold datasets
tar -xzf ./data/dta-5fold-dataset/davis.tar.gz -C ./data/dta-5fold-dataset/
tar -xzf ./data/dta-5fold-dataset/kiba.tar.gz -C ./data/dta-5fold-dataset/
tar -xzf ./data/dta-5fold-dataset/metz.tar.gz -C ./data/dta-5fold-dataset/
```

### 2. CÃ i Ä‘áº·t dependencies

```bash
pip install numpy pandas scipy scikit-learn torch tqdm gensim matplotlib mol2vec fair-esm rdkit
```

### 3. Cáº¥p quyá»n thá»±c thi cho scripts

```bash
chmod +x scripts/*.sh
```

---

## ğŸ¯ CÃ¡ch cháº¡y Experiments

### **Option 1: Cháº¡y tuáº§n tá»± Táº¤T Cáº¢ 60 runs (Sequential)**

**DÃ¹ng khi:** Chá»‰ cÃ³ 1 GPU hoáº·c muá»‘n cháº¡y an toÃ n

```bash
# Cháº¡y táº¥t cáº£
bash scripts/run_all_experiments.sh
```

**Thá»i gian Æ°á»›c tÃ­nh:** ~30-60 giá» (tÃ¹y GPU vÃ  early stopping)

**Æ¯u Ä‘iá»ƒm:**
- âœ… An toÃ n, Ã­t lá»—i
- âœ… Dá»… theo dÃµi log
- âœ… KhÃ´ng tá»‘n nhiá»u RAM

**NhÆ°á»£c Ä‘iá»ƒm:**
- âŒ Ráº¥t cháº­m

---

### **Option 2: Cháº¡y song song vá»›i nhiá»u GPU (Parallel - KHUYáº¾N NGHá»Š)**

**DÃ¹ng khi:** CÃ³ 2-4 GPUs

```bash
# Chá»‰nh sá»­a file trÆ°á»›c (dÃ²ng 12-13)
nano scripts/run_all_experiments_parallel.sh

# Sá»­a:
NUM_GPUS=4
GPU_DEVICES=(0 1 2 3)  # IDs cá»§a GPUs báº¡n cÃ³

# Cháº¡y
bash scripts/run_all_experiments_parallel.sh
```

**Thá»i gian Æ°á»›c tÃ­nh:** 
- 2 GPUs: ~15-30 giá»
- 4 GPUs: ~8-15 giá»

**Æ¯u Ä‘iá»ƒm:**
- âœ… Nhanh gáº¥p N láº§n (N = sá»‘ GPU)
- âœ… Táº­n dá»¥ng tá»‘i Ä‘a hardware

**NhÆ°á»£c Ä‘iá»ƒm:**
- âŒ Tá»‘n nhiá»u RAM (má»—i process load embeddings riÃªng)
- âŒ KhÃ³ debug náº¿u cÃ³ lá»—i

---

### **Option 3: Cháº¡y tá»«ng dataset riÃªng láº»**

**DÃ¹ng khi:** Muá»‘n cháº¡y tá»«ng dataset má»™t, hoáº·c test trÆ°á»›c

```bash
# Cháº¡y DAVIS (20 runs: 4 settings Ã— 5 folds)
bash scripts/run_single_dataset.sh davis

# Cháº¡y KIBA
bash scripts/run_single_dataset.sh kiba

# Cháº¡y METZ
bash scripts/run_single_dataset.sh metz
```

**Thá»i gian má»—i dataset:** ~10-20 giá»

---

### **Option 4: Cháº¡y thá»§ cÃ´ng tá»«ng run (Debug)**

```bash
# Test nhanh vá»›i 1 epoch
python code/train.py --fold 0 --cuda "0" --dataset davis --running_set warm --epochs 1 --batch_size 16

# Cháº¡y tháº­t 1 run
python code/train.py --fold 0 --cuda "0" --dataset davis --running_set novel-pair --epochs 200 --batch_size 16
```

---

## ğŸ“Š Theo dÃµi tiáº¿n Ä‘á»™

### **Xem log real-time**

```bash
# Xem master log
tail -f ./results/experiment_master_log_*.txt

# Xem log cá»§a 1 run cá»¥ thá»ƒ
tail -f ./log/*davis*novel-pair*fold0*.log
```

### **Kiá»ƒm tra GPU usage**

```bash
# Xem real-time
watch -n 1 nvidia-smi

# Hoáº·c
nvidia-smi -l 1
```

### **Äáº¿m sá»‘ runs Ä‘Ã£ hoÃ n thÃ nh**

```bash
# Äáº¿m model files
ls -1 ./savemodel/*.pth | wc -l

# Äáº¿m test result files
ls -1 ./log/Test-*-fold*.csv | wc -l
```

---

## ğŸ“ˆ Tá»•ng há»£p káº¿t quáº£

### **Tá»•ng há»£p tá»«ng dataset-setting**

```bash
# Tá»± Ä‘á»™ng cháº¡y sau má»—i setting (náº¿u dÃ¹ng script)
# Hoáº·c cháº¡y thá»§ cÃ´ng:
python code/aggregate_results.py --dataset davis --running_set warm
python code/aggregate_results.py --dataset davis --running_set novel-pair
# ... (lÃ m tÆ°Æ¡ng tá»± cho táº¥t cáº£)
```

### **Táº¡o bÃ¡o cÃ¡o tá»•ng há»£p cuá»‘i cÃ¹ng**

```bash
python code/generate_final_report.py
```

**Output:**
- `./log/FINAL_SUMMARY_REPORT_<timestamp>.csv`
- Console output vá»›i báº£ng so sÃ¡nh vÃ  best results

---

## ğŸ”§ Cáº¥u hÃ¬nh nÃ¢ng cao

### **Thay Ä‘á»•i GPU trong script**

```bash
# Edit script
nano scripts/run_all_experiments.sh

# DÃ²ng 9: Thay Ä‘á»•i GPU
CUDA_DEVICE="1"  # Chuyá»ƒn sang GPU 1
```

### **Thay Ä‘á»•i hyperparameters**

```bash
# Edit script
nano scripts/run_all_experiments.sh

# DÃ²ng 7-8: Thay Ä‘á»•i
EPOCHS=100       # Giáº£m xuá»‘ng 100 epochs
BATCH_SIZE=32    # TÄƒng batch size
```

### **Cháº¡y subset cá»§a experiments**

```bash
# Edit script Ä‘á»ƒ chá»‰ cháº¡y má»™t vÃ i settings
nano scripts/run_all_experiments.sh

# DÃ²ng 13: Bá» bá»›t settings
SETTINGS=("warm" "novel-pair")  # Chá»‰ cháº¡y 2 settings thay vÃ¬ 4
```

---

## ğŸš¨ Xá»­ lÃ½ lá»—i

### **Náº¿u má»™t run bá»‹ lá»—i:**

```bash
# Xem log chi tiáº¿t
cat ./results/run_davis_warm_fold0.log

# Cháº¡y láº¡i run Ä‘Ã³
python code/train.py --fold 0 --cuda "0" --dataset davis --running_set warm --epochs 200 --batch_size 16

# Tá»•ng há»£p láº¡i results
python code/aggregate_results.py --dataset davis --running_set warm
```

### **Náº¿u háº¿t VRAM (Out of Memory):**

```bash
# Giáº£m batch size
python code/train.py --fold 0 --cuda "0" --dataset davis --running_set warm --epochs 200 --batch_size 8
```

### **Náº¿u server bá»‹ disconnect:**

DÃ¹ng `tmux` hoáº·c `screen` Ä‘á»ƒ cháº¡y background:

```bash
# Sá»­ dá»¥ng tmux (khuyáº¿n nghá»‹)
tmux new -s llmdta
bash scripts/run_all_experiments.sh

# Detach: Ctrl+B, D
# Reattach: tmux attach -t llmdta

# Hoáº·c dÃ¹ng nohup
nohup bash scripts/run_all_experiments.sh > experiment.log 2>&1 &

# Xem tiáº¿n Ä‘á»™
tail -f experiment.log
```

---

## ğŸ“ Cáº¥u trÃºc output

Sau khi cháº¡y xong:

```
log/
â”œâ”€â”€ experiment_master_log_<timestamp>.txt          # Master log
â”œâ”€â”€ Nov12_10-30-45-davis-warm-fold0.csv           # Training curves
â”œâ”€â”€ Test-davis-warm-fold0-Nov12_10-30-45.csv      # Individual fold results
â”œâ”€â”€ Test-davis-warm-AGGREGATED.csv                 # Aggregated 5 folds
â”œâ”€â”€ Test-davis-warm-SUMMARY.csv                    # Statistics
â”œâ”€â”€ ... (tÆ°Æ¡ng tá»± cho 12 combinations)
â””â”€â”€ FINAL_SUMMARY_REPORT_<timestamp>.csv           # Final report

savemodel/
â”œâ”€â”€ davis-warm-fold0-Nov12_10-30-45.pth
â”œâ”€â”€ davis-warm-fold1-Nov12_10-35-12.pth
â””â”€â”€ ... (60 model files total)

results/
â”œâ”€â”€ experiment_master_log_<timestamp>.txt
â”œâ”€â”€ run_davis_warm_fold0.log
â””â”€â”€ ... (60 individual run logs)
```

---

## ğŸ“Š Káº¿t quáº£ máº«u

```
============================================================
LLMDTA - Final Experiment Summary Report
============================================================

DAVIS - warm
------------------------------------------------------------
  mse       : 0.421000 Â± 0.009154
  rmse      : 0.649000 Â± 0.007280
  ci        : 0.856000 Â± 0.004658
  r2        : 0.712000 Â± 0.007958
  pearson   : 0.844000 Â± 0.005070
  spearman  : 0.838000 Â± 0.005263

[... tÆ°Æ¡ng tá»± cho 11 combinations khÃ¡c ...]

Comparison Table (Mean MSE)
============================================================
setting     warm  novel-drug  novel-prot  novel-pair
dataset                                              
davis      0.421       0.512       0.634       0.789
kiba       0.345       0.445       0.556       0.667
metz       0.398       0.498       0.598       0.698

Best Results by Metric
============================================================
Best MSE: kiba - warm = 0.345000
Best CI:  davis - warm = 0.856000
Best RÂ²:  davis - warm = 0.712000
```

---

## â±ï¸ Thá»i gian Æ°á»›c tÃ­nh

| Method | GPUs | Time per run | Total time |
|--------|------|--------------|------------|
| Sequential | 1 | ~30-60 min | ~30-60 hours |
| Parallel (2 GPUs) | 2 | ~30-60 min | ~15-30 hours |
| Parallel (4 GPUs) | 4 | ~30-60 min | ~8-15 hours |

*Thá»i gian thá»±c táº¿ tÃ¹y thuá»™c vÃ o GPU, early stopping, vÃ  data size*

---

## ğŸ’¡ Tips

1. **Test trÆ°á»›c vá»›i 1-2 epochs**: 
   ```bash
   python code/train.py --fold 0 --cuda "0" --dataset davis --running_set warm --epochs 2
   ```

2. **Cháº¡y overnight vá»›i tmux**: Server cÃ³ thá»ƒ disconnect nhÆ°ng job váº«n cháº¡y

3. **Backup Ä‘á»‹nh ká»³**: Copy `./log/` vÃ  `./savemodel/` vá» local

4. **Monitor GPU**: Äáº£m báº£o GPU utilization ~80-100%

5. **Check disk space**: Má»—i model ~100MB, tá»•ng cá»™ng ~6GB

---

## ğŸ“ Troubleshooting

| Lá»—i | Giáº£i phÃ¡p |
|-----|-----------|
| `CUDA out of memory` | Giáº£m `--batch_size` xuá»‘ng 8 hoáº·c 4 |
| `File not found` | Kiá»ƒm tra Ä‘Æ°á»ng dáº«n data, cháº¡y setup láº¡i |
| `Permission denied` | `chmod +x scripts/*.sh` |
| Script dá»«ng giá»¯a chá»«ng | DÃ¹ng `tmux` hoáº·c `nohup` |
| Káº¿t quáº£ khÃ´ng aggregated | Cháº¡y `python code/aggregate_results.py` thá»§ cÃ´ng |

---

**Happy Training! ğŸš€**
