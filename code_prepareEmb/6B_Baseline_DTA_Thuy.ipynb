{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RW-AHAcvJ8fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdb30961-abc6-4765-cabb-9cbba12cb802"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m133.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.7/302.7 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.2/14.2 MB\u001b[0m \u001b[31m138.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m115.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.2/36.2 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -q numpy pandas scipy scikit-learn torch tqdm gensim matplotlib mol2vec esm\n",
        "%pip install -q rdkit psutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"from esm.models.esmc import ESMC; print('ESM-C installed successfully!')\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWpb5s4tTTbG",
        "outputId": "d65061e7-f56f-47d6-e087-eaecc46b6741"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ESM-C installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SuLUkGJhKA9j",
        "outputId": "52596200-ca59-476d-c412-2bb9ee26f65f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Temp'...\n",
            "remote: Enumerating objects: 223, done.\u001b[K\n",
            "remote: Counting objects: 100% (178/178), done.\u001b[K\n",
            "remote: Compressing objects: 100% (119/119), done.\u001b[K\n",
            "remote: Total 223 (delta 76), reused 154 (delta 58), pack-reused 45 (from 2)\u001b[K\n",
            "Receiving objects: 100% (223/223), 94.48 MiB | 22.31 MiB/s, done.\n",
            "Resolving deltas: 100% (77/77), done.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import subprocess\n",
        "\n",
        "!git clone --branch new-prot-embs https://github.com/glucose20/Temp.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU-YGrUTLM-U",
        "outputId": "b731cfe9-9d6a-4279-cca4-02f73b4f3c1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'llmdta' dataset.\n",
            "davis  kiba  metz\n"
          ]
        }
      ],
      "source": [
        "# Import library\n",
        "import kagglehub\n",
        "\n",
        "# Download dataset from Kaggle (LLMDTA dataset)\n",
        "path = kagglehub.dataset_download(\"christang0002/llmdta\")\n",
        "\n",
        "!ls {path}/pretrain-feature/pretrained-feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2Xj3dixhU3R",
        "outputId": "572fbcd8-2530-4057-9267-5402ba965223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied /kaggle/input/llmdta/pretrain-feature/pretrained-feature/metz to /content/Temp/data\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "source_dir1 = f'{path}/pretrain-feature/pretrained-feature/davis'\n",
        "source_dir2 = f'{path}/pretrain-feature/pretrained-feature/kiba'\n",
        "source_dir3 = f'{path}/pretrain-feature/pretrained-feature/metz'\n",
        "destination_dir = '/content/Temp/data'\n",
        "\n",
        "# Remove destination directories if they exist\n",
        "if os.path.exists(os.path.join(destination_dir, os.path.basename(source_dir1))):\n",
        "    shutil.rmtree(os.path.join(destination_dir, os.path.basename(source_dir1)))\n",
        "if os.path.exists(os.path.join(destination_dir, os.path.basename(source_dir2))):\n",
        "    shutil.rmtree(os.path.join(destination_dir, os.path.basename(source_dir2)))\n",
        "if os.path.exists(os.path.join(destination_dir, os.path.basename(source_dir3))):\n",
        "    shutil.rmtree(os.path.join(destination_dir, os.path.basename(source_dir3)))\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "# Copy the directory\n",
        "shutil.copytree(source_dir1, os.path.join(destination_dir, os.path.basename(source_dir1)))\n",
        "shutil.copytree(source_dir2, os.path.join(destination_dir, os.path.basename(source_dir2)))\n",
        "shutil.copytree(source_dir3, os.path.join(destination_dir, os.path.basename(source_dir3)))\n",
        "\n",
        "print(f\"Copied {source_dir3} to {destination_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YSjTs-VJmis5"
      },
      "outputs": [],
      "source": [
        "!tar -xzf /content/Temp/data/dta-5fold-dataset/davis.tar.gz -C /content/Temp/data/dta-5fold-dataset/\n",
        "!tar -xzf /content/Temp/data/dta-5fold-dataset/kiba.tar.gz -C /content/Temp/data/dta-5fold-dataset/\n",
        "!tar -xzf /content/Temp/data/dta-5fold-dataset/metz.tar.gz -C /content/Temp/data/dta-5fold-dataset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4rTa8MnoGca",
        "outputId": "86ae6ab4-c717-4838-e51f-ae91d113b9de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Temp\n"
          ]
        }
      ],
      "source": [
        "%cd /content/Temp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "\n",
        "# ESM-C imports\n",
        "from esm.models.esmc import ESMC\n",
        "from esm.sdk.api import ESMProtein, LogitsConfig"
      ],
      "metadata": {
        "id": "bbar-pZYju8U"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select ESM-C model variant\n",
        "MODEL_NAME = \"esmc_6b\"  # Options: esmc_300m, esmc_600m, esmc_6b (via Forge API)\n",
        "USE_FORGE_API = True  # Set True for esmc_6b with Forge token\n",
        "\n",
        "# Forge API configuration (only needed for esmc_6b)\n",
        "FORGE_TOKEN = \"7GPrbbYRP3JXI1JXYz1z2t\"  # Get from https://forge.evolutionaryscale.ai\n",
        "\n",
        "# Device configuration (for local models only)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Model dimensions\n",
        "MODEL_DIMS = {\n",
        "    \"esmc_300m\": 960,\n",
        "    \"esmc_600m\": 1152,\n",
        "    \"esmc_6b\": 2560\n",
        "}\n",
        "EMBEDDING_DIM = MODEL_DIMS[MODEL_NAME]\n",
        "print(f\"Model: {MODEL_NAME}, Embedding dimension: {EMBEDDING_DIM}\")\n",
        "print(f\"Using Forge API: {USE_FORGE_API}\")"
      ],
      "metadata": {
        "id": "m5H_g7mtj8JE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55fd6502-d061-4c77-d971-cad57c8e57cb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Model: esmc_6b, Embedding dimension: 2560\n",
            "Using Forge API: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "if USE_FORGE_API:\n",
        "    print(f\"Connecting to ESM Forge API for {MODEL_NAME}...\")\n",
        "    from esm.sdk.forge import ESM3ForgeInferenceClient\n",
        "\n",
        "    model = ESM3ForgeInferenceClient(\n",
        "        model=\"esmc-6b-2024-12\",\n",
        "        url=\"https://forge.evolutionaryscale.ai\",\n",
        "        token=FORGE_TOKEN\n",
        "    )\n",
        "    print(\"Connected to Forge API successfully!\")\n",
        "else:\n",
        "    print(f\"Loading local model: {MODEL_NAME}...\")\n",
        "    model = ESMC.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "    model.eval()  # Set to evaluation mode\n",
        "    print(\"Model loaded successfully!\")"
      ],
      "metadata": {
        "id": "HhqihH_BkLZE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd9d592d-9f92-4477-c820-11cedec9d4eb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to ESM Forge API for esmc_6b...\n",
            "Connected to Forge API successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_esmc_pretrain(model, df_dir, db_name, sep=' ', header=None,\n",
        "                      col_names=['drug_id', 'prot_id', 'drug_smile', 'prot_seq', 'label'],\n",
        "                      use_forge_api=False, device='cuda'):\n",
        "    \"\"\"\n",
        "    Extract ESM-C embeddings for protein sequences.\n",
        "\n",
        "    Args:\n",
        "        model: Loaded ESM-C model or Forge API client\n",
        "        df_dir: Path to input CSV file\n",
        "        db_name: Database name for output filename\n",
        "        sep: CSV separator\n",
        "        header: CSV header row (None for no header)\n",
        "        col_names: Column names for the dataframe\n",
        "        use_forge_api: Whether using Forge API for esmc_6b\n",
        "        device: Device for computation ('cuda' or 'cpu')\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing embeddings\n",
        "    \"\"\"\n",
        "    # Load data\n",
        "    df = pd.read_csv(df_dir, sep=sep, header=header)\n",
        "    df.columns = col_names\n",
        "    df.drop_duplicates(subset='prot_id', inplace=True)\n",
        "\n",
        "    prot_ids = df['prot_id'].tolist()\n",
        "    prot_seqs = df['prot_seq'].tolist()\n",
        "\n",
        "    emb_dict = {}\n",
        "    emb_mat_dict = {}\n",
        "    length_dict = {}\n",
        "\n",
        "    print(f\"Processing {len(prot_ids)} proteins on {device}...\")\n",
        "\n",
        "    if use_forge_api:\n",
        "        # Use Forge Batch Executor for efficient API calls\n",
        "        from esm.sdk import batch_executor\n",
        "\n",
        "        def embed_sequence(client, sequence, prot_id):\n",
        "            \"\"\"Helper function for batch processing\"\"\"\n",
        "            try:\n",
        "                seq = sequence[:2048]\n",
        "                protein = ESMProtein(sequence=seq)\n",
        "                protein_tensor = client.encode(protein)\n",
        "\n",
        "                from esm.sdk.api import ESMProteinError\n",
        "                if isinstance(protein_tensor, ESMProteinError):\n",
        "                    raise protein_tensor\n",
        "\n",
        "                output = client.logits(\n",
        "                    protein_tensor,\n",
        "                    LogitsConfig(sequence=True, return_embeddings=True)\n",
        "                )\n",
        "\n",
        "                # CRITICAL: Convert BFloat16 immediately after receiving from API\n",
        "                embeddings = output.embeddings\n",
        "\n",
        "                # Convert to torch tensor first if needed, then to float32\n",
        "                if not isinstance(embeddings, torch.Tensor):\n",
        "                    embeddings = torch.tensor(embeddings)\n",
        "\n",
        "                # Convert BFloat16 to Float32\n",
        "                if embeddings.dtype == torch.bfloat16:\n",
        "                    embeddings = embeddings.to(torch.float32)\n",
        "\n",
        "                # Now convert to numpy (Forge API runs on server, so CPU here is fine)\n",
        "                if embeddings.is_cuda:\n",
        "                    embeddings = embeddings.cpu()\n",
        "                embeddings = embeddings.numpy()\n",
        "\n",
        "                return (prot_id, embeddings, len(seq))\n",
        "            except Exception as e:\n",
        "                import traceback\n",
        "                print(f\"\\nError processing {prot_id}: {e}\")\n",
        "                print(traceback.format_exc())\n",
        "                return (prot_id, None, len(sequence[:2048]))\n",
        "\n",
        "        # Process in batches using Forge executor\n",
        "        print(\"Using Forge Batch Executor (server-side GPU processing)...\")\n",
        "        with batch_executor() as executor:\n",
        "            # Prepare data for batch execution\n",
        "            batch_data = [\n",
        "                {'client': model, 'sequence': seq, 'prot_id': pid}\n",
        "                for pid, seq in zip(prot_ids, prot_seqs)\n",
        "            ]\n",
        "\n",
        "            outputs = executor.execute_batch(\n",
        "                user_func=embed_sequence,\n",
        "                **{k: [d[k] for d in batch_data] for k in batch_data[0].keys()}\n",
        "            )\n",
        "\n",
        "        # Process outputs\n",
        "        for prot_id, embeddings, seq_len in outputs:\n",
        "            length_dict[prot_id] = seq_len\n",
        "\n",
        "            if embeddings is not None:\n",
        "                # Ensure float32 dtype\n",
        "                if embeddings.dtype != np.float32:\n",
        "                    embeddings = embeddings.astype(np.float32)\n",
        "\n",
        "                # Store mean embedding as sequence representation\n",
        "                emb_dict[prot_id] = embeddings.mean(axis=0)\n",
        "                emb_mat_dict[prot_id] = embeddings\n",
        "            else:\n",
        "                # Fallback to zeros\n",
        "                emb_dict[prot_id] = np.zeros(EMBEDDING_DIM, dtype=np.float32)\n",
        "                emb_mat_dict[prot_id] = np.zeros((seq_len, EMBEDDING_DIM), dtype=np.float32)\n",
        "\n",
        "    else:\n",
        "        # Local model processing - USE GPU!\n",
        "        print(f\"Using local GPU: {device}\")\n",
        "        for idx in tqdm(range(len(prot_ids))):\n",
        "            prot_id = str(prot_ids[idx])\n",
        "            seq = prot_seqs[idx][:2048]\n",
        "            length_dict[prot_id] = len(seq)\n",
        "\n",
        "            try:\n",
        "                # Create ESMProtein object\n",
        "                protein = ESMProtein(sequence=seq)\n",
        "\n",
        "                # Encode the protein (stays on GPU)\n",
        "                protein_tensor = model.encode(protein)\n",
        "\n",
        "                # Extract embeddings (computed on GPU)\n",
        "                with torch.no_grad():\n",
        "                    logits_output = model.logits(\n",
        "                        protein_tensor,\n",
        "                        LogitsConfig(return_embeddings=True)\n",
        "                    )\n",
        "\n",
        "                embeddings = logits_output.embeddings\n",
        "\n",
        "                # Convert BFloat16 to Float32 if needed (still on GPU)\n",
        "                if embeddings.dtype == torch.bfloat16:\n",
        "                    embeddings = embeddings.to(torch.float32)\n",
        "\n",
        "                # Only move to CPU at the last step before numpy conversion\n",
        "                embeddings = embeddings.cpu().numpy()\n",
        "\n",
        "                # Store mean embedding as sequence representation\n",
        "                emb_dict[prot_id] = embeddings.mean(axis=0)  # Shape: (d_model,)\n",
        "\n",
        "                # Store full embedding matrix\n",
        "                emb_mat_dict[prot_id] = embeddings  # Shape: (seq_len, d_model)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nError processing {prot_id}: {e}\")\n",
        "                # Use zero embeddings as fallback\n",
        "                emb_dict[prot_id] = np.zeros(EMBEDDING_DIM, dtype=np.float32)\n",
        "                emb_mat_dict[prot_id] = np.zeros((len(seq), EMBEDDING_DIM), dtype=np.float32)\n",
        "\n",
        "    # Prepare output dictionary\n",
        "    dump_data = {\n",
        "        \"dataset\": db_name,\n",
        "        \"vec_dict\": emb_dict,\n",
        "        \"mat_dict\": emb_mat_dict,\n",
        "        \"length_dict\": length_dict,\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"embedding_dim\": EMBEDDING_DIM,\n",
        "        \"use_forge_api\": use_forge_api\n",
        "    }\n",
        "\n",
        "    # Save to pickle\n",
        "    output_file = f'./{db_name}_esmc_pretrain.pkl'\n",
        "    with open(output_file, 'wb') as f:\n",
        "        pickle.dump(dump_data, f)\n",
        "\n",
        "    print(f\"\\nSaved embeddings to: {output_file}\")\n",
        "    print(f\"Total proteins: {len(emb_dict)}\")\n",
        "    print(f\"Embedding dimension: {EMBEDDING_DIM}\")\n",
        "\n",
        "    return dump_data\n"
      ],
      "metadata": {
        "id": "RdoPeaoPkSkj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Process simple-Case study data\n",
        "db_name = 'simple_case'\n",
        "df_dir = r'/content/Temp/data/simple-Case/proteins.csv'\n",
        "col_names = ['prot_id', 'prot_seq']\n",
        "\n",
        "# Run the extraction (passes device to function)\n",
        "result = get_esmc_pretrain(\n",
        "    model=model,\n",
        "    df_dir=df_dir,\n",
        "    db_name=db_name,\n",
        "    sep=',',\n",
        "    header=0,\n",
        "    col_names=col_names,\n",
        "    use_forge_api=USE_FORGE_API,\n",
        "    device=DEVICE  # Pass device config\n",
        ")\n"
      ],
      "metadata": {
        "id": "CR9xB5h8TySv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "746a9021-2d77-4819-e8d4-c63d3850ed63"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 4 proteins on cuda...\n",
            "Using Forge Batch Executor (server-side GPU processing)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing  100%|████████████████████████| 4/4 [Elapsed: 00:56 | Remaining: 00:00] , Success=4 Fail=0 Retry=0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved embeddings to: ./simple_case_esmc_pretrain.pkl\n",
            "Total proteins: 4\n",
            "Embedding dimension: 2560\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset configurations\n",
        "datasets = {\n",
        "    # 'davis': './data/dta-5fold-dataset/davis/davis_prots.csv',\n",
        "    'kiba': './data/dta-5fold-dataset/kiba/kiba_prots.csv',\n",
        "    # 'metz': './data/dta-5fold-dataset/metz/metz_prots.csv'\n",
        "}\n",
        "\n",
        "col_names = ['prot_id', 'prot_seq']\n",
        "\n",
        "# Process each dataset\n",
        "for db_name, df_dir in datasets.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Processing {db_name.upper()} dataset\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    try:\n",
        "        result = get_esmc_pretrain(\n",
        "            model=model,\n",
        "            df_dir=df_dir,\n",
        "            db_name=db_name,\n",
        "            sep=',',  # Adjust if needed\n",
        "            header=0,\n",
        "            col_names=col_names,\n",
        "            use_forge_api=USE_FORGE_API,\n",
        "            device=DEVICE  # Pass device config\n",
        "        )\n",
        "        print(f\"✓ Successfully processed {db_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error processing {db_name}: {e}\")\n"
      ],
      "metadata": {
        "id": "D4kpHSy-T9wV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cb97238-f840-4b91-9b41-91fb78c52505"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Processing KIBA dataset\n",
            "============================================================\n",
            "Processing 229 proteins on cuda...\n",
            "Using Forge Batch Executor (server-side GPU processing)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing  100%|████████████████████████| 229/229 [Elapsed: 02:50 | Remaining: 00:00] , Success=229 Fail=0 Retry=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved embeddings to: ./kiba_esmc_pretrain.pkl\n",
            "Total proteins: 229\n",
            "Embedding dimension: 2560\n",
            "✓ Successfully processed kiba\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and inspect generated embeddings\n",
        "def verify_embeddings(db_name):\n",
        "    file_path = f'./{db_name}_esmc_pretrain.pkl'\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Verification: {db_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Dataset: {data['dataset']}\")\n",
        "        print(f\"Model: {data['model']}\")\n",
        "        print(f\"Embedding dimension: {data['embedding_dim']}\")\n",
        "        print(f\"Number of proteins: {len(data['vec_dict'])}\")\n",
        "\n",
        "        # Check first protein\n",
        "        first_prot_id = list(data['vec_dict'].keys())[0]\n",
        "        first_vec = data['vec_dict'][first_prot_id]\n",
        "        first_mat = data['mat_dict'][first_prot_id]\n",
        "\n",
        "        print(f\"\\nFirst protein: {first_prot_id}\")\n",
        "        print(f\"  - Vec shape: {first_vec.shape}\")\n",
        "        print(f\"  - Mat shape: {first_mat.shape}\")\n",
        "        print(f\"  - Sequence length: {data['length_dict'][first_prot_id]}\")\n",
        "\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"✗ File not found: {file_path}\")\n",
        "        return None\n",
        "\n",
        "# Verify each dataset\n",
        "for db_name in ['davis', 'kiba', 'metz']:\n",
        "    verify_embeddings(db_name)"
      ],
      "metadata": {
        "id": "3Gb9EHeBXRyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "file_names = ['davis_esmc_pretrain.pkl', 'kiba_esmc_pretrain.pkl', 'metz_esmc_pretrain.pkl', 'case_study_esmc_pretrain.pkl']\n",
        "current_dir = os.getcwd()\n",
        "\n",
        "print(\"Địa chỉ cụ thể của các file _esmc_pretrain.pkl:\")\n",
        "for file_name in file_names:\n",
        "    full_path = os.path.join(current_dir, file_name)\n",
        "    if os.path.exists(full_path):\n",
        "        print(f\"  - File '{file_name}' TỒN TẠI tại: {full_path}\")\n",
        "    else:\n",
        "        print(f\"  - File '{file_name}' KHÔNG TỒN TẠI tại: {full_path}\")"
      ],
      "metadata": {
        "id": "JdkKFMqGaHWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c76a917e-3990-410a-b302-31122a222858",
        "outputId": "9a06f9a1-7291-498a-8256-2de5ab212b02"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "file_suffixes = ['davis_esmc_pretrain.pkl', 'kiba_esmc_6b_pretrain.pkl', 'metz_esmc_pretrain.pkl', 'case_study_esmc_pretrain.pkl']\n",
        "base_source_dir = '/content/Temp'\n",
        "base_dest_dir = '/content/Temp/data'\n",
        "\n",
        "print(\"Di chuyển các file _esmc_pretrain.pkl:\")\n",
        "\n",
        "for file_suffix in file_suffixes:\n",
        "    dataset_name = file_suffix.split('_')[0] # e.g., 'davis' from 'davis_esmc_pretrain.pkl'\n",
        "    source_path = os.path.join(base_source_dir, file_suffix)\n",
        "\n",
        "    # Construct destination directory: /content/Temp/data/davis/\n",
        "    destination_dataset_dir = os.path.join(base_dest_dir, dataset_name)\n",
        "    os.makedirs(destination_dataset_dir, exist_ok=True)\n",
        "\n",
        "    # Construct destination file name: /content/Temp/data/davis/davis_esm_pretrain.pkl\n",
        "    # Note: The user requested _esm_pretrain.pkl, but files are _esmc_pretrain.pkl.\n",
        "    # Keeping _esmc_pretrain.pkl for consistency with generated files.\n",
        "    destination_file_name = f\"{dataset_name}_esmc_pretrain.pkl\"\n",
        "    destination_path = os.path.join(destination_dataset_dir, destination_file_name)\n",
        "\n",
        "    if os.path.exists(source_path):\n",
        "        try:\n",
        "            shutil.move(source_path, destination_path)\n",
        "            print(f\"  - Đã di chuyển '{file_suffix}' tới '{destination_path}'\")\n",
        "        except Exception as e:\n",
        "            print(f\"  - Lỗi khi di chuyển '{file_suffix}': {e}\")\n",
        "    else:\n",
        "        print(f\"  - File '{file_suffix}' KHÔNG TỒN TẠI tại '{source_path}', bỏ qua.\")\n",
        "\n",
        "print(\"Hoàn tất việc di chuyển file.\")\n",
        "\n",
        "# Verify new locations\n",
        "print(\"\\nKiểm tra lại địa chỉ mới của các file:\")\n",
        "for file_suffix in file_suffixes:\n",
        "    dataset_name = file_suffix.split('_')[0]\n",
        "    destination_file_name = f\"{dataset_name}_esmc_pretrain.pkl\"\n",
        "    destination_path = os.path.join(base_dest_dir, dataset_name, destination_file_name)\n",
        "    if os.path.exists(destination_path):\n",
        "        print(f\"  - File '{destination_file_name}' TỒN TẠI tại: {destination_path}\")\n",
        "    else:\n",
        "        print(f\"  - File '{destination_file_name}' KHÔNG TỒN TẠI tại: {destination_path}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Di chuyển các file _esmc_pretrain.pkl:\n",
            "  - File 'davis_esmc_pretrain.pkl' KHÔNG TỒN TẠI tại '/content/Temp/davis_esmc_pretrain.pkl', bỏ qua.\n",
            "  - Đã di chuyển 'kiba_esmc_6b_pretrain.pkl' tới '/content/Temp/data/kiba/kiba_esmc_pretrain.pkl'\n",
            "  - File 'metz_esmc_pretrain.pkl' KHÔNG TỒN TẠI tại '/content/Temp/metz_esmc_pretrain.pkl', bỏ qua.\n",
            "  - File 'case_study_esmc_pretrain.pkl' KHÔNG TỒN TẠI tại '/content/Temp/case_study_esmc_pretrain.pkl', bỏ qua.\n",
            "Hoàn tất việc di chuyển file.\n",
            "\n",
            "Kiểm tra lại địa chỉ mới của các file:\n",
            "  - File 'davis_esmc_pretrain.pkl' KHÔNG TỒN TẠI tại: /content/Temp/data/davis/davis_esmc_pretrain.pkl\n",
            "  - File 'kiba_esmc_pretrain.pkl' TỒN TẠI tại: /content/Temp/data/kiba/kiba_esmc_pretrain.pkl\n",
            "  - File 'metz_esmc_pretrain.pkl' KHÔNG TỒN TẠI tại: /content/Temp/data/metz/metz_esmc_pretrain.pkl\n",
            "  - File 'case_esmc_pretrain.pkl' KHÔNG TỒN TẠI tại: /content/Temp/data/case/case_esmc_pretrain.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ac906f8",
        "outputId": "5340ebd0-da5c-4276-fe3a-d773b4f7bab9"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Temp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python code/train.py --fold 0 --cuda \"0\" --dataset davis --running_set warm --epochs 1 --batch_size 256"
      ],
      "metadata": {
        "id": "aWyncDouX_Vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python regenerate_esmc_embeddings.py --dataset davis --model esmc_300m"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrEUIfNPordt",
        "outputId": "5545f4fb-685f-42b3-fd6a-bf7fae6865e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Regenerating ESM-C embeddings for DAVIS\n",
            "Model: esmc_300m (dim=960)\n",
            "Device: cuda\n",
            "============================================================\n",
            "\n",
            "Loading esmc_300m...\n",
            "Fetching 4 files: 100% 4/4 [00:00<00:00, 5466.67it/s]\n",
            "Model loaded!\n",
            "\n",
            "Loading proteins from: ./data/dta-5fold-dataset/davis/davis_prots.csv\n",
            "Found 442 unique proteins\n",
            "\n",
            "Extracting embeddings...\n",
            "100% 442/442 [02:23<00:00,  3.08it/s]\n",
            "\n",
            "============================================================\n",
            "✅ Successfully saved to: ./data/davis/davis_esmc_pretrain.pkl\n",
            "Total proteins: 442\n",
            "Embedding dimension: 960\n",
            "\n",
            "Verification:\n",
            "  First protein: AAK1\n",
            "  Vec shape: (963, 960) ❌\n",
            "  Mat shape: (1, 963, 960) ❌\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python code/train.py --fold 0 --cuda \"0\" --dataset davis --running_set warm --epochs 1 --batch_size 256"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciUIBZSjp50V",
        "outputId": "99f5f03f-eae9-43a0-ec3c-1f4bc84266a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Training Fold 0/4\n",
            "Dataset: davis-warm\n",
            "Device: cuda (CUDA_VISIBLE_DEVICES=0)\n",
            "Pretrain-./data/davis/davis_drug_pretrain.pkl\n",
            "Pretrain-./data/davis/davis_esmc_pretrain.pkl\n",
            "============================================================\n",
            "Loading fold 0 data...\n",
            "  Train: ./data/dta-5fold-dataset/davis/warm/fold_0_train.csv\n",
            "  Valid: ./data/dta-5fold-dataset/davis/warm/fold_0_valid.csv\n",
            "  Test:  ./data/dta-5fold-dataset/davis/warm/fold_0_test.csv\n",
            "Dataset loaded: 19236 train, 4809 valid, 6011 test samples\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "/content/Temp/code/MyDataset.py:68: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  drug_id, prot_id, label = pair[-3], pair[-2], pair[-1]\n",
            "Traing Log at fold-0 epoch-1: mse-5.901536, rmse-2.429308, r2--0.484114\n",
            "/content/Temp/code/MyDataset.py:68: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  drug_id, prot_id, label = pair[-3], pair[-2], pair[-1]\n",
            "Valid at fold-0: mse-1.04939\n",
            "Update best_mse, Valid at fold-0 epoch-1: mse-1.04939, rmse-1.024397, ci--1, r2--0.319342, pearson-0.220527, spearman-0.209109\n",
            "Save log over at ./log/Nov19_10-31-11-davis-warm-fold0.csv\n",
            "\n",
            "============================================================\n",
            "Testing fold 0 with best model...\n",
            "============================================================\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "/content/Temp/code/MyDataset.py:68: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  drug_id, prot_id, label = pair[-3], pair[-2], pair[-1]\n",
            "Test at fold-0, mse: 1.031492, rmse: 1.015624, ci: 0.61617, r2: -0.335446, pearson: 0.230741, spearman: 0.219546\n",
            "\n",
            "Fold 0 results saved to: ./log/Test-davis-warm-fold0-Nov19_10-31-11.csv\n",
            "============================================================\n",
            "Training fold 0 completed successfully!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python code/train.py --fold 0 --cuda \"0\" --dataset kiba --running_set warm --epochs 1 --batch_size 256"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5AhwwBkp68w",
        "outputId": "e536cea3-1529-46a0-e212-5b1da390c0bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Training Fold 0/4\n",
            "Dataset: kiba-warm\n",
            "Device: cuda (CUDA_VISIBLE_DEVICES=0)\n",
            "Pretrain-./data/kiba/kiba_drug_pretrain.pkl\n",
            "Pretrain-./data/kiba/kiba_esmc_pretrain.pkl\n",
            "============================================================\n",
            "Loading fold 0 data...\n",
            "  Train: ./data/dta-5fold-dataset/kiba/warm/fold_0_train.csv\n",
            "  Valid: ./data/dta-5fold-dataset/kiba/warm/fold_0_valid.csv\n",
            "  Test:  ./data/dta-5fold-dataset/kiba/warm/fold_0_test.csv\n",
            "Dataset loaded: 75683 train, 18921 valid, 23650 test samples\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "/content/Temp/code/MyDataset.py:68: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  drug_id, prot_id, label = pair[-3], pair[-2], pair[-1]\n",
            "Traing Log at fold-0 epoch-1: mse-12.641649, rmse-3.55551, r2--0.174813\n",
            "/content/Temp/code/MyDataset.py:68: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  drug_id, prot_id, label = pair[-3], pair[-2], pair[-1]\n",
            "Valid at fold-0: mse-2.244431\n",
            "Update best_mse, Valid at fold-0 epoch-1: mse-2.244431, rmse-1.498143, ci--1, r2--2.214841, pearson-0.227209, spearman-0.224625\n",
            "Save log over at ./log/Nov19_10-34-58-kiba-warm-fold0.csv\n",
            "\n",
            "============================================================\n",
            "Testing fold 0 with best model...\n",
            "============================================================\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "/content/Temp/code/MyDataset.py:68: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  drug_id, prot_id, label = pair[-3], pair[-2], pair[-1]\n",
            "Test at fold-0, mse: 2.231826, rmse: 1.49393, ci: 0.584381, r2: -2.245996, pearson: 0.232262, spearman: 0.233588\n",
            "\n",
            "Fold 0 results saved to: ./log/Test-kiba-warm-fold0-Nov19_10-34-58.csv\n",
            "============================================================\n",
            "Training fold 0 completed successfully!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python code/train.py --fold 0 --use_esmc true --esmc_model esmc_6b --epochs 1 --batch_size 128 --cuda 0 --dataset kiba\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_6PR0W4RKHm",
        "outputId": "e62fac51-925a-4cd2-d9e9-0fd02a51002b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Training Fold 0/4\n",
            "Dataset: kiba-novel-pair\n",
            "ESM Model: ESM-C-esmc_6b (dim=2560)\n",
            "Device: cuda (CUDA_VISIBLE_DEVICES=0)\n",
            "Pretrain-./data/kiba/kiba_drug_pretrain.pkl\n",
            "Pretrain-./data/kiba/kiba_esmc_6b_pretrain.pkl\n",
            "============================================================\n",
            "Loading fold 0 data...\n",
            "  Train: ./data/dta-5fold-dataset/kiba/novel-pair/fold_0_train.csv\n",
            "  Valid: ./data/dta-5fold-dataset/kiba/novel-pair/fold_0_valid.csv\n",
            "  Test:  ./data/dta-5fold-dataset/kiba/novel-pair/fold_0_test.csv\n",
            "Dataset loaded: 40301 train, 57463 valid, 20490 test samples\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "/content/Temp/code/MyDataset.py:68: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  drug_id, prot_id, label = pair[-3], pair[-2], pair[-1]\n",
            "Traing Log at fold-0 epoch-1: mse-9.816054, rmse-3.133058, r2--0.149047\n",
            "/content/Temp/code/MyDataset.py:68: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  drug_id, prot_id, label = pair[-3], pair[-2], pair[-1]\n",
            "Valid at fold-0: mse-10.36696\n",
            "Save log over at ./log/Nov20_08-01-13-kiba-novel-pair-fold0.csv\n",
            "\n",
            "============================================================\n",
            "Testing fold 0 with best model...\n",
            "============================================================\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Temp/code/train.py\", line 237, in <module>\n",
            "    predModel.load_state_dict(torch.load(model_fromTrain))\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1484, in load\n",
            "    with _open_file_like(f, \"rb\") as opened_file:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 759, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 740, in __init__\n",
            "    super().__init__(open(name, mode))\n",
            "                     ^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: './savemodel/kiba-novel-pair-fold0-Nov20_08-01-13.pth'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}