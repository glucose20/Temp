{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start: Generate Embeddings for Custom Dataset\n",
    "\n",
    "This notebook generates pretrained embeddings for your drugs and proteins.\n",
    "\n",
    "## Input Files Required:\n",
    "1. **drugs.csv** with columns: `drug_id`, `drug_smile`\n",
    "2. **proteins.csv** with columns: `prot_id`, `prot_seq`\n",
    "3. **model_300dim.pkl** (mol2vec model) - already in `data/`\n",
    "4. **protVec_100d_3grams.csv** (protein model) - download if needed\n",
    "\n",
    "## Output:\n",
    "- `data/{dataset_name}/{dataset_name}_drug_pretrain.pkl`\n",
    "- `data/{dataset_name}/{dataset_name}_prot_pretrain.pkl`\n",
    "\n",
    "## Steps:\n",
    "1. Run all cells in order\n",
    "2. Update `dataset_name`, `drugs_csv`, and `proteins_csv` paths in configuration cells\n",
    "3. Check output files in `data/{dataset_name}/` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare ESM2 pretrain\n",
    "\n",
    "We choose the esm2_t33_650M_UR50D model from this [hub](https://github.com/facebookresearch/esm#available-models).\n",
    "\n",
    "For fit esm2 model input, we set the amino acid seqs MAX_LEN = 1022 (+ cls, eos == 1024).\n",
    "\n",
    "For each protein(length = m), it will generate a feature mat, shape=(m, 1280). \n",
    "\n",
    "By mean this mat, it output a feature vec, dim=(1280).\n",
    "\n",
    "Cuz the limitation of the memory, we only compute one protein each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Input\n",
    "- [dta-origin-dataset](https://www.kaggle.com/datasets/christang0002/llmdta/data)\n",
    "    - davis.txt\n",
    "    - kiba.txt\n",
    "    - metz.txt\n",
    "- model_300dim.pkl\n",
    "- protVec_100d_3grams.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install fair-esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.7' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/msys64/ucrt64/bin/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ESM-2 model (after installing fair-esm)\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()  # disables dropout for deterministic results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import gc\n",
    "\n",
    "def get_esm_pretrain(model, df_dir, db_name, sep=' ', header=None, col_names=['drug_id', 'prot_id', 'drug_smile', 'prot_seq', 'label'], batch_size=50):\n",
    "    df = pd.read_csv(df_dir, sep=sep, header=header)\n",
    "    \n",
    "    # Only set column names if number matches\n",
    "    if len(df.columns) == len(col_names):\n",
    "        df.columns = col_names\n",
    "    else:\n",
    "        print(f\"Warning: CSV has {len(df.columns)} columns but {len(col_names)} names provided. Using first 2 columns as prot_id and prot_seq.\")\n",
    "        if len(df.columns) >= 2:\n",
    "            df.columns = ['prot_id', 'prot_seq'] + [f'col_{i}' for i in range(2, len(df.columns))]\n",
    "        else:\n",
    "            # Only 1 column, assume it's sequence and generate IDs\n",
    "            df.columns = ['prot_seq']\n",
    "            df['prot_id'] = [f'prot_{i}' for i in range(len(df))]\n",
    "    \n",
    "    df.drop_duplicates(subset='prot_id', inplace=True)\n",
    "    prot_ids = df['prot_id'].tolist()\n",
    "    prot_seqs = df['prot_seq'].tolist()\n",
    "    data = []\n",
    "    prot_size = len(prot_ids)\n",
    "    for i in range(prot_size):\n",
    "        seq_len = min(len(prot_seqs[i]),1022)\n",
    "        data.append((prot_ids[i], prot_seqs[i][:seq_len]))\n",
    "    \n",
    "    emb_dict = {}\n",
    "    emb_mat_dict = {}\n",
    "    length_target = {}\n",
    "\n",
    "    print(f\"Processing {len(data)} proteins in batches of {batch_size}...\")\n",
    "    \n",
    "    # Process in batches and save periodically to avoid RAM overflow\n",
    "    for d in tqdm(data):\n",
    "        prot_id = d[0]\n",
    "        batch_labels, batch_strs, batch_tokens = batch_converter([d])\n",
    "        batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "        \n",
    "        # Extract per-residue representations (on CPU)\n",
    "        with torch.no_grad():\n",
    "            results = model(batch_tokens, repr_layers=[33], return_contacts=False)  # Disable contact map to save memory\n",
    "        token_representations = results[\"representations\"][33].cpu().numpy()  # Explicitly move to CPU\n",
    "\n",
    "        sequence_representations = []\n",
    "        for i, tokens_len in enumerate(batch_lens):\n",
    "            sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "\n",
    "        emb_dict[prot_id] = sequence_representations[0]\n",
    "        emb_mat_dict[prot_id] = token_representations[0]\n",
    "        length_target[prot_id] = len(d[1])\n",
    "        \n",
    "        # Clear tensors every batch_size iterations\n",
    "        if len(emb_dict) % batch_size == 0:\n",
    "            del batch_tokens, results, token_representations\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    dump_data = {\n",
    "        \"dataset\": db_name,\n",
    "        \"vec_dict\": emb_dict,\n",
    "        \"mat_dict\": emb_mat_dict,\n",
    "        \"length_dict\": length_target\n",
    "    }\n",
    "    \n",
    "    # Create directory if not exists\n",
    "    os.makedirs(f'./data/{db_name}', exist_ok=True)\n",
    "    \n",
    "    with open(f'./data/{db_name}/{db_name}_esm_pretrain.pkl', 'wb+') as f:\n",
    "        pickle.dump(dump_data, f)\n",
    "    print(f\"✓ Saved to ./data/{db_name}/{db_name}_esm_pretrain.pkl\")\n",
    "    \n",
    "    # Final cleanup\n",
    "    del emb_dict, emb_mat_dict, length_target\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check proteins.csv format first\n",
    "df_dir = '/content/Temp/data/simple-Case/proteins.csv'\n",
    "df_test = pd.read_csv(df_dir, sep=',', header=0)  # Try comma separator\n",
    "print(f\"Shape: {df_test.shape}\")\n",
    "print(f\"Columns: {df_test.columns.tolist()}\")\n",
    "print(f\"First 3 rows:\\n{df_test.head(3)}\")\n",
    "\n",
    "# Now run get_esm_pretrain with correct separator\n",
    "db_name = 'case_study'\n",
    "col_names = ['prot_id', 'prot_seq']\n",
    "get_esm_pretrain(model, df_dir, db_name, sep=',', header=0, col_names=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(df_dir, sep='\\t')\n",
    "df.columns = col_names\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_names = ['davis', 'kiba', 'metz']\n",
    "df_dirs = [r'/home/tangwuguo/datasets/davis.txt', r'/home/tangwuguo/datasets/kiba.txt', r'/home/tangwuguo/datasets/metz.txt']\n",
    "\n",
    "for i in range(1,2):\n",
    "    print(f'Compute {df_dirs[i]} protein pretrain feature by esm2.')\n",
    "    get_esm_pretrain(model, df_dirs[i], db_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mol2Vec pretrain\n",
    "Input drug smiles seq, firstly it will compute the sub-structure of this drug.\n",
    "\n",
    "For one SMILES, sub-strutures num=m, it outputs a (m,300) feature mat and a 300-dim feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for mol2vec\n",
    "!pip install gensim mol2vec rdkit\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import pickle\n",
    "from gensim.models import word2vec\n",
    "from mol2vec.features import mol2alt_sentence\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mol2vec(mol2vec_dir, df_dir, db_name, sep=',', header=0, col_names=['drug_id', 'drug_smile'], embedding_dimension=300, is_debug=False, show_miss_details=False):\n",
    "    mol2vec_model = word2vec.Word2Vec.load(mol2vec_dir)\n",
    "    \n",
    "    df = pd.read_csv(df_dir, header=header, sep=sep)\n",
    "    df.columns = col_names\n",
    "    df.drop_duplicates(subset='drug_id', inplace=True)    \n",
    "    drug_ids = df['drug_id'].tolist()\n",
    "    drug_seqs = df['drug_smile'].tolist()\n",
    "    \n",
    "    emb_dict = {}\n",
    "    emb_mat_dict = {}\n",
    "    length_dict = {}\n",
    "    \n",
    "    percent_unknown = []\n",
    "    bad_mol = 0\n",
    "    miss_details = []  # Track which drugs have missing substructures\n",
    "    \n",
    "    # get pretrain feature\n",
    "    for idx in tqdm(range(len(drug_ids))):\n",
    "        flag = 0\n",
    "        mol_miss_words = 0\n",
    "        \n",
    "        drug_id = str(drug_ids[idx])\n",
    "        molecule = Chem.MolFromSmiles(drug_seqs[idx])\n",
    "        \n",
    "        try:\n",
    "            # Get fingerprint from molecule\n",
    "            sub_structures = mol2alt_sentence(molecule, 2)\n",
    "        except Exception as e: \n",
    "            if is_debug: \n",
    "                print (e)\n",
    "            percent_unknown.append(100)\n",
    "            continue    \n",
    "                \n",
    "        emb_mat = np.zeros((len(sub_structures), embedding_dimension))\n",
    "        length_dict[drug_id] = len(sub_structures)\n",
    "        \n",
    "        missed_subs = []  # Track missing substructures for this drug\n",
    "        for i, sub in enumerate(sub_structures):\n",
    "            # Check to see if substructure exists\n",
    "            try:\n",
    "                emb_dict[drug_id] = emb_dict.get(drug_id, np.zeros(embedding_dimension)) + mol2vec_model.wv[sub]  \n",
    "                emb_mat[i] = mol2vec_model.wv[sub]  \n",
    "            # If not, replace with UNK (unknown)\n",
    "            except Exception as e:\n",
    "                if is_debug : \n",
    "                    print (\"Sub structure not found\")\n",
    "                    print (e)\n",
    "                emb_dict[drug_id] = emb_dict.get(drug_id, np.zeros(embedding_dimension)) + mol2vec_model.wv['UNK']\n",
    "                emb_mat[i] = mol2vec_model.wv['UNK']                \n",
    "                flag = 1\n",
    "                mol_miss_words = mol_miss_words + 1\n",
    "                missed_subs.append(sub)\n",
    "        \n",
    "        emb_mat_dict[drug_id] = emb_mat\n",
    "        \n",
    "        miss_rate = (mol_miss_words / len(sub_structures)) * 100\n",
    "        percent_unknown.append(miss_rate)\n",
    "        if flag == 1:\n",
    "            bad_mol = bad_mol + 1\n",
    "            if show_miss_details:\n",
    "                miss_details.append({\n",
    "                    'drug_id': drug_id,\n",
    "                    'smile': drug_seqs[idx],\n",
    "                    'total_subs': len(sub_structures),\n",
    "                    'missed_subs': mol_miss_words,\n",
    "                    'miss_rate': miss_rate,\n",
    "                    'missed_substructures': missed_subs[:5]  # Show first 5 missing\n",
    "                })\n",
    "            \n",
    "    print(f'All Bad Mol: {bad_mol}, Avg Miss Rate: {sum(percent_unknown)/len(percent_unknown):.2f}%')\n",
    "    \n",
    "    # Show details of top 10 worst drugs\n",
    "    if show_miss_details and miss_details:\n",
    "        print(\"\\n=== Top 10 drugs with highest miss rates ===\")\n",
    "        miss_details.sort(key=lambda x: x['miss_rate'], reverse=True)\n",
    "        for i, detail in enumerate(miss_details[:10]):\n",
    "            print(f\"\\n{i+1}. Drug ID: {detail['drug_id']}\")\n",
    "            print(f\"   SMILES: {detail['smile'][:50]}...\")\n",
    "            print(f\"   Miss Rate: {detail['miss_rate']:.1f}% ({detail['missed_subs']}/{detail['total_subs']} substructures)\")\n",
    "            print(f\"   Missing substructures: {detail['missed_substructures']}\")\n",
    "        \n",
    "    dump_data = {\n",
    "        \"dataset\": db_name,\n",
    "        \"vec_dict\": emb_dict,\n",
    "        \"mat_dict\": emb_mat_dict,\n",
    "        \"length_dict\": length_dict\n",
    "    }\n",
    "    \n",
    "    # Create directory if not exists\n",
    "    import os\n",
    "    os.makedirs(f'./data/{db_name}', exist_ok=True)\n",
    "    \n",
    "    with open(f'./data/{db_name}/{db_name}_drug_pretrain.pkl', 'wb+') as f:\n",
    "        pickle.dump(dump_data, f)\n",
    "    print(f\"\\n✓ Saved to ./data/{db_name}/{db_name}_drug_pretrain.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download mol2vec model if not exists\n",
    "import os\n",
    "if not os.path.exists('./data/model_300dim.pkl'):\n",
    "    print(\"Downloading mol2vec model (73MB)...\")\n",
    "    !mkdir -p ./data\n",
    "    # Download from GitHub repo or use wget\n",
    "    !wget -O ./data/model_300dim.pkl https://github.com/samoturk/mol2vec/raw/master/examples/models/model_300dim.pkl\n",
    "    print(\"✓ Downloaded model_300dim.pkl\")\n",
    "else:\n",
    "    print(\"✓ model_300dim.pkl already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for your custom dataset\n",
    "mol2vec_dir = './data/model_300dim.pkl'\n",
    "dataset_name = 'custom'  # Change this to your dataset name\n",
    "drugs_csv = './data/simple-Case/drugs.csv'  # Path to your drugs.csv\n",
    "\n",
    "# Generate mol2vec embeddings with miss details\n",
    "get_mol2vec(mol2vec_dir, drugs_csv, dataset_name, sep=',', header=0, \n",
    "            col_names=['drug_id', 'drug_smile'], show_miss_details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol2vec_dir = './data/model_300dim.pkl'\n",
    "db_names = ['davis', 'kiba', 'metz']\n",
    "df_dirs = [r'/home/tangwuguo/datasets/davis.txt', r'/home/tangwuguo/datasets/kiba.txt', r'/home/tangwuguo/datasets/metz.txt']\n",
    "\n",
    "for i in range(3):\n",
    "    print(f'Compute {db_names[i]} drug pretrain feature by protvec.')\n",
    "    get_esm_pretrain(mol2vec_dir, df_dirs[i], db_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProtVec pretrain\n",
    "Inuput protein's amino acid seq, len = m\n",
    "\n",
    "Output feature mat(m,100), feature vector(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_protvec(protvec_dir, df_dir, db_name, col_names=['prot_id', 'prot_seq'], embedding_dimension=100, is_debug=False, sep=',', header=0):        \n",
    "    protvec_model = pd.read_csv(protvec_dir, delimiter = '\\t')\n",
    "    trigram_dict = {}\n",
    "    for idx, row in tqdm(protvec_model.iterrows()):\n",
    "        trigram_dict[row['words']] = protvec_model.iloc[idx, 1:].values.astype(np.float64)\n",
    "    trigram_list = set(trigram_dict.keys())\n",
    "    \n",
    "    # Read CSV with proper parameters\n",
    "    df = pd.read_csv(df_dir, header=header, sep=sep)\n",
    "    df.columns = col_names\n",
    "    df.drop_duplicates(subset='prot_id', inplace=True)    \n",
    "    prot_ids = df['prot_id'].tolist()\n",
    "    prot_seqs = df['prot_seq'].tolist()\n",
    "    \n",
    "    emb_dict = {}\n",
    "    emb_mat_dict = {}\n",
    "    length_3mer_target = {}\n",
    "\n",
    "    # get pretrain feature\n",
    "    for idx in tqdm(range(len(prot_ids))):\n",
    "        n = 3\n",
    "        target = prot_seqs[idx]\n",
    "        prot_id = str(prot_ids[idx])\n",
    "        split_by_three = [target[i : i + n] for i in range(0, len(target), n)]\n",
    "        mer_len = len(split_by_three)\n",
    "        length_3mer_target[prot_id] = mer_len\n",
    "        \n",
    "        emb_mat = np.zeros((mer_len, embedding_dimension))\n",
    "        for i, trigram in enumerate(split_by_three): \n",
    "            if len(trigram) == 2: \n",
    "                trigram = \"X\" + trigram\n",
    "            elif len(trigram) == 1:\n",
    "                trigram = \"XX\" + trigram\n",
    "            if trigram in trigram_list:\n",
    "                emb_dict[prot_id] = emb_dict.get(prot_id, np.zeros(embedding_dimension))+ trigram_dict[trigram]\n",
    "                emb_mat[i] = trigram_dict[trigram]\n",
    "        emb_mat_dict[prot_id] = emb_mat\n",
    "        \n",
    "    # Save pretrain embeddings\n",
    "    dump_data = {\n",
    "        \"dataset\": db_name,\n",
    "        \"vec_dict\": emb_dict,\n",
    "        \"mat_dict\": emb_mat_dict,\n",
    "        \"length_dict\": length_3mer_target\n",
    "    }    \n",
    "    with open(f'./data/{db_name}/{db_name}_prot_pretrain.pkl', 'wb+') as f:\n",
    "        pickle.dump(dump_data, f)\n",
    "    print(f\"✓ Saved to ./data/{db_name}/{db_name}_prot_pretrain.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for protein embeddings\n",
    "protvec_dir = './data/protVec_100d_3grams.csv'\n",
    "dataset_name = 'custom'  # Same name as above\n",
    "proteins_csv = './data/simple-Case/proteins.csv'  # Path to your proteins.csv\n",
    "\n",
    "# Generate protein embeddings\n",
    "print(f'Computing {dataset_name} protein pretrain features by protvec...')\n",
    "get_protvec(protvec_dir, proteins_csv, dataset_name, col_names=['prot_id', 'prot_seq'], embedding_dimension=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
