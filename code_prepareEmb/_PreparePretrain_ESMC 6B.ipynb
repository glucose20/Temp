{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75029867",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a639046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# ESM-C imports\n",
    "from esm.models.esmc import ESMC\n",
    "from esm.sdk.api import ESMProtein, LogitsConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb32354",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de720ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select ESM-C model variant\n",
    "MODEL_NAME = \"esmc_300m\"  # Options: esmc_300m, esmc_600m, esmc_6b (via Forge API)\n",
    "USE_FORGE_API = False  # Set True for esmc_6b with Forge token\n",
    "\n",
    "# Forge API configuration (only needed for esmc_6b)\n",
    "FORGE_TOKEN = \"<your forge token>\"  # Get from https://forge.evolutionaryscale.ai\n",
    "\n",
    "# Device configuration (for local models only)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Model dimensions\n",
    "MODEL_DIMS = {\n",
    "    \"esmc_300m\": 960,\n",
    "    \"esmc_600m\": 1152,\n",
    "    \"esmc_6b\": 2560\n",
    "}\n",
    "EMBEDDING_DIM = MODEL_DIMS[MODEL_NAME]\n",
    "print(f\"Model: {MODEL_NAME}, Embedding dimension: {EMBEDDING_DIM}\")\n",
    "print(f\"Using Forge API: {USE_FORGE_API}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86937bd8",
   "metadata": {},
   "source": [
    "## 3. Load ESM-C Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8008b0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "if USE_FORGE_API:\n",
    "    print(f\"Connecting to ESM Forge API for {MODEL_NAME}...\")\n",
    "    from esm.sdk.forge import ESM3ForgeInferenceClient\n",
    "    \n",
    "    model = ESM3ForgeInferenceClient(\n",
    "        model=\"esmc-6b-2024-12\",\n",
    "        url=\"https://forge.evolutionaryscale.ai\",\n",
    "        token=FORGE_TOKEN\n",
    "    )\n",
    "    print(\"Connected to Forge API successfully!\")\n",
    "else:\n",
    "    print(f\"Loading local model: {MODEL_NAME}...\")\n",
    "    model = ESMC.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377698b4",
   "metadata": {},
   "source": [
    "## 4. Define Embedding Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02ae37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_esmc_pretrain(model, df_dir, db_name, sep=' ', header=None, \n",
    "                      col_names=['drug_id', 'prot_id', 'drug_smile', 'prot_seq', 'label'],\n",
    "                      use_forge_api=False, device='cuda'):\n",
    "    \"\"\"\n",
    "    Extract ESM-C embeddings for protein sequences.\n",
    "    \n",
    "    Args:\n",
    "        model: Loaded ESM-C model or Forge API client\n",
    "        df_dir: Path to input CSV file\n",
    "        db_name: Database name for output filename\n",
    "        sep: CSV separator\n",
    "        header: CSV header row (None for no header)\n",
    "        col_names: Column names for the dataframe\n",
    "        use_forge_api: Whether using Forge API for esmc_6b\n",
    "        device: Device for computation ('cuda' or 'cpu')\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing embeddings\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(df_dir, sep=sep, header=header)\n",
    "    df.columns = col_names\n",
    "    df.drop_duplicates(subset='prot_id', inplace=True)\n",
    "    \n",
    "    prot_ids = df['prot_id'].tolist()\n",
    "    prot_seqs = df['prot_seq'].tolist()\n",
    "    \n",
    "    emb_dict = {}\n",
    "    emb_mat_dict = {}\n",
    "    length_dict = {}\n",
    "    \n",
    "    print(f\"Processing {len(prot_ids)} proteins on {device}...\")\n",
    "    \n",
    "    if use_forge_api:\n",
    "        # Use Forge Batch Executor for efficient API calls\n",
    "        from esm.sdk import batch_executor\n",
    "        \n",
    "        def embed_sequence(client, sequence, prot_id):\n",
    "            \"\"\"Helper function for batch processing\"\"\"\n",
    "            try:\n",
    "                seq = sequence[:2048]\n",
    "                protein = ESMProtein(sequence=seq)\n",
    "                protein_tensor = client.encode(protein)\n",
    "                \n",
    "                from esm.sdk.api import ESMProteinError\n",
    "                if isinstance(protein_tensor, ESMProteinError):\n",
    "                    raise protein_tensor\n",
    "                \n",
    "                output = client.logits(\n",
    "                    protein_tensor,\n",
    "                    LogitsConfig(sequence=True, return_embeddings=True)\n",
    "                )\n",
    "                \n",
    "                # CRITICAL: Convert BFloat16 immediately after receiving from API\n",
    "                embeddings = output.embeddings\n",
    "                \n",
    "                # Convert to torch tensor first if needed, then to float32\n",
    "                if not isinstance(embeddings, torch.Tensor):\n",
    "                    embeddings = torch.tensor(embeddings)\n",
    "                \n",
    "                # Convert BFloat16 to Float32\n",
    "                if embeddings.dtype == torch.bfloat16:\n",
    "                    embeddings = embeddings.to(torch.float32)\n",
    "                \n",
    "                # Now convert to numpy (Forge API runs on server, so CPU here is fine)\n",
    "                if embeddings.is_cuda:\n",
    "                    embeddings = embeddings.cpu()\n",
    "                embeddings = embeddings.numpy()\n",
    "                \n",
    "                return (prot_id, embeddings, len(seq))\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                print(f\"\\nError processing {prot_id}: {e}\")\n",
    "                print(traceback.format_exc())\n",
    "                return (prot_id, None, len(sequence[:2048]))\n",
    "        \n",
    "        # Process in batches using Forge executor\n",
    "        print(\"Using Forge Batch Executor (server-side GPU processing)...\")\n",
    "        with batch_executor() as executor:\n",
    "            # Prepare data for batch execution\n",
    "            batch_data = [\n",
    "                {'client': model, 'sequence': seq, 'prot_id': pid}\n",
    "                for pid, seq in zip(prot_ids, prot_seqs)\n",
    "            ]\n",
    "            \n",
    "            outputs = executor.execute_batch(\n",
    "                user_func=embed_sequence,\n",
    "                **{k: [d[k] for d in batch_data] for k in batch_data[0].keys()}\n",
    "            )\n",
    "        \n",
    "        # Process outputs\n",
    "        for prot_id, embeddings, seq_len in outputs:\n",
    "            length_dict[prot_id] = seq_len\n",
    "            \n",
    "            if embeddings is not None:\n",
    "                # Ensure float32 dtype\n",
    "                if embeddings.dtype != np.float32:\n",
    "                    embeddings = embeddings.astype(np.float32)\n",
    "                \n",
    "                # Store mean embedding as sequence representation\n",
    "                emb_dict[prot_id] = embeddings.mean(axis=0)\n",
    "                emb_mat_dict[prot_id] = embeddings\n",
    "            else:\n",
    "                # Fallback to zeros\n",
    "                emb_dict[prot_id] = np.zeros(EMBEDDING_DIM, dtype=np.float32)\n",
    "                emb_mat_dict[prot_id] = np.zeros((seq_len, EMBEDDING_DIM), dtype=np.float32)\n",
    "    \n",
    "    else:\n",
    "        # Local model processing - USE GPU!\n",
    "        print(f\"Using local GPU: {device}\")\n",
    "        for idx in tqdm(range(len(prot_ids))):\n",
    "            prot_id = str(prot_ids[idx])\n",
    "            seq = prot_seqs[idx][:2048]\n",
    "            length_dict[prot_id] = len(seq)\n",
    "            \n",
    "            try:\n",
    "                # Create ESMProtein object\n",
    "                protein = ESMProtein(sequence=seq)\n",
    "                \n",
    "                # Encode the protein (stays on GPU)\n",
    "                protein_tensor = model.encode(protein)\n",
    "                \n",
    "                # Extract embeddings (computed on GPU)\n",
    "                with torch.no_grad():\n",
    "                    logits_output = model.logits(\n",
    "                        protein_tensor,\n",
    "                        LogitsConfig(return_embeddings=True)\n",
    "                    )\n",
    "                \n",
    "                embeddings = logits_output.embeddings\n",
    "                \n",
    "                # Convert BFloat16 to Float32 if needed (still on GPU)\n",
    "                if embeddings.dtype == torch.bfloat16:\n",
    "                    embeddings = embeddings.to(torch.float32)\n",
    "                \n",
    "                # Only move to CPU at the last step before numpy conversion\n",
    "                embeddings = embeddings.cpu().numpy()\n",
    "                \n",
    "                # Store mean embedding as sequence representation\n",
    "                emb_dict[prot_id] = embeddings.mean(axis=0)  # Shape: (d_model,)\n",
    "                \n",
    "                # Store full embedding matrix\n",
    "                emb_mat_dict[prot_id] = embeddings  # Shape: (seq_len, d_model)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing {prot_id}: {e}\")\n",
    "                # Use zero embeddings as fallback\n",
    "                emb_dict[prot_id] = np.zeros(EMBEDDING_DIM, dtype=np.float32)\n",
    "                emb_mat_dict[prot_id] = np.zeros((len(seq), EMBEDDING_DIM), dtype=np.float32)\n",
    "    \n",
    "    # Prepare output dictionary\n",
    "    dump_data = {\n",
    "        \"dataset\": db_name,\n",
    "        \"vec_dict\": emb_dict,\n",
    "        \"mat_dict\": emb_mat_dict,\n",
    "        \"length_dict\": length_dict,\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"embedding_dim\": EMBEDDING_DIM,\n",
    "        \"use_forge_api\": use_forge_api\n",
    "    }\n",
    "    \n",
    "    # Save to pickle\n",
    "    output_file = f'./{db_name}_esmc_pretrain.pkl'\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(dump_data, f)\n",
    "    \n",
    "    print(f\"\\nSaved embeddings to: {output_file}\")\n",
    "    print(f\"Total proteins: {len(emb_dict)}\")\n",
    "    print(f\"Embedding dimension: {EMBEDDING_DIM}\")\n",
    "    \n",
    "    return dump_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ba42cd",
   "metadata": {},
   "source": [
    "## 5. Test on Single Case Study\n",
    "\n",
    "Test the function on a small dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18e1b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Process simple-Case study data\n",
    "db_name = 'simple_case'\n",
    "df_dir = r'D:\\Download D\\2025\\DTA\\EXPLAIN 2\\Temp\\data\\simple-Case\\proteins.csv'\n",
    "col_names = ['prot_id', 'prot_seq']\n",
    "\n",
    "# Run the extraction (passes device to function)\n",
    "result = get_esmc_pretrain(\n",
    "    model=model, \n",
    "    df_dir=df_dir, \n",
    "    db_name=db_name, \n",
    "    sep=',', \n",
    "    header=0, \n",
    "    col_names=col_names, \n",
    "    use_forge_api=USE_FORGE_API,\n",
    "    device=DEVICE  # Pass device config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a67eb6",
   "metadata": {},
   "source": [
    "## 6. Process Full Datasets (Davis, KIBA, Metz)\n",
    "\n",
    "⚠️ **Important**: Update the file paths before running!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baae3791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configurations\n",
    "datasets = {\n",
    "    'davis': './data/dta-5fold-dataset/davis/davis_prots.csv',\n",
    "    'kiba': './data/dta-5fold-dataset/kiba/kiba_prots.csv',\n",
    "    'metz': './data/dta-5fold-dataset/metz/metz_prots.csv'\n",
    "}\n",
    "\n",
    "col_names = ['prot_id', 'prot_seq']\n",
    "\n",
    "# Process each dataset\n",
    "for db_name, df_dir in datasets.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing {db_name.upper()} dataset\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        result = get_esmc_pretrain(\n",
    "            model=model,\n",
    "            df_dir=df_dir,\n",
    "            db_name=db_name,\n",
    "            sep=',',  # Adjust if needed\n",
    "            header=0,\n",
    "            col_names=col_names,\n",
    "            use_forge_api=USE_FORGE_API,\n",
    "            device=DEVICE  # Pass device config\n",
    "        )\n",
    "        print(f\"✓ Successfully processed {db_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error processing {db_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36853e66",
   "metadata": {},
   "source": [
    "## 7. Verify Generated Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a27e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect generated embeddings\n",
    "def verify_embeddings(db_name):\n",
    "    file_path = f'./{db_name}_esmc_pretrain.pkl'\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Verification: {db_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Dataset: {data['dataset']}\")\n",
    "        print(f\"Model: {data['model']}\")\n",
    "        print(f\"Embedding dimension: {data['embedding_dim']}\")\n",
    "        print(f\"Number of proteins: {len(data['vec_dict'])}\")\n",
    "        \n",
    "        # Check first protein\n",
    "        first_prot_id = list(data['vec_dict'].keys())[0]\n",
    "        first_vec = data['vec_dict'][first_prot_id]\n",
    "        first_mat = data['mat_dict'][first_prot_id]\n",
    "        \n",
    "        print(f\"\\nFirst protein: {first_prot_id}\")\n",
    "        print(f\"  - Vec shape: {first_vec.shape}\")\n",
    "        print(f\"  - Mat shape: {first_mat.shape}\")\n",
    "        print(f\"  - Sequence length: {data['length_dict'][first_prot_id]}\")\n",
    "        \n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"✗ File not found: {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Verify each dataset\n",
    "for db_name in ['davis', 'kiba', 'metz']:\n",
    "    verify_embeddings(db_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4550f81",
   "metadata": {},
   "source": [
    "## 8. Compare with ESM2 (Optional)\n",
    "\n",
    "If you have ESM2 embeddings, you can compare dimensions and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca92b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_embeddings(db_name):\n",
    "    esmc_file = f'./{db_name}_esmc_pretrain.pkl'\n",
    "    esm2_file = f'./{db_name}_esm_pretrain.pkl'\n",
    "    \n",
    "    try:\n",
    "        with open(esmc_file, 'rb') as f:\n",
    "            esmc_data = pickle.load(f)\n",
    "        \n",
    "        with open(esm2_file, 'rb') as f:\n",
    "            esm2_data = pickle.load(f)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Comparison: {db_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"ESM-C proteins: {len(esmc_data['vec_dict'])}\")\n",
    "        print(f\"ESM2 proteins:  {len(esm2_data['vec_dict'])}\")\n",
    "        print(f\"\\nESM-C embedding dim: {esmc_data['embedding_dim']}\")\n",
    "        print(f\"ESM2 embedding dim:  {list(esm2_data['vec_dict'].values())[0].shape[0]}\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"✗ File not found: {e}\")\n",
    "\n",
    "# Run comparison\n",
    "# compare_embeddings('davis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a21a4f",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "After generating ESM-C embeddings:\n",
    "\n",
    "1. **Update hyperparameter.py**:\n",
    "   - Set `use_esmc = True`\n",
    "   - Set `esmc_model = \"esmc_300m\"` (or your chosen variant)\n",
    "   - Verify `protvec_dim` matches your model (960, 1152, or 2560)\n",
    "\n",
    "2. **Update data paths**:\n",
    "   - Move generated `.pkl` files to `./data/{dataset}/` directory\n",
    "   - Ensure filenames match: `{dataset}_esmc_pretrain.pkl`\n",
    "\n",
    "3. **Retrain model**:\n",
    "   ```bash\n",
    "   python code/train.py\n",
    "   ```\n",
    "\n",
    "4. **Compare performance**:\n",
    "   - Note: You'll need to retrain with new embeddings\n",
    "   - Compare metrics (MSE, CI, R²) between ESM2 and ESM-C\n",
    "\n",
    "## Summary\n",
    "\n",
    "✅ ESM-C provides:\n",
    "- Better performance (smaller models, better results)\n",
    "- Longer sequences (2048 vs 1022 tokens)\n",
    "- Faster inference\n",
    "- Modern architecture\n",
    "\n",
    "⚠️ Trade-offs:\n",
    "- Different API (not drop-in replacement)\n",
    "- Different dimensions (960/1152/2560 vs 1280)\n",
    "- Requires retraining model with new embeddings"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
