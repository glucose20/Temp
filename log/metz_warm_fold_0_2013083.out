==========================================
Job ID: 2013083
Array Task ID: 0
Node: v100-f-11
Start Time: Thu Nov 13 07:19:12 AM AEDT 2025
==========================================
Activating conda environment...
Conda environment activated: LLMDTA
Checking GPU...
Thu Nov 13 07:19:13 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             41W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Setting environment variables...

Starting training for fold 0...


============================================================
Starting training for Fold 0
Dataset: metz, Running Set: warm
Epochs: 200, Batch Size: 16
============================================================

Executing: python -u code/train.py --fold 0 --cuda 0 --dataset metz --running_set warm --epochs 200 --batch_size 16 --wandb_project LLMDTA
============================================================
Training Fold 0/4
Dataset: metz-warm
Device: cuda (CUDA_VISIBLE_DEVICES=0)
Pretrain-./data/metz/metz_drug_pretrain.pkl
Pretrain-./data/metz/metz_esm_pretrain.pkl
============================================================
wandb: Currently logged in as: tringuyen to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: setting up run 1uj8ky1q
wandb: Tracking run with wandb version 0.23.0
wandb: Run data is saved locally in /vast/minhtrin/DTA/Temp/wandb/run-20251113_071928-1uj8ky1q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run metz-warm-fold0
wandb: â­ï¸ View project at https://wandb.ai/tringuyen/LLMDTA
wandb: ğŸš€ View run at https://wandb.ai/tringuyen/LLMDTA/runs/1uj8ky1q
Weights & Biases initialized: LLMDTA
Loading fold 0 data...
  Train: ./data/dta-5fold-dataset/metz/warm/fold_0_train.csv
  Valid: ./data/dta-5fold-dataset/metz/warm/fold_0_valid.csv
  Test:  ./data/dta-5fold-dataset/metz/warm/fold_0_test.csv
Dataset loaded: 22566 train, 5642 valid, 7051 test samples
/home/minhtrin/.conda/envs/LLMDTA/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Traing Log at fold-0 epoch-1: mse-2.158773, rmse-1.469276, r2--0.231033
Valid at fold-0: mse-1.47282
Update best_mse, Valid at fold-0 epoch-1: mse-1.47282, rmse-1.213598, ci--1, r2--0.566676, pearson-0.478659, spearman-0.443632
Traing Log at fold-0 epoch-2: mse-0.804622, rmse-0.897007, r2--0.197475
Valid at fold-0: mse-0.659214
Update best_mse, Valid at fold-0 epoch-2: mse-0.659214, rmse-0.81192, ci--1, r2-0.298778, pearson-0.611876, spearman-0.573908
Traing Log at fold-0 epoch-3: mse-0.655847, rmse-0.809844, r2--0.105169
Valid at fold-0: mse-0.750084
Traing Log at fold-0 epoch-4: mse-0.579003, rmse-0.760923, r2--0.046861
Valid at fold-0: mse-0.662281
Traing Log at fold-0 epoch-5: mse-0.51703, rmse-0.719048, r2-0.047124
Valid at fold-0: mse-0.473487
Update best_mse, Valid at fold-0 epoch-5: mse-0.473487, rmse-0.688104, ci--1, r2-0.496339, pearson-0.710011, spearman-0.657997
Traing Log at fold-0 epoch-6: mse-0.477308, rmse-0.690875, r2-0.102414
Valid at fold-0: mse-0.456075
Update best_mse, Valid at fold-0 epoch-6: mse-0.456075, rmse-0.675333, ci--1, r2-0.514862, pearson-0.719514, spearman-0.664
Traing Log at fold-0 epoch-7: mse-0.442044, rmse-0.664864, r2-0.173305
Valid at fold-0: mse-0.430222
Update best_mse, Valid at fold-0 epoch-7: mse-0.430222, rmse-0.655913, ci--1, r2-0.542362, pearson-0.746075, spearman-0.706664
Traing Log at fold-0 epoch-8: mse-0.414826, rmse-0.64407, r2-0.232179
Valid at fold-0: mse-0.41206
Update best_mse, Valid at fold-0 epoch-8: mse-0.41206, rmse-0.641919, ci--1, r2-0.561681, pearson-0.761199, spearman-0.717431
Traing Log at fold-0 epoch-9: mse-0.392727, rmse-0.626679, r2-0.291068
Valid at fold-0: mse-0.401197
Update best_mse, Valid at fold-0 epoch-9: mse-0.401197, rmse-0.633401, ci--1, r2-0.573236, pearson-0.759105, spearman-0.713942
Traing Log at fold-0 epoch-10: mse-0.375783, rmse-0.613011, r2-0.330989
Valid at fold-0: mse-0.40023
Update best_mse, Valid at fold-0 epoch-10: mse-0.40023, rmse-0.632637, ci--1, r2-0.574266, pearson-0.765104, spearman-0.724476
Traing Log at fold-0 epoch-11: mse-0.360274, rmse-0.600228, r2-0.372824
Valid at fold-0: mse-0.393092
Update best_mse, Valid at fold-0 epoch-11: mse-0.393092, rmse-0.62697, ci--1, r2-0.581858, pearson-0.768808, spearman-0.72363
Traing Log at fold-0 epoch-12: mse-0.344027, rmse-0.586538, r2-0.412159
Valid at fold-0: mse-0.371804
Update best_mse, Valid at fold-0 epoch-12: mse-0.371804, rmse-0.609758, ci--1, r2-0.604502, pearson-0.779246, spearman-0.731142
Traing Log at fold-0 epoch-13: mse-0.332684, rmse-0.576787, r2-0.444584
Valid at fold-0: mse-0.37249
Traing Log at fold-0 epoch-14: mse-0.321938, rmse-0.567396, r2-0.468072
Valid at fold-0: mse-0.359008
Update best_mse, Valid at fold-0 epoch-14: mse-0.359008, rmse-0.599173, ci--1, r2-0.618114, pearson-0.790288, spearman-0.740165
Traing Log at fold-0 epoch-15: mse-0.31203, rmse-0.558597, r2-0.494222
Valid at fold-0: mse-0.354836
Update best_mse, Valid at fold-0 epoch-15: mse-0.354836, rmse-0.595681, ci--1, r2-0.622552, pearson-0.790831, spearman-0.74621
Traing Log at fold-0 epoch-16: mse-0.300194, rmse-0.5479, r2-0.520044
Valid at fold-0: mse-0.358633
Traing Log at fold-0 epoch-17: mse-0.289867, rmse-0.538393, r2-0.543843
Valid at fold-0: mse-0.362889
Traing Log at fold-0 epoch-18: mse-0.281772, rmse-0.530822, r2-0.563527
Valid at fold-0: mse-0.343987
Update best_mse, Valid at fold-0 epoch-18: mse-0.343987, rmse-0.586504, ci--1, r2-0.634093, pearson-0.801503, spearman-0.75473
Traing Log at fold-0 epoch-19: mse-0.274675, rmse-0.524094, r2-0.57806
Valid at fold-0: mse-0.33976
Update best_mse, Valid at fold-0 epoch-19: mse-0.33976, rmse-0.582889, ci--1, r2-0.638589, pearson-0.801271, spearman-0.754414
Traing Log at fold-0 epoch-20: mse-0.264611, rmse-0.514404, r2-0.599723
Valid at fold-0: mse-0.351788
Traing Log at fold-0 epoch-21: mse-0.26167, rmse-0.511537, r2-0.603861
Valid at fold-0: mse-0.345829
Traing Log at fold-0 epoch-22: mse-0.253683, rmse-0.50367, r2-0.623688
Valid at fold-0: mse-0.335246
Update best_mse, Valid at fold-0 epoch-22: mse-0.335246, rmse-0.579004, ci--1, r2-0.64339, pearson-0.804676, spearman-0.756083
Traing Log at fold-0 epoch-23: mse-0.245851, rmse-0.495833, r2-0.636671
Valid at fold-0: mse-0.332274
Update best_mse, Valid at fold-0 epoch-23: mse-0.332274, rmse-0.576432, ci--1, r2-0.646551, pearson-0.807461, spearman-0.765929
Traing Log at fold-0 epoch-24: mse-0.240943, rmse-0.49086, r2-0.648157
Valid at fold-0: mse-0.348792
Traing Log at fold-0 epoch-25: mse-0.236325, rmse-0.486132, r2-0.656783
Valid at fold-0: mse-0.33754
Traing Log at fold-0 epoch-26: mse-0.229435, rmse-0.478994, r2-0.670811
Valid at fold-0: mse-0.336596
Traing Log at fold-0 epoch-27: mse-0.222188, rmse-0.471368, r2-0.683402
Valid at fold-0: mse-0.328264
Update best_mse, Valid at fold-0 epoch-27: mse-0.328264, rmse-0.572943, ci--1, r2-0.650817, pearson-0.80935, spearman-0.76405
Traing Log at fold-0 epoch-28: mse-0.220514, rmse-0.46959, r2-0.68675
Valid at fold-0: mse-0.332718
Traing Log at fold-0 epoch-29: mse-0.215636, rmse-0.464366, r2-0.695738
Valid at fold-0: mse-0.32613
Update best_mse, Valid at fold-0 epoch-29: mse-0.32613, rmse-0.571078, ci--1, r2-0.653088, pearson-0.813688, spearman-0.766922
Traing Log at fold-0 epoch-30: mse-0.208992, rmse-0.457157, r2-0.706527
Valid at fold-0: mse-0.32544
Update best_mse, Valid at fold-0 epoch-30: mse-0.32544, rmse-0.570474, ci--1, r2-0.653821, pearson-0.815249, spearman-0.76793
Traing Log at fold-0 epoch-31: mse-0.20211, rmse-0.449567, r2-0.72054
Valid at fold-0: mse-0.331023
Traing Log at fold-0 epoch-32: mse-0.197623, rmse-0.444548, r2-0.728446
Valid at fold-0: mse-0.332867
Traing Log at fold-0 epoch-33: mse-0.19547, rmse-0.44212, r2-0.7322
Valid at fold-0: mse-0.323537
Update best_mse, Valid at fold-0 epoch-33: mse-0.323537, rmse-0.568804, ci--1, r2-0.655845, pearson-0.818835, spearman-0.772943
Traing Log at fold-0 epoch-34: mse-0.191352, rmse-0.437438, r2-0.738789
Valid at fold-0: mse-0.335705
Traing Log at fold-0 epoch-35: mse-0.186721, rmse-0.432112, r2-0.747028
Valid at fold-0: mse-0.322221
Update best_mse, Valid at fold-0 epoch-35: mse-0.322221, rmse-0.567645, ci--1, r2-0.657245, pearson-0.815124, spearman-0.767505
Traing Log at fold-0 epoch-36: mse-0.181072, rmse-0.425526, r2-0.755995
Valid at fold-0: mse-0.335924
Traing Log at fold-0 epoch-37: mse-0.179173, rmse-0.423288, r2-0.759764
Valid at fold-0: mse-0.329114
Traing Log at fold-0 epoch-38: mse-0.176261, rmse-0.419835, r2-0.764882
Valid at fold-0: mse-0.33332
Traing Log at fold-0 epoch-39: mse-0.171121, rmse-0.413667, r2-0.772305
Valid at fold-0: mse-0.327281
Traing Log at fold-0 epoch-40: mse-0.168718, rmse-0.410753, r2-0.776149
Valid at fold-0: mse-0.331794
Traing Log at fold-0 epoch-41: mse-0.164021, rmse-0.404995, r2-0.784737
Valid at fold-0: mse-0.328294
Traing Log at fold-0 epoch-42: mse-0.164066, rmse-0.40505, r2-0.784389
Valid at fold-0: mse-0.330576
Traing Log at fold-0 epoch-43: mse-0.158471, rmse-0.398084, r2-0.792517
Valid at fold-0: mse-0.33357
Traing Log at fold-0 epoch-44: mse-0.159486, rmse-0.399357, r2-0.791766
Valid at fold-0: mse-0.329582
Traing Log at fold-0 epoch-45: mse-0.151478, rmse-0.389201, r2-0.804138
Valid at fold-0: mse-0.333945
Traing Log at fold-0 epoch-46: mse-0.15091, rmse-0.388471, r2-0.804575
Valid at fold-0: mse-0.328174
Traing Log at fold-0 epoch-47: mse-0.14711, rmse-0.383549, r2-0.810696
Valid at fold-0: mse-0.326594
Traing Log at fold-0 epoch-48: mse-0.143878, rmse-0.379312, r2-0.815841
Valid at fold-0: mse-0.32713
Traing Log at fold-0 epoch-49: mse-0.143033, rmse-0.378197, r2-0.817034
Valid at fold-0: mse-0.32726
Traing Log at fold-0 epoch-50: mse-0.140112, rmse-0.374315, r2-0.821612
Valid at fold-0: mse-0.333438
Traing Log at fold-0 epoch-51: mse-0.138747, rmse-0.372487, r2-0.82319
Valid at fold-0: mse-0.322427
Traing Log at fold-0 epoch-52: mse-0.136651, rmse-0.369664, r2-0.826789
Valid at fold-0: mse-0.337253
Traing Log at fold-0 epoch-53: mse-0.132417, rmse-0.363891, r2-0.832704
Valid at fold-0: mse-0.317818
Update best_mse, Valid at fold-0 epoch-53: mse-0.317818, rmse-0.563754, ci--1, r2-0.661929, pearson-0.821603, spearman-0.777312
Traing Log at fold-0 epoch-54: mse-0.132945, rmse-0.364617, r2-0.832305
Valid at fold-0: mse-0.318174
Traing Log at fold-0 epoch-55: mse-0.12932, rmse-0.359611, r2-0.837107
Valid at fold-0: mse-0.330671
Traing Log at fold-0 epoch-56: mse-0.129216, rmse-0.359467, r2-0.837788
Valid at fold-0: mse-0.330045
Traing Log at fold-0 epoch-57: mse-0.125999, rmse-0.354963, r2-0.842471
Valid at fold-0: mse-0.329232
Traing Log at fold-0 epoch-58: mse-0.123394, rmse-0.351274, r2-0.846175
Valid at fold-0: mse-0.332435
Traing Log at fold-0 epoch-59: mse-0.122964, rmse-0.350663, r2-0.84634
Valid at fold-0: mse-0.323216
Traing Log at fold-0 epoch-60: mse-0.116992, rmse-0.342041, r2-0.855205
Valid at fold-0: mse-0.331911
Traing Log at fold-0 epoch-61: mse-0.117908, rmse-0.343378, r2-0.853586
Valid at fold-0: mse-0.329974
Traing Log at fold-0 epoch-62: mse-0.1172, rmse-0.342345, r2-0.854958
Valid at fold-0: mse-0.320433
Traing Log at fold-0 epoch-63: mse-0.115826, rmse-0.340333, r2-0.856906
Valid at fold-0: mse-0.328585
Traing Log at fold-0 epoch-64: mse-0.112553, rmse-0.335489, r2-0.861397
Valid at fold-0: mse-0.327141
Traing Log at fold-0 epoch-65: mse-0.110934, rmse-0.333067, r2-0.8638
Valid at fold-0: mse-0.334345
Traing Log at fold-0 epoch-66: mse-0.110533, rmse-0.332465, r2-0.864245
Valid at fold-0: mse-0.331198
Traing Log at fold-0 epoch-67: mse-0.108426, rmse-0.329281, r2-0.86712
Valid at fold-0: mse-0.328305
Traing Log at fold-0 epoch-68: mse-0.107342, rmse-0.327631, r2-0.869137
Valid at fold-0: mse-0.329983
Traing Log at fold-0 epoch-69: mse-0.10506, rmse-0.32413, r2-0.871949
Valid at fold-0: mse-0.334265
Traing Log at fold-0 epoch-70: mse-0.10643, rmse-0.326237, r2-0.869682
Valid at fold-0: mse-0.327238
Traing Log at fold-0 epoch-71: mse-0.102885, rmse-0.320757, r2-0.874825
Valid at fold-0: mse-0.328559
Traing Log at fold-0 epoch-72: mse-0.102993, rmse-0.320925, r2-0.875
Valid at fold-0: mse-0.320274
Traing Log at fold-0 epoch-73: mse-0.099931, rmse-0.316119, r2-0.878672
Valid at fold-0: mse-0.326079
Traing Log at fold-0 epoch-74: mse-0.100611, rmse-0.317192, r2-0.87797
Valid at fold-0: mse-0.333631
Traing stop at epoch-74, model save at-./savemodel/metz-warm-fold0-Nov13_07-19-27.pth
Save log over at ./log/Nov13_07-19-27-metz-warm-fold0.csv

============================================================
Testing fold 0 with best model...
============================================================
/home/minhtrin/.conda/envs/LLMDTA/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Test at fold-0, mse: 0.331122, rmse: 0.575432, ci: 0.794314, r2: 0.631733, pearson: 0.805084, spearman: 0.757031

Fold 0 results saved to: ./log/Test-metz-warm-fold0-Nov13_07-19-27.csv
============================================================
Training fold 0 completed successfully!
============================================================
wandb: updating run metadata
wandb: updating run metadata; uploading output.log; uploading wandb-summary.json
wandb: updating run metadata; uploading wandb-summary.json
wandb: updating run metadata
wandb: uploading history steps 170-170, summary, console lines 186-191
wandb: 
wandb: Run history:
wandb:      best_valid/mse â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  best_valid/pearson â–â–„â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:       best_valid/r2 â–â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:     best_valid/rmse â–ˆâ–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: best_valid/spearman â–â–„â–…â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:             test/ci â–
wandb:            test/mse â–
wandb:        test/pearson â–
wandb:             test/r2 â–
wandb:                 +13 ...
wandb: 
wandb: Run summary:
wandb:      best_valid/mse 0.31782
wandb:  best_valid/pearson 0.8216
wandb:       best_valid/r2 0.66193
wandb:     best_valid/rmse 0.56375
wandb: best_valid/spearman 0.77731
wandb:               epoch 74
wandb:       final_test_ci 0.79431
wandb:      final_test_mse 0.33112
wandb:  final_test_pearson 0.80508
wandb:       final_test_r2 0.63173
wandb:                 +19 ...
wandb: 
wandb: ğŸš€ View run metz-warm-fold0 at: https://wandb.ai/tringuyen/LLMDTA/runs/1uj8ky1q
wandb: â­ï¸ View project at: https://wandb.ai/tringuyen/LLMDTA
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251113_071928-1uj8ky1q/logs
Weights & Biases run finished

Training for fold 0 completed successfully.
Python script exit code: 0
==========================================
End Time: Thu Nov 13 11:09:10 AM AEDT 2025
==========================================
