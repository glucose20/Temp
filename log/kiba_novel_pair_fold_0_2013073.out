==========================================
Job ID: 2013073
Array Task ID: 0
Node: v100l-f-06
Start Time: Thu Nov 13 07:15:42 AM AEDT 2025
==========================================
Activating conda environment...
Conda environment activated: LLMDTA
Checking GPU...
Thu Nov 13 07:15:43 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           On  |   00000000:06:00.0 Off |                    0 |
| N/A   28C    P0             42W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Setting environment variables...

Starting training for fold 0...


============================================================
Starting training for Fold 0
Dataset: kiba, Running Set: novel-pair
Epochs: 200, Batch Size: 16
============================================================

Executing: python -u code/train.py --fold 0 --cuda 0 --dataset kiba --running_set novel-pair --epochs 200 --batch_size 16 --wandb_project LLMDTA
============================================================
Training Fold 0/4
Dataset: kiba-novel-pair
Device: cuda (CUDA_VISIBLE_DEVICES=0)
Pretrain-./data/kiba/kiba_drug_pretrain.pkl
Pretrain-./data/kiba/kiba_esm_pretrain.pkl
============================================================
wandb: Currently logged in as: tringuyen to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: setting up run 08zwblao
wandb: Tracking run with wandb version 0.23.0
wandb: Run data is saved locally in /vast/minhtrin/DTA/Temp/wandb/run-20251113_071549-08zwblao
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run kiba-novel-pair-fold0
wandb: â­ï¸ View project at https://wandb.ai/tringuyen/LLMDTA
wandb: ğŸš€ View run at https://wandb.ai/tringuyen/LLMDTA/runs/08zwblao
Weights & Biases initialized: LLMDTA
Loading fold 0 data...
  Train: ./data/dta-5fold-dataset/kiba/novel-pair/fold_0_train.csv
  Valid: ./data/dta-5fold-dataset/kiba/novel-pair/fold_0_valid.csv
  Test:  ./data/dta-5fold-dataset/kiba/novel-pair/fold_0_test.csv
Dataset loaded: 40301 train, 57463 valid, 20490 test samples
/home/minhtrin/.conda/envs/LLMDTA/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Traing Log at fold-0 epoch-1: mse-3.884246, rmse-1.970849, r2--0.10272
Valid at fold-0: mse-1.373394
Update best_mse, Valid at fold-0 epoch-1: mse-1.373394, rmse-1.171919, ci--1, r2--0.953218, pearson-0.298945, spearman-0.289252
Traing Log at fold-0 epoch-2: mse-0.597246, rmse-0.772817, r2--0.341978
Valid at fold-0: mse-0.548337
Update best_mse, Valid at fold-0 epoch-2: mse-0.548337, rmse-0.740498, ci--1, r2-0.220165, pearson-0.530273, spearman-0.510131
Traing Log at fold-0 epoch-3: mse-0.459574, rmse-0.677919, r2--0.22138
Valid at fold-0: mse-0.492416
Update best_mse, Valid at fold-0 epoch-3: mse-0.492416, rmse-0.701724, ci--1, r2-0.299694, pearson-0.560867, spearman-0.512204
Traing Log at fold-0 epoch-4: mse-0.391572, rmse-0.625757, r2--0.066598
Valid at fold-0: mse-0.484368
Update best_mse, Valid at fold-0 epoch-4: mse-0.484368, rmse-0.695966, ci--1, r2-0.31114, pearson-0.624858, spearman-0.563732
Traing Log at fold-0 epoch-5: mse-0.375759, rmse-0.612992, r2--0.006743
Valid at fold-0: mse-0.408585
Update best_mse, Valid at fold-0 epoch-5: mse-0.408585, rmse-0.639207, ci--1, r2-0.418917, pearson-0.650024, spearman-0.586886
Traing Log at fold-0 epoch-6: mse-0.323307, rmse-0.568601, r2-0.130216
Valid at fold-0: mse-0.404002
Update best_mse, Valid at fold-0 epoch-6: mse-0.404002, rmse-0.635612, ci--1, r2-0.425435, pearson-0.657633, spearman-0.593416
Traing Log at fold-0 epoch-7: mse-0.307502, rmse-0.554529, r2-0.193957
Valid at fold-0: mse-0.398184
Update best_mse, Valid at fold-0 epoch-7: mse-0.398184, rmse-0.631018, ci--1, r2-0.433709, pearson-0.668325, spearman-0.617706
Traing Log at fold-0 epoch-8: mse-0.290359, rmse-0.538849, r2-0.262816
Valid at fold-0: mse-0.380516
Update best_mse, Valid at fold-0 epoch-8: mse-0.380516, rmse-0.61686, ci--1, r2-0.458837, pearson-0.680494, spearman-0.64543
Traing Log at fold-0 epoch-9: mse-0.274908, rmse-0.524317, r2-0.323823
Valid at fold-0: mse-0.395615
Traing Log at fold-0 epoch-10: mse-0.25984, rmse-0.509745, r2-0.38364
Valid at fold-0: mse-0.365243
Update best_mse, Valid at fold-0 epoch-10: mse-0.365243, rmse-0.604354, ci--1, r2-0.480557, pearson-0.69379, spearman-0.662394
Traing Log at fold-0 epoch-11: mse-0.248913, rmse-0.498912, r2-0.421042
Valid at fold-0: mse-0.356158
Update best_mse, Valid at fold-0 epoch-11: mse-0.356158, rmse-0.59679, ci--1, r2-0.493478, pearson-0.704352, spearman-0.653302
Traing Log at fold-0 epoch-12: mse-0.237075, rmse-0.486904, r2-0.462855
Valid at fold-0: mse-0.362254
Traing Log at fold-0 epoch-13: mse-0.229053, rmse-0.478594, r2-0.491685
Valid at fold-0: mse-0.348829
Update best_mse, Valid at fold-0 epoch-13: mse-0.348829, rmse-0.590618, ci--1, r2-0.503901, pearson-0.713059, spearman-0.673876
Traing Log at fold-0 epoch-14: mse-0.217722, rmse-0.466607, r2-0.528842
Valid at fold-0: mse-0.362616
Traing Log at fold-0 epoch-15: mse-0.21401, rmse-0.462613, r2-0.540279
Valid at fold-0: mse-0.370913
Traing Log at fold-0 epoch-16: mse-0.205307, rmse-0.453108, r2-0.567585
Valid at fold-0: mse-0.355286
Traing Log at fold-0 epoch-17: mse-0.199711, rmse-0.446891, r2-0.583264
Valid at fold-0: mse-0.334947
Update best_mse, Valid at fold-0 epoch-17: mse-0.334947, rmse-0.578746, ci--1, r2-0.523643, pearson-0.729307, spearman-0.681001
Traing Log at fold-0 epoch-18: mse-0.194398, rmse-0.440906, r2-0.600614
Valid at fold-0: mse-0.330255
Update best_mse, Valid at fold-0 epoch-18: mse-0.330255, rmse-0.574678, ci--1, r2-0.530318, pearson-0.729912, spearman-0.687739
Traing Log at fold-0 epoch-19: mse-0.189574, rmse-0.435401, r2-0.612731
Valid at fold-0: mse-0.324486
Update best_mse, Valid at fold-0 epoch-19: mse-0.324486, rmse-0.569636, ci--1, r2-0.538522, pearson-0.73787, spearman-0.69699
Traing Log at fold-0 epoch-20: mse-0.182593, rmse-0.427309, r2-0.631033
Valid at fold-0: mse-0.329017
Traing Log at fold-0 epoch-21: mse-0.177289, rmse-0.421057, r2-0.646592
Valid at fold-0: mse-0.335965
Traing Log at fold-0 epoch-22: mse-0.173653, rmse-0.416717, r2-0.655854
Valid at fold-0: mse-0.333048
Traing Log at fold-0 epoch-23: mse-0.168886, rmse-0.410958, r2-0.667376
Valid at fold-0: mse-0.343711
Traing Log at fold-0 epoch-24: mse-0.164518, rmse-0.405608, r2-0.681424
Valid at fold-0: mse-0.328236
Traing Log at fold-0 epoch-25: mse-0.161905, rmse-0.402374, r2-0.686872
Valid at fold-0: mse-0.324952
Traing Log at fold-0 epoch-26: mse-0.157683, rmse-0.397093, r2-0.696969
Valid at fold-0: mse-0.31112
Update best_mse, Valid at fold-0 epoch-26: mse-0.31112, rmse-0.557782, ci--1, r2-0.55753, pearson-0.751376, spearman-0.701697
Traing Log at fold-0 epoch-27: mse-0.153798, rmse-0.392171, r2-0.706785
Valid at fold-0: mse-0.323751
Traing Log at fold-0 epoch-28: mse-0.152154, rmse-0.390069, r2-0.71119
Valid at fold-0: mse-0.323156
Traing Log at fold-0 epoch-29: mse-0.148462, rmse-0.385308, r2-0.719562
Valid at fold-0: mse-0.311021
Update best_mse, Valid at fold-0 epoch-29: mse-0.311021, rmse-0.557693, ci--1, r2-0.557671, pearson-0.754646, spearman-0.704394
Traing Log at fold-0 epoch-30: mse-0.144337, rmse-0.379918, r2-0.730261
Valid at fold-0: mse-0.330858
Traing Log at fold-0 epoch-31: mse-0.142068, rmse-0.376919, r2-0.734958
Valid at fold-0: mse-0.317525
Traing Log at fold-0 epoch-32: mse-0.138697, rmse-0.372421, r2-0.742832
Valid at fold-0: mse-0.317707
Traing Log at fold-0 epoch-33: mse-0.133933, rmse-0.365968, r2-0.753611
Valid at fold-0: mse-0.315236
Traing Log at fold-0 epoch-34: mse-0.132983, rmse-0.364669, r2-0.756163
Valid at fold-0: mse-0.302726
Update best_mse, Valid at fold-0 epoch-34: mse-0.302726, rmse-0.550205, ci--1, r2-0.569468, pearson-0.758041, spearman-0.706192
Traing Log at fold-0 epoch-35: mse-0.129593, rmse-0.359991, r2-0.763293
Valid at fold-0: mse-0.318017
Traing Log at fold-0 epoch-36: mse-0.127359, rmse-0.356874, r2-0.768752
Valid at fold-0: mse-0.317827
Traing Log at fold-0 epoch-37: mse-0.124834, rmse-0.353319, r2-0.774718
Valid at fold-0: mse-0.312804
Traing Log at fold-0 epoch-38: mse-0.123409, rmse-0.351296, r2-0.777256
Valid at fold-0: mse-0.317607
Traing Log at fold-0 epoch-39: mse-0.120285, rmse-0.346821, r2-0.785136
Valid at fold-0: mse-0.305104
Traing Log at fold-0 epoch-40: mse-0.11842, rmse-0.344123, r2-0.788449
Valid at fold-0: mse-0.302201
Update best_mse, Valid at fold-0 epoch-40: mse-0.302201, rmse-0.549729, ci--1, r2-0.570214, pearson-0.760918, spearman-0.708272
Traing Log at fold-0 epoch-41: mse-0.11712, rmse-0.342228, r2-0.791267
Valid at fold-0: mse-0.306056
Traing Log at fold-0 epoch-42: mse-0.113408, rmse-0.336761, r2-0.799309
Valid at fold-0: mse-0.29891
Update best_mse, Valid at fold-0 epoch-42: mse-0.29891, rmse-0.546727, ci--1, r2-0.574895, pearson-0.761668, spearman-0.712446
Traing Log at fold-0 epoch-43: mse-0.111673, rmse-0.334175, r2-0.802674
Valid at fold-0: mse-0.300794
Traing Log at fold-0 epoch-44: mse-0.110605, rmse-0.332573, r2-0.805334
Valid at fold-0: mse-0.296909
Update best_mse, Valid at fold-0 epoch-44: mse-0.296909, rmse-0.544894, ci--1, r2-0.577741, pearson-0.765839, spearman-0.71364
Traing Log at fold-0 epoch-45: mse-0.108153, rmse-0.328867, r2-0.810395
Valid at fold-0: mse-0.298501
Traing Log at fold-0 epoch-46: mse-0.106595, rmse-0.326489, r2-0.813313
Valid at fold-0: mse-0.302774
Traing Log at fold-0 epoch-47: mse-0.104234, rmse-0.322852, r2-0.818243
Valid at fold-0: mse-0.299821
Traing Log at fold-0 epoch-48: mse-0.102529, rmse-0.320202, r2-0.821782
Valid at fold-0: mse-0.301202
Traing Log at fold-0 epoch-49: mse-0.101301, rmse-0.318278, r2-0.824903
Valid at fold-0: mse-0.2946
Update best_mse, Valid at fold-0 epoch-49: mse-0.2946, rmse-0.54277, ci--1, r2-0.581026, pearson-0.768777, spearman-0.714415
Traing Log at fold-0 epoch-50: mse-0.099042, rmse-0.31471, r2-0.829235
Valid at fold-0: mse-0.289526
Update best_mse, Valid at fold-0 epoch-50: mse-0.289526, rmse-0.538076, ci--1, r2-0.588241, pearson-0.774642, spearman-0.721166
Traing Log at fold-0 epoch-51: mse-0.098588, rmse-0.313988, r2-0.829683
Valid at fold-0: mse-0.302765
Traing Log at fold-0 epoch-52: mse-0.097015, rmse-0.311472, r2-0.833027
Valid at fold-0: mse-0.297968
Traing Log at fold-0 epoch-53: mse-0.095703, rmse-0.309359, r2-0.836102
Valid at fold-0: mse-0.293258
Traing Log at fold-0 epoch-54: mse-0.093357, rmse-0.305545, r2-0.840393
Valid at fold-0: mse-0.291558
Traing Log at fold-0 epoch-55: mse-0.092251, rmse-0.303728, r2-0.842756
Valid at fold-0: mse-0.300052
Traing Log at fold-0 epoch-56: mse-0.09044, rmse-0.300732, r2-0.845703
Valid at fold-0: mse-0.290833
Traing Log at fold-0 epoch-57: mse-0.089627, rmse-0.299378, r2-0.84782
Valid at fold-0: mse-0.295379
Traing Log at fold-0 epoch-58: mse-0.088465, rmse-0.29743, r2-0.850165
Valid at fold-0: mse-0.293324
Traing Log at fold-0 epoch-59: mse-0.088205, rmse-0.296994, r2-0.850372
Valid at fold-0: mse-0.294041
Traing Log at fold-0 epoch-60: mse-0.085867, rmse-0.293031, r2-0.855065
Valid at fold-0: mse-0.288781
Update best_mse, Valid at fold-0 epoch-60: mse-0.288781, rmse-0.537383, ci--1, r2-0.589301, pearson-0.773942, spearman-0.728569
Traing Log at fold-0 epoch-61: mse-0.085292, rmse-0.292048, r2-0.856173
Valid at fold-0: mse-0.298745
Traing Log at fold-0 epoch-62: mse-0.083564, rmse-0.289075, r2-0.859715
Valid at fold-0: mse-0.285059
Update best_mse, Valid at fold-0 epoch-62: mse-0.285059, rmse-0.533909, ci--1, r2-0.594594, pearson-0.775894, spearman-0.729194
Traing Log at fold-0 epoch-63: mse-0.080842, rmse-0.284328, r2-0.864565
Valid at fold-0: mse-0.292184
Traing Log at fold-0 epoch-64: mse-0.081231, rmse-0.28501, r2-0.86405
Valid at fold-0: mse-0.285936
Traing Log at fold-0 epoch-65: mse-0.079968, rmse-0.282785, r2-0.866339
Valid at fold-0: mse-0.281834
Update best_mse, Valid at fold-0 epoch-65: mse-0.281834, rmse-0.53088, ci--1, r2-0.599181, pearson-0.779363, spearman-0.731632
Traing Log at fold-0 epoch-66: mse-0.079048, rmse-0.281154, r2-0.86833
Valid at fold-0: mse-0.287973
Traing Log at fold-0 epoch-67: mse-0.078349, rmse-0.279909, r2-0.869558
Valid at fold-0: mse-0.287277
Traing Log at fold-0 epoch-68: mse-0.076528, rmse-0.276638, r2-0.872666
Valid at fold-0: mse-0.290803
Traing Log at fold-0 epoch-69: mse-0.075558, rmse-0.274878, r2-0.874832
Valid at fold-0: mse-0.291202
Traing Log at fold-0 epoch-70: mse-0.074985, rmse-0.273833, r2-0.875624
Valid at fold-0: mse-0.284952
Traing Log at fold-0 epoch-71: mse-0.073718, rmse-0.27151, r2-0.878205
Valid at fold-0: mse-0.289965
Traing Log at fold-0 epoch-72: mse-0.073801, rmse-0.271663, r2-0.877956
Valid at fold-0: mse-0.294578
Traing Log at fold-0 epoch-73: mse-0.072325, rmse-0.268934, r2-0.8806
Valid at fold-0: mse-0.287026
Traing Log at fold-0 epoch-74: mse-0.071339, rmse-0.267093, r2-0.88251
Valid at fold-0: mse-0.284915
Traing Log at fold-0 epoch-75: mse-0.071009, rmse-0.266476, r2-0.883179
Valid at fold-0: mse-0.284374
Traing Log at fold-0 epoch-76: mse-0.069542, rmse-0.263709, r2-0.885664
Valid at fold-0: mse-0.285748
Traing Log at fold-0 epoch-77: mse-0.068113, rmse-0.260984, r2-0.888342
Valid at fold-0: mse-0.29548
Traing Log at fold-0 epoch-78: mse-0.068902, rmse-0.262492, r2-0.887002
Valid at fold-0: mse-0.289792
Traing Log at fold-0 epoch-79: mse-0.067739, rmse-0.260267, r2-0.889194
Valid at fold-0: mse-0.285125
Traing Log at fold-0 epoch-80: mse-0.06603, rmse-0.256963, r2-0.89193
Valid at fold-0: mse-0.287896
Traing Log at fold-0 epoch-81: mse-0.064996, rmse-0.254944, r2-0.893974
Valid at fold-0: mse-0.29151
Traing Log at fold-0 epoch-82: mse-0.065949, rmse-0.256806, r2-0.892234
Valid at fold-0: mse-0.283847
Traing Log at fold-0 epoch-83: mse-0.063536, rmse-0.252063, r2-0.896852
Valid at fold-0: mse-0.285508
Traing Log at fold-0 epoch-84: mse-0.063677, rmse-0.252342, r2-0.89638
Valid at fold-0: mse-0.282307
Traing Log at fold-0 epoch-85: mse-0.061913, rmse-0.248823, r2-0.899371
Valid at fold-0: mse-0.283646
Traing Log at fold-0 epoch-86: mse-0.06119, rmse-0.247367, r2-0.900783
Valid at fold-0: mse-0.29427
Traing stop at epoch-86, model save at-./savemodel/kiba-novel-pair-fold0-Nov13_07-15-48.pth
Save log over at ./log/Nov13_07-15-48-kiba-novel-pair-fold0.csv

============================================================
Testing fold 0 with best model...
============================================================
/home/minhtrin/.conda/envs/LLMDTA/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Test at fold-0, mse: 0.519007, rmse: 0.720421, ci: 0.679349, r2: 0.278312, pearson: 0.547971, spearman: 0.47994

Fold 0 results saved to: ./log/Test-kiba-novel-pair-fold0-Nov13_07-15-48.csv
============================================================
Training fold 0 completed successfully!
============================================================
wandb: updating run metadata
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading history steps 197-197, summary, console lines 213-218
wandb: 
wandb: Run history:
wandb:      best_valid/mse â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  best_valid/pearson â–â–„â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:       best_valid/r2 â–â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:     best_valid/rmse â–ˆâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: best_valid/spearman â–â–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             test/ci â–
wandb:            test/mse â–
wandb:        test/pearson â–
wandb:             test/r2 â–
wandb:                 +13 ...
wandb: 
wandb: Run summary:
wandb:      best_valid/mse 0.28183
wandb:  best_valid/pearson 0.77936
wandb:       best_valid/r2 0.59918
wandb:     best_valid/rmse 0.53088
wandb: best_valid/spearman 0.73163
wandb:               epoch 86
wandb:       final_test_ci 0.67935
wandb:      final_test_mse 0.51901
wandb:  final_test_pearson 0.54797
wandb:       final_test_r2 0.27831
wandb:                 +19 ...
wandb: 
wandb: ğŸš€ View run kiba-novel-pair-fold0 at: https://wandb.ai/tringuyen/LLMDTA/runs/08zwblao
wandb: â­ï¸ View project at: https://wandb.ai/tringuyen/LLMDTA
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251113_071549-08zwblao/logs
Weights & Biases run finished

Training for fold 0 completed successfully.
Python script exit code: 0
==========================================
End Time: Thu Nov 13 10:30:12 PM AEDT 2025
==========================================
