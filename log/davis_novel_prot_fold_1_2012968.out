==========================================
Job ID: 2012968
Array Task ID: 1
Node: v100-f-21
Start Time: Wed Nov 12 04:42:40 PM AEDT 2025
==========================================
Activating conda environment...
Conda environment activated: LLMDTA
Checking GPU...
Wed Nov 12 16:42:40 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           On  |   00000000:AF:00.0 Off |                    0 |
| N/A   37C    P0             44W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Setting environment variables...

Starting training for fold 1...


============================================================
Starting training for Fold 1
Dataset: davis, Running Set: novel-prot
Epochs: 200, Batch Size: 16
============================================================

Executing: python -u code/train.py --fold 1 --cuda 0 --dataset davis --running_set novel-prot --epochs 200 --batch_size 16 --wandb_project LLMDTA
============================================================
Training Fold 1/4
Dataset: davis-novel-prot
Device: cuda (CUDA_VISIBLE_DEVICES=0)
Pretrain-./data/davis/davis_drug_pretrain.pkl
Pretrain-./data/davis/davis_esm_pretrain.pkl
============================================================
wandb: Currently logged in as: tringuyen to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: setting up run 9wqpgh1k
wandb: Tracking run with wandb version 0.23.0
wandb: Run data is saved locally in /vast/minhtrin/DTA/Temp/wandb/run-20251112_164248-9wqpgh1k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run davis-novel-prot-fold1
wandb: â­ï¸ View project at https://wandb.ai/tringuyen/LLMDTA
wandb: ğŸš€ View run at https://wandb.ai/tringuyen/LLMDTA/runs/9wqpgh1k
Weights & Biases initialized: LLMDTA
Loading fold 1 data...
  Train: ./data/dta-5fold-dataset/davis/novel-prot/fold_1_train.csv
  Valid: ./data/dta-5fold-dataset/davis/novel-prot/fold_1_valid.csv
  Test:  ./data/dta-5fold-dataset/davis/novel-prot/fold_1_test.csv
Dataset loaded: 19257 train, 4815 valid, 5984 test samples
/home/minhtrin/.conda/envs/LLMDTA/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Traing Log at fold-1 epoch-1: mse-1.770633, rmse-1.330651, r2--0.225495
Valid at fold-1: mse-1.46528
Update best_mse, Valid at fold-1 epoch-1: mse-1.46528, rmse-1.210487, ci--1, r2--0.899289, pearson-0.505597, spearman-0.458088
Traing Log at fold-1 epoch-2: mse-0.691142, rmse-0.831349, r2--0.28195
Valid at fold-1: mse-0.883878
Update best_mse, Valid at fold-1 epoch-2: mse-0.883878, rmse-0.940148, ci--1, r2--0.145679, pearson-0.554263, spearman-0.485118
Traing Log at fold-1 epoch-3: mse-0.554087, rmse-0.74437, r2--0.163196
Valid at fold-1: mse-1.015662
Traing Log at fold-1 epoch-4: mse-0.475642, rmse-0.689668, r2--0.043408
Valid at fold-1: mse-0.459105
Update best_mse, Valid at fold-1 epoch-4: mse-0.459105, rmse-0.677573, ci--1, r2-0.40491, pearson-0.661413, spearman-0.588927
Traing Log at fold-1 epoch-5: mse-0.416553, rmse-0.645409, r2-0.054623
Valid at fold-1: mse-0.435503
Update best_mse, Valid at fold-1 epoch-5: mse-0.435503, rmse-0.659926, ci--1, r2-0.435503, pearson-0.70994, spearman-0.585031
Traing Log at fold-1 epoch-6: mse-0.380584, rmse-0.616915, r2-0.136476
Valid at fold-1: mse-0.435518
Traing Log at fold-1 epoch-7: mse-0.354552, rmse-0.595443, r2-0.201851
Valid at fold-1: mse-0.386128
Update best_mse, Valid at fold-1 epoch-7: mse-0.386128, rmse-0.621392, ci--1, r2-0.499502, pearson-0.741507, spearman-0.622013
Traing Log at fold-1 epoch-8: mse-0.332485, rmse-0.576616, r2-0.265233
Valid at fold-1: mse-0.318566
Update best_mse, Valid at fold-1 epoch-8: mse-0.318566, rmse-0.564416, ci--1, r2-0.587076, pearson-0.768012, spearman-0.638751
Traing Log at fold-1 epoch-9: mse-0.314085, rmse-0.560433, r2-0.319542
Valid at fold-1: mse-0.361242
Traing Log at fold-1 epoch-10: mse-0.294423, rmse-0.542608, r2-0.382387
Valid at fold-1: mse-0.30793
Update best_mse, Valid at fold-1 epoch-10: mse-0.30793, rmse-0.554915, ci--1, r2-0.600862, pearson-0.778062, spearman-0.637013
Traing Log at fold-1 epoch-11: mse-0.283177, rmse-0.532143, r2-0.414653
Valid at fold-1: mse-0.288464
Update best_mse, Valid at fold-1 epoch-11: mse-0.288464, rmse-0.537088, ci--1, r2-0.626095, pearson-0.792052, spearman-0.644258
Traing Log at fold-1 epoch-12: mse-0.270221, rmse-0.519828, r2-0.450372
Valid at fold-1: mse-0.295041
Traing Log at fold-1 epoch-13: mse-0.256891, rmse-0.506844, r2-0.490806
Valid at fold-1: mse-0.265215
Update best_mse, Valid at fold-1 epoch-13: mse-0.265215, rmse-0.51499, ci--1, r2-0.656229, pearson-0.810548, spearman-0.654057
Traing Log at fold-1 epoch-14: mse-0.246971, rmse-0.496962, r2-0.515799
Valid at fold-1: mse-0.293987
Traing Log at fold-1 epoch-15: mse-0.238982, rmse-0.488858, r2-0.541047
Valid at fold-1: mse-0.260362
Update best_mse, Valid at fold-1 epoch-15: mse-0.260362, rmse-0.510257, ci--1, r2-0.66252, pearson-0.814401, spearman-0.660452
Traing Log at fold-1 epoch-16: mse-0.227921, rmse-0.477411, r2-0.565275
Valid at fold-1: mse-0.278221
Traing Log at fold-1 epoch-17: mse-0.225569, rmse-0.474941, r2-0.577049
Valid at fold-1: mse-0.272129
Traing Log at fold-1 epoch-18: mse-0.215339, rmse-0.464047, r2-0.602887
Valid at fold-1: mse-0.262758
Traing Log at fold-1 epoch-19: mse-0.209017, rmse-0.457184, r2-0.618081
Valid at fold-1: mse-0.253991
Update best_mse, Valid at fold-1 epoch-19: mse-0.253991, rmse-0.503976, ci--1, r2-0.670778, pearson-0.819941, spearman-0.673545
Traing Log at fold-1 epoch-20: mse-0.202308, rmse-0.449786, r2-0.633868
Valid at fold-1: mse-0.259493
Traing Log at fold-1 epoch-21: mse-0.199864, rmse-0.447061, r2-0.638376
Valid at fold-1: mse-0.272225
Traing Log at fold-1 epoch-22: mse-0.19311, rmse-0.439443, r2-0.655746
Valid at fold-1: mse-0.260327
Traing Log at fold-1 epoch-23: mse-0.18062, rmse-0.424994, r2-0.686124
Valid at fold-1: mse-0.233009
Update best_mse, Valid at fold-1 epoch-23: mse-0.233009, rmse-0.48271, ci--1, r2-0.697974, pearson-0.836008, spearman-0.674999
Traing Log at fold-1 epoch-24: mse-0.179654, rmse-0.423856, r2-0.687866
Valid at fold-1: mse-0.250646
Traing Log at fold-1 epoch-25: mse-0.175791, rmse-0.419274, r2-0.695801
Valid at fold-1: mse-0.249689
Traing Log at fold-1 epoch-26: mse-0.172083, rmse-0.414829, r2-0.705316
Valid at fold-1: mse-0.244793
Traing Log at fold-1 epoch-27: mse-0.169688, rmse-0.411932, r2-0.711334
Valid at fold-1: mse-0.2393
Traing Log at fold-1 epoch-28: mse-0.166523, rmse-0.408072, r2-0.715844
Valid at fold-1: mse-0.240325
Traing Log at fold-1 epoch-29: mse-0.159196, rmse-0.398993, r2-0.733243
Valid at fold-1: mse-0.242416
Traing Log at fold-1 epoch-30: mse-0.161717, rmse-0.402141, r2-0.728553
Valid at fold-1: mse-0.238878
Traing Log at fold-1 epoch-31: mse-0.153135, rmse-0.391324, r2-0.745224
Valid at fold-1: mse-0.235956
Traing Log at fold-1 epoch-32: mse-0.151052, rmse-0.388654, r2-0.75094
Valid at fold-1: mse-0.243144
Traing Log at fold-1 epoch-33: mse-0.147941, rmse-0.384631, r2-0.755391
Valid at fold-1: mse-0.233103
Traing Log at fold-1 epoch-34: mse-0.147597, rmse-0.384183, r2-0.756945
Valid at fold-1: mse-0.259302
Traing Log at fold-1 epoch-35: mse-0.143751, rmse-0.379145, r2-0.764922
Valid at fold-1: mse-0.243881
Traing Log at fold-1 epoch-36: mse-0.141331, rmse-0.37594, r2-0.769829
Valid at fold-1: mse-0.245261
Traing Log at fold-1 epoch-37: mse-0.136126, rmse-0.368952, r2-0.779664
Valid at fold-1: mse-0.242329
Traing Log at fold-1 epoch-38: mse-0.13469, rmse-0.367002, r2-0.783786
Valid at fold-1: mse-0.253606
Traing Log at fold-1 epoch-39: mse-0.134349, rmse-0.366536, r2-0.783576
Valid at fold-1: mse-0.231727
Update best_mse, Valid at fold-1 epoch-39: mse-0.231727, rmse-0.48138, ci--1, r2-0.699637, pearson-0.839966, spearman-0.672008
Traing Log at fold-1 epoch-40: mse-0.131575, rmse-0.362733, r2-0.787525
Valid at fold-1: mse-0.250609
Traing Log at fold-1 epoch-41: mse-0.12868, rmse-0.358719, r2-0.795256
Valid at fold-1: mse-0.239182
Traing Log at fold-1 epoch-42: mse-0.127693, rmse-0.357341, r2-0.796442
Valid at fold-1: mse-0.235219
Traing Log at fold-1 epoch-43: mse-0.125458, rmse-0.354201, r2-0.801162
Valid at fold-1: mse-0.226954
Update best_mse, Valid at fold-1 epoch-43: mse-0.226954, rmse-0.476397, ci--1, r2-0.705824, pearson-0.841662, spearman-0.675624
Traing Log at fold-1 epoch-44: mse-0.122002, rmse-0.349287, r2-0.805945
Valid at fold-1: mse-0.249121
Traing Log at fold-1 epoch-45: mse-0.123269, rmse-0.351097, r2-0.806265
Valid at fold-1: mse-0.241011
Traing Log at fold-1 epoch-46: mse-0.121688, rmse-0.348837, r2-0.806609
Valid at fold-1: mse-0.243143
Traing Log at fold-1 epoch-47: mse-0.118068, rmse-0.34361, r2-0.815318
Valid at fold-1: mse-0.229068
Traing Log at fold-1 epoch-48: mse-0.117688, rmse-0.343057, r2-0.815857
Valid at fold-1: mse-0.234317
Traing Log at fold-1 epoch-49: mse-0.114696, rmse-0.338669, r2-0.820031
Valid at fold-1: mse-0.236433
Traing Log at fold-1 epoch-50: mse-0.113877, rmse-0.337457, r2-0.821968
Valid at fold-1: mse-0.226512
Update best_mse, Valid at fold-1 epoch-50: mse-0.226512, rmse-0.475932, ci--1, r2-0.706397, pearson-0.84362, spearman-0.679162
Traing Log at fold-1 epoch-51: mse-0.11192, rmse-0.334545, r2-0.826351
Valid at fold-1: mse-0.235778
Traing Log at fold-1 epoch-52: mse-0.111294, rmse-0.333608, r2-0.826449
Valid at fold-1: mse-0.23963
Traing Log at fold-1 epoch-53: mse-0.110883, rmse-0.332991, r2-0.828503
Valid at fold-1: mse-0.231388
Traing Log at fold-1 epoch-54: mse-0.108246, rmse-0.329008, r2-0.832401
Valid at fold-1: mse-0.225218
Update best_mse, Valid at fold-1 epoch-54: mse-0.225218, rmse-0.474571, ci--1, r2-0.708074, pearson-0.845064, spearman-0.678221
Traing Log at fold-1 epoch-55: mse-0.106722, rmse-0.326683, r2-0.835744
Valid at fold-1: mse-0.238175
Traing Log at fold-1 epoch-56: mse-0.10621, rmse-0.325898, r2-0.836752
Valid at fold-1: mse-0.238432
Traing Log at fold-1 epoch-57: mse-0.106607, rmse-0.326507, r2-0.835273
Valid at fold-1: mse-0.233246
Traing Log at fold-1 epoch-58: mse-0.104134, rmse-0.322699, r2-0.840024
Valid at fold-1: mse-0.231506
Traing Log at fold-1 epoch-59: mse-0.104633, rmse-0.323471, r2-0.839103
Valid at fold-1: mse-0.230649
Traing Log at fold-1 epoch-60: mse-0.102064, rmse-0.319474, r2-0.843496
Valid at fold-1: mse-0.236769
Traing Log at fold-1 epoch-61: mse-0.09753, rmse-0.312298, r2-0.851727
Valid at fold-1: mse-0.231204
Traing Log at fold-1 epoch-62: mse-0.10008, rmse-0.316355, r2-0.847012
Valid at fold-1: mse-0.235914
Traing Log at fold-1 epoch-63: mse-0.097581, rmse-0.31238, r2-0.851876
Valid at fold-1: mse-0.223632
Update best_mse, Valid at fold-1 epoch-63: mse-0.223632, rmse-0.472897, ci--1, r2-0.71013, pearson-0.844121, spearman-0.679519
Traing Log at fold-1 epoch-64: mse-0.099152, rmse-0.314883, r2-0.84902
Valid at fold-1: mse-0.235976
Traing Log at fold-1 epoch-65: mse-0.094631, rmse-0.307621, r2-0.855913
Valid at fold-1: mse-0.230819
Traing Log at fold-1 epoch-66: mse-0.096885, rmse-0.311264, r2-0.853166
Valid at fold-1: mse-0.23568
Traing Log at fold-1 epoch-67: mse-0.094963, rmse-0.308161, r2-0.855904
Valid at fold-1: mse-0.227256
Traing Log at fold-1 epoch-68: mse-0.093834, rmse-0.306323, r2-0.858125
Valid at fold-1: mse-0.229817
Traing Log at fold-1 epoch-69: mse-0.093336, rmse-0.305509, r2-0.858248
Valid at fold-1: mse-0.230949
Traing Log at fold-1 epoch-70: mse-0.090943, rmse-0.301567, r2-0.863349
Valid at fold-1: mse-0.224974
Traing Log at fold-1 epoch-71: mse-0.090199, rmse-0.300332, r2-0.864221
Valid at fold-1: mse-0.231358
Traing Log at fold-1 epoch-72: mse-0.090514, rmse-0.300856, r2-0.863341
Valid at fold-1: mse-0.230948
Traing Log at fold-1 epoch-73: mse-0.088894, rmse-0.298151, r2-0.866683
Valid at fold-1: mse-0.223679
Traing Log at fold-1 epoch-74: mse-0.088241, rmse-0.297054, r2-0.867155
Valid at fold-1: mse-0.23101
Traing Log at fold-1 epoch-75: mse-0.085587, rmse-0.292552, r2-0.872289
Valid at fold-1: mse-0.220975
Update best_mse, Valid at fold-1 epoch-75: mse-0.220975, rmse-0.470079, ci--1, r2-0.713574, pearson-0.850093, spearman-0.676644
Traing Log at fold-1 epoch-76: mse-0.087176, rmse-0.295256, r2-0.869183
Valid at fold-1: mse-0.23122
Traing Log at fold-1 epoch-77: mse-0.084703, rmse-0.291038, r2-0.873929
Valid at fold-1: mse-0.223678
Traing Log at fold-1 epoch-78: mse-0.086644, rmse-0.294354, r2-0.870107
Valid at fold-1: mse-0.229
Traing Log at fold-1 epoch-79: mse-0.085135, rmse-0.29178, r2-0.872499
Valid at fold-1: mse-0.224865
Traing Log at fold-1 epoch-80: mse-0.082905, rmse-0.287931, r2-0.876735
Valid at fold-1: mse-0.224873
Traing Log at fold-1 epoch-81: mse-0.082057, rmse-0.286456, r2-0.878192
Valid at fold-1: mse-0.223062
Traing Log at fold-1 epoch-82: mse-0.083236, rmse-0.288506, r2-0.875787
Valid at fold-1: mse-0.227748
Traing Log at fold-1 epoch-83: mse-0.081122, rmse-0.284819, r2-0.879986
Valid at fold-1: mse-0.222394
Traing Log at fold-1 epoch-84: mse-0.08192, rmse-0.286218, r2-0.877939
Valid at fold-1: mse-0.223587
Traing Log at fold-1 epoch-85: mse-0.080768, rmse-0.284197, r2-0.880269
Valid at fold-1: mse-0.229112
Traing Log at fold-1 epoch-86: mse-0.079256, rmse-0.281524, r2-0.882411
Valid at fold-1: mse-0.222473
Traing Log at fold-1 epoch-87: mse-0.081186, rmse-0.284931, r2-0.879658
Valid at fold-1: mse-0.227328
Traing Log at fold-1 epoch-88: mse-0.078735, rmse-0.280597, r2-0.883273
Valid at fold-1: mse-0.225712
Traing Log at fold-1 epoch-89: mse-0.078792, rmse-0.280699, r2-0.883555
Valid at fold-1: mse-0.219338
Update best_mse, Valid at fold-1 epoch-89: mse-0.219338, rmse-0.468335, ci--1, r2-0.715696, pearson-0.847252, spearman-0.676339
Traing Log at fold-1 epoch-90: mse-0.078702, rmse-0.280539, r2-0.883105
Valid at fold-1: mse-0.229288
Traing Log at fold-1 epoch-91: mse-0.076958, rmse-0.277414, r2-0.886583
Valid at fold-1: mse-0.21882
Update best_mse, Valid at fold-1 epoch-91: mse-0.21882, rmse-0.467782, ci--1, r2-0.716366, pearson-0.847572, spearman-0.678808
Traing Log at fold-1 epoch-92: mse-0.076554, rmse-0.276684, r2-0.887294
Valid at fold-1: mse-0.216544
Update best_mse, Valid at fold-1 epoch-92: mse-0.216544, rmse-0.465343, ci--1, r2-0.719317, pearson-0.851189, spearman-0.68195
Traing Log at fold-1 epoch-93: mse-0.078005, rmse-0.279293, r2-0.884834
Valid at fold-1: mse-0.222287
Traing Log at fold-1 epoch-94: mse-0.075603, rmse-0.274961, r2-0.888191
Valid at fold-1: mse-0.221251
Traing Log at fold-1 epoch-95: mse-0.076293, rmse-0.276212, r2-0.88793
Valid at fold-1: mse-0.226687
Traing Log at fold-1 epoch-96: mse-0.074139, rmse-0.272284, r2-0.89097
Valid at fold-1: mse-0.222495
Traing Log at fold-1 epoch-97: mse-0.074737, rmse-0.27338, r2-0.889853
Valid at fold-1: mse-0.230571
Traing Log at fold-1 epoch-98: mse-0.074379, rmse-0.272725, r2-0.890874
Valid at fold-1: mse-0.219831
Traing Log at fold-1 epoch-99: mse-0.07199, rmse-0.268309, r2-0.894947
Valid at fold-1: mse-0.236567
Traing Log at fold-1 epoch-100: mse-0.073907, rmse-0.271858, r2-0.891211
Valid at fold-1: mse-0.230118
Traing Log at fold-1 epoch-101: mse-0.071938, rmse-0.268212, r2-0.89433
Valid at fold-1: mse-0.219632
Traing Log at fold-1 epoch-102: mse-0.073382, rmse-0.27089, r2-0.892442
Valid at fold-1: mse-0.229715
Traing Log at fold-1 epoch-103: mse-0.071787, rmse-0.26793, r2-0.894743
Valid at fold-1: mse-0.221273
Traing Log at fold-1 epoch-104: mse-0.070486, rmse-0.265493, r2-0.896714
Valid at fold-1: mse-0.228928
Traing Log at fold-1 epoch-105: mse-0.070809, rmse-0.2661, r2-0.896252
Valid at fold-1: mse-0.219079
Traing Log at fold-1 epoch-106: mse-0.069043, rmse-0.26276, r2-0.899541
Valid at fold-1: mse-0.219895
Traing Log at fold-1 epoch-107: mse-0.070014, rmse-0.264601, r2-0.897494
Valid at fold-1: mse-0.225299
Traing Log at fold-1 epoch-108: mse-0.069549, rmse-0.263721, r2-0.898611
Valid at fold-1: mse-0.221257
Traing Log at fold-1 epoch-109: mse-0.068667, rmse-0.262044, r2-0.899592
Valid at fold-1: mse-0.222173
Traing Log at fold-1 epoch-110: mse-0.069338, rmse-0.263321, r2-0.898905
Valid at fold-1: mse-0.218804
Traing Log at fold-1 epoch-111: mse-0.068834, rmse-0.262363, r2-0.899537
Valid at fold-1: mse-0.223084
Traing Log at fold-1 epoch-112: mse-0.067447, rmse-0.259705, r2-0.901902
Valid at fold-1: mse-0.219393
Traing Log at fold-1 epoch-113: mse-0.068878, rmse-0.262446, r2-0.899475
Valid at fold-1: mse-0.221318
Traing stop at epoch-113, model save at-./savemodel/davis-novel-prot-fold1-Nov12_16-42-47.pth
Save log over at ./log/Nov12_16-42-47-davis-novel-prot-fold1.csv

============================================================
Testing fold 1 with best model...
============================================================
/home/minhtrin/.conda/envs/LLMDTA/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Test at fold-1, mse: 0.682086, rmse: 0.825885, ci: 0.784375, r2: 0.302891, pearson: 0.576183, spearman: 0.56156

Fold 1 results saved to: ./log/Test-davis-novel-prot-fold1-Nov12_16-42-47.csv
============================================================
Training fold 1 completed successfully!
============================================================
wandb: updating run metadata
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading history steps 247-247, summary, console lines 263-268
wandb: 
wandb: Run history:
wandb:      best_valid/mse â–ˆâ–…â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  best_valid/pearson â–â–‚â–„â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:       best_valid/r2 â–â–„â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:     best_valid/rmse â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: best_valid/spearman â–â–‚â–…â–…â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               epoch â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:             test/ci â–
wandb:            test/mse â–
wandb:        test/pearson â–
wandb:             test/r2 â–
wandb:                 +13 ...
wandb: 
wandb: Run summary:
wandb:      best_valid/mse 0.21654
wandb:  best_valid/pearson 0.85119
wandb:       best_valid/r2 0.71932
wandb:     best_valid/rmse 0.46534
wandb: best_valid/spearman 0.68195
wandb:               epoch 113
wandb:       final_test_ci 0.78438
wandb:      final_test_mse 0.68209
wandb:  final_test_pearson 0.57618
wandb:       final_test_r2 0.30289
wandb:                 +19 ...
wandb: 
wandb: ğŸš€ View run davis-novel-prot-fold1 at: https://wandb.ai/tringuyen/LLMDTA/runs/9wqpgh1k
wandb: â­ï¸ View project at: https://wandb.ai/tringuyen/LLMDTA
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251112_164248-9wqpgh1k/logs
Weights & Biases run finished

Training for fold 1 completed successfully.
Python script exit code: 0
==========================================
End Time: Wed Nov 12 09:34:43 PM AEDT 2025
==========================================
