==========================================
Job ID: 2013086
Array Task ID: 3
Node: v100-f-21
Start Time: Thu Nov 13 07:19:12 AM AEDT 2025
==========================================
Activating conda environment...
Conda environment activated: LLMDTA
Checking GPU...
Thu Nov 13 07:19:13 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           On  |   00000000:18:00.0 Off |                    0 |
| N/A   33C    P0             42W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Setting environment variables...

Starting training for fold 3...


============================================================
Starting training for Fold 3
Dataset: metz, Running Set: warm
Epochs: 200, Batch Size: 16
============================================================

Executing: python -u code/train.py --fold 3 --cuda 0 --dataset metz --running_set warm --epochs 200 --batch_size 16 --wandb_project LLMDTA
============================================================
Training Fold 3/4
Dataset: metz-warm
Device: cuda (CUDA_VISIBLE_DEVICES=0)
Pretrain-./data/metz/metz_drug_pretrain.pkl
Pretrain-./data/metz/metz_esm_pretrain.pkl
============================================================
wandb: Currently logged in as: tringuyen to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: setting up run vpozh9au
wandb: Tracking run with wandb version 0.23.0
wandb: Run data is saved locally in /vast/minhtrin/DTA/Temp/wandb/run-20251113_071919-vpozh9au
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run metz-warm-fold3
wandb: â­ï¸ View project at https://wandb.ai/tringuyen/LLMDTA
wandb: ğŸš€ View run at https://wandb.ai/tringuyen/LLMDTA/runs/vpozh9au
Weights & Biases initialized: LLMDTA
Loading fold 3 data...
  Train: ./data/dta-5fold-dataset/metz/warm/fold_3_train.csv
  Valid: ./data/dta-5fold-dataset/metz/warm/fold_3_valid.csv
  Test:  ./data/dta-5fold-dataset/metz/warm/fold_3_test.csv
Dataset loaded: 22566 train, 5642 valid, 7051 test samples
/home/minhtrin/.conda/envs/LLMDTA/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Traing Log at fold-3 epoch-1: mse-2.06989, rmse-1.438711, r2--0.201806
Valid at fold-3: mse-0.633138
Update best_mse, Valid at fold-3 epoch-1: mse-0.633138, rmse-0.7957, ci--1, r2-0.294472, pearson-0.55694, spearman-0.52228
Traing Log at fold-3 epoch-2: mse-0.794808, rmse-0.89152, r2--0.22697
Valid at fold-3: mse-0.701367
Traing Log at fold-3 epoch-3: mse-0.625092, rmse-0.790628, r2--0.116174
Valid at fold-3: mse-0.539824
Update best_mse, Valid at fold-3 epoch-3: mse-0.539824, rmse-0.734727, ci--1, r2-0.398455, pearson-0.655716, spearman-0.608411
Traing Log at fold-3 epoch-4: mse-0.555081, rmse-0.745038, r2--0.042419
Valid at fold-3: mse-0.484589
Update best_mse, Valid at fold-3 epoch-4: mse-0.484589, rmse-0.696124, ci--1, r2-0.460006, pearson-0.689196, spearman-0.643013
Traing Log at fold-3 epoch-5: mse-0.499267, rmse-0.706588, r2-0.034906
Valid at fold-3: mse-0.521167
Traing Log at fold-3 epoch-6: mse-0.464817, rmse-0.681775, r2-0.104444
Valid at fold-3: mse-0.431255
Update best_mse, Valid at fold-3 epoch-6: mse-0.431255, rmse-0.6567, ci--1, r2-0.519437, pearson-0.727826, spearman-0.672611
Traing Log at fold-3 epoch-7: mse-0.43259, rmse-0.657716, r2-0.179796
Valid at fold-3: mse-0.423034
Update best_mse, Valid at fold-3 epoch-7: mse-0.423034, rmse-0.650411, ci--1, r2-0.528598, pearson-0.745072, spearman-0.702653
Traing Log at fold-3 epoch-8: mse-0.41122, rmse-0.641264, r2-0.235926
Valid at fold-3: mse-0.416833
Update best_mse, Valid at fold-3 epoch-8: mse-0.416833, rmse-0.645626, ci--1, r2-0.535509, pearson-0.739531, spearman-0.693681
Traing Log at fold-3 epoch-9: mse-0.394081, rmse-0.627759, r2-0.281412
Valid at fold-3: mse-0.391575
Update best_mse, Valid at fold-3 epoch-9: mse-0.391575, rmse-0.625759, ci--1, r2-0.563654, pearson-0.758023, spearman-0.719125
Traing Log at fold-3 epoch-10: mse-0.375792, rmse-0.613018, r2-0.328707
Valid at fold-3: mse-0.386901
Update best_mse, Valid at fold-3 epoch-10: mse-0.386901, rmse-0.622014, ci--1, r2-0.568862, pearson-0.759078, spearman-0.711551
Traing Log at fold-3 epoch-11: mse-0.365807, rmse-0.60482, r2-0.354337
Valid at fold-3: mse-0.3916
Traing Log at fold-3 epoch-12: mse-0.348271, rmse-0.590145, r2-0.400738
Valid at fold-3: mse-0.381814
Update best_mse, Valid at fold-3 epoch-12: mse-0.381814, rmse-0.617911, ci--1, r2-0.574531, pearson-0.768401, spearman-0.71701
Traing Log at fold-3 epoch-13: mse-0.342305, rmse-0.585068, r2-0.416575
Valid at fold-3: mse-0.373743
Update best_mse, Valid at fold-3 epoch-13: mse-0.373743, rmse-0.611345, ci--1, r2-0.583525, pearson-0.778523, spearman-0.731989
Traing Log at fold-3 epoch-14: mse-0.330925, rmse-0.57526, r2-0.445991
Valid at fold-3: mse-0.362235
Update best_mse, Valid at fold-3 epoch-14: mse-0.362235, rmse-0.601859, ci--1, r2-0.596349, pearson-0.77883, spearman-0.735394
Traing Log at fold-3 epoch-15: mse-0.31388, rmse-0.56025, r2-0.487058
Valid at fold-3: mse-0.351408
Update best_mse, Valid at fold-3 epoch-15: mse-0.351408, rmse-0.592797, ci--1, r2-0.608414, pearson-0.784473, spearman-0.736774
Traing Log at fold-3 epoch-16: mse-0.305212, rmse-0.55246, r2-0.508555
Valid at fold-3: mse-0.350736
Update best_mse, Valid at fold-3 epoch-16: mse-0.350736, rmse-0.59223, ci--1, r2-0.609162, pearson-0.78376, spearman-0.740196
Traing Log at fold-3 epoch-17: mse-0.299111, rmse-0.546911, r2-0.523846
Valid at fold-3: mse-0.340354
Update best_mse, Valid at fold-3 epoch-17: mse-0.340354, rmse-0.583399, ci--1, r2-0.620732, pearson-0.790281, spearman-0.748939
Traing Log at fold-3 epoch-18: mse-0.288446, rmse-0.537072, r2-0.545347
Valid at fold-3: mse-0.3431
Traing Log at fold-3 epoch-19: mse-0.281027, rmse-0.530119, r2-0.564348
Valid at fold-3: mse-0.332284
Update best_mse, Valid at fold-3 epoch-19: mse-0.332284, rmse-0.576441, ci--1, r2-0.629724, pearson-0.795576, spearman-0.752759
Traing Log at fold-3 epoch-20: mse-0.272083, rmse-0.521615, r2-0.584229
Valid at fold-3: mse-0.33665
Traing Log at fold-3 epoch-21: mse-0.264917, rmse-0.514701, r2-0.598305
Valid at fold-3: mse-0.343827
Traing Log at fold-3 epoch-22: mse-0.259539, rmse-0.509449, r2-0.610468
Valid at fold-3: mse-0.333384
Traing Log at fold-3 epoch-23: mse-0.252002, rmse-0.501998, r2-0.624702
Valid at fold-3: mse-0.337992
Traing Log at fold-3 epoch-24: mse-0.241029, rmse-0.490947, r2-0.647701
Valid at fold-3: mse-0.333757
Traing Log at fold-3 epoch-25: mse-0.237407, rmse-0.487244, r2-0.653519
Valid at fold-3: mse-0.33269
Traing Log at fold-3 epoch-26: mse-0.234565, rmse-0.48432, r2-0.659471
Valid at fold-3: mse-0.32103
Update best_mse, Valid at fold-3 epoch-26: mse-0.32103, rmse-0.566595, ci--1, r2-0.642265, pearson-0.803166, spearman-0.758579
Traing Log at fold-3 epoch-27: mse-0.227915, rmse-0.477405, r2-0.672002
Valid at fold-3: mse-0.333452
Traing Log at fold-3 epoch-28: mse-0.221441, rmse-0.470575, r2-0.684765
Valid at fold-3: mse-0.328321
Traing Log at fold-3 epoch-29: mse-0.219045, rmse-0.468022, r2-0.688354
Valid at fold-3: mse-0.323847
Traing Log at fold-3 epoch-30: mse-0.21181, rmse-0.460228, r2-0.703339
Valid at fold-3: mse-0.31866
Update best_mse, Valid at fold-3 epoch-30: mse-0.31866, rmse-0.5645, ci--1, r2-0.644906, pearson-0.809323, spearman-0.766351
Traing Log at fold-3 epoch-31: mse-0.206208, rmse-0.454101, r2-0.713005
Valid at fold-3: mse-0.322212
Traing Log at fold-3 epoch-32: mse-0.205682, rmse-0.453522, r2-0.713727
Valid at fold-3: mse-0.333296
Traing Log at fold-3 epoch-33: mse-0.19938, rmse-0.44652, r2-0.724224
Valid at fold-3: mse-0.327993
Traing Log at fold-3 epoch-34: mse-0.192166, rmse-0.438368, r2-0.737513
Valid at fold-3: mse-0.315708
Update best_mse, Valid at fold-3 epoch-34: mse-0.315708, rmse-0.561879, ci--1, r2-0.648195, pearson-0.808932, spearman-0.768254
Traing Log at fold-3 epoch-35: mse-0.191678, rmse-0.43781, r2-0.738065
Valid at fold-3: mse-0.322374
Traing Log at fold-3 epoch-36: mse-0.187539, rmse-0.433058, r2-0.745795
Valid at fold-3: mse-0.318314
Traing Log at fold-3 epoch-37: mse-0.180355, rmse-0.424683, r2-0.756983
Valid at fold-3: mse-0.318969
Traing Log at fold-3 epoch-38: mse-0.181774, rmse-0.426349, r2-0.755693
Valid at fold-3: mse-0.327855
Traing Log at fold-3 epoch-39: mse-0.176441, rmse-0.420049, r2-0.763611
Valid at fold-3: mse-0.3246
Traing Log at fold-3 epoch-40: mse-0.172525, rmse-0.415361, r2-0.770392
Valid at fold-3: mse-0.323016
Traing Log at fold-3 epoch-41: mse-0.169045, rmse-0.41115, r2-0.77541
Valid at fold-3: mse-0.326136
Traing Log at fold-3 epoch-42: mse-0.163852, rmse-0.404786, r2-0.784347
Valid at fold-3: mse-0.327195
Traing Log at fold-3 epoch-43: mse-0.163905, rmse-0.404852, r2-0.784654
Valid at fold-3: mse-0.3201
Traing Log at fold-3 epoch-44: mse-0.160184, rmse-0.40023, r2-0.791003
Valid at fold-3: mse-0.322958
Traing Log at fold-3 epoch-45: mse-0.154647, rmse-0.393252, r2-0.798335
Valid at fold-3: mse-0.312938
Update best_mse, Valid at fold-3 epoch-45: mse-0.312938, rmse-0.559409, ci--1, r2-0.651282, pearson-0.812309, spearman-0.773254
Traing Log at fold-3 epoch-46: mse-0.152696, rmse-0.390763, r2-0.802028
Valid at fold-3: mse-0.327961
Traing Log at fold-3 epoch-47: mse-0.150126, rmse-0.387461, r2-0.80593
Valid at fold-3: mse-0.332215
Traing Log at fold-3 epoch-48: mse-0.149852, rmse-0.387108, r2-0.805976
Valid at fold-3: mse-0.322724
Traing Log at fold-3 epoch-49: mse-0.147523, rmse-0.384087, r2-0.810317
Valid at fold-3: mse-0.324684
Traing Log at fold-3 epoch-50: mse-0.142271, rmse-0.377189, r2-0.817818
Valid at fold-3: mse-0.327054
Traing Log at fold-3 epoch-51: mse-0.142865, rmse-0.377975, r2-0.817207
Valid at fold-3: mse-0.313389
Traing Log at fold-3 epoch-52: mse-0.13733, rmse-0.37058, r2-0.82547
Valid at fold-3: mse-0.321309
Traing Log at fold-3 epoch-53: mse-0.137826, rmse-0.371249, r2-0.824674
Valid at fold-3: mse-0.339241
Traing Log at fold-3 epoch-54: mse-0.136031, rmse-0.368824, r2-0.827506
Valid at fold-3: mse-0.322321
Traing Log at fold-3 epoch-55: mse-0.132577, rmse-0.364112, r2-0.832479
Valid at fold-3: mse-0.320792
Traing Log at fold-3 epoch-56: mse-0.131114, rmse-0.362097, r2-0.834447
Valid at fold-3: mse-0.331584
Traing Log at fold-3 epoch-57: mse-0.127729, rmse-0.357392, r2-0.83989
Valid at fold-3: mse-0.326441
Traing Log at fold-3 epoch-58: mse-0.125773, rmse-0.354645, r2-0.842303
Valid at fold-3: mse-0.315025
Traing Log at fold-3 epoch-59: mse-0.124752, rmse-0.353203, r2-0.843795
Valid at fold-3: mse-0.314059
Traing Log at fold-3 epoch-60: mse-0.123415, rmse-0.351305, r2-0.84613
Valid at fold-3: mse-0.320665
Traing Log at fold-3 epoch-61: mse-0.121828, rmse-0.349039, r2-0.848017
Valid at fold-3: mse-0.322975
Traing Log at fold-3 epoch-62: mse-0.120995, rmse-0.347844, r2-0.849884
Valid at fold-3: mse-0.318844
Traing Log at fold-3 epoch-63: mse-0.117088, rmse-0.342181, r2-0.854994
Valid at fold-3: mse-0.322276
Traing Log at fold-3 epoch-64: mse-0.116351, rmse-0.341103, r2-0.856144
Valid at fold-3: mse-0.326456
Traing Log at fold-3 epoch-65: mse-0.113755, rmse-0.337275, r2-0.859716
Valid at fold-3: mse-0.324956
Traing Log at fold-3 epoch-66: mse-0.112312, rmse-0.33513, r2-0.861307
Valid at fold-3: mse-0.327599
Traing stop at epoch-66, model save at-./savemodel/metz-warm-fold3-Nov13_07-19-19.pth
Save log over at ./log/Nov13_07-19-19-metz-warm-fold3.csv

============================================================
Testing fold 3 with best model...
============================================================
/home/minhtrin/.conda/envs/LLMDTA/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Test at fold-3, mse: 0.312688, rmse: 0.559185, ci: 0.8079, r2: 0.664731, pearson: 0.821498, spearman: 0.781007

Fold 3 results saved to: ./log/Test-metz-warm-fold3-Nov13_07-19-19.csv
============================================================
Training fold 3 completed successfully!
============================================================
wandb: updating run metadata
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      best_valid/mse â–ˆâ–†â–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:  best_valid/pearson â–â–„â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:       best_valid/r2 â–â–ƒâ–„â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:     best_valid/rmse â–ˆâ–†â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb: best_valid/spearman â–â–ƒâ–„â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆ
wandb:             test/ci â–
wandb:            test/mse â–
wandb:        test/pearson â–
wandb:             test/r2 â–
wandb:                 +13 ...
wandb: 
wandb: Run summary:
wandb:      best_valid/mse 0.31294
wandb:  best_valid/pearson 0.81231
wandb:       best_valid/r2 0.65128
wandb:     best_valid/rmse 0.55941
wandb: best_valid/spearman 0.77325
wandb:               epoch 66
wandb:       final_test_ci 0.8079
wandb:      final_test_mse 0.31269
wandb:  final_test_pearson 0.8215
wandb:       final_test_r2 0.66473
wandb:                 +19 ...
wandb: 
wandb: ğŸš€ View run metz-warm-fold3 at: https://wandb.ai/tringuyen/LLMDTA/runs/vpozh9au
wandb: â­ï¸ View project at: https://wandb.ai/tringuyen/LLMDTA
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251113_071919-vpozh9au/logs
Weights & Biases run finished

Training for fold 3 completed successfully.
Python script exit code: 0
==========================================
End Time: Thu Nov 13 10:45:14 AM AEDT 2025
==========================================
