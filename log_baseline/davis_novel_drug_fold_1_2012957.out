==========================================
Job ID: 2012957
Array Task ID: 1
Node: v100l-f-04
Start Time: Wed Nov 12 04:41:09 PM AEDT 2025
==========================================
Activating conda environment...
Conda environment activated: LLMDTA
Checking GPU...
Wed Nov 12 16:41:10 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           On  |   00000000:06:00.0 Off |                    0 |
| N/A   33C    P0             43W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Setting environment variables...

Starting training for fold 1...


============================================================
Starting training for Fold 1
Dataset: davis, Running Set: novel-drug
Epochs: 200, Batch Size: 16
============================================================

Executing: python -u code/train.py --fold 1 --cuda 0 --dataset davis --running_set novel-drug --epochs 200 --batch_size 16 --wandb_project LLMDTA
/home/minhtrin/.conda/envs/LLMDTA/lib/python3.10/site-packages/torch/cuda/__init__.py:182: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:119.)
  return torch._C._cuda_getDeviceCount() > 0
============================================================
Training Fold 1/4
Dataset: davis-novel-drug
Device: cpu (CUDA_VISIBLE_DEVICES=0)
Pretrain-./data/davis/davis_drug_pretrain.pkl
Pretrain-./data/davis/davis_esm_pretrain.pkl
============================================================
wandb: Currently logged in as: tringuyen to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: setting up run 8ygce2e6
wandb: Tracking run with wandb version 0.23.0
wandb: Run data is saved locally in /vast/minhtrin/DTA/Temp/wandb/run-20251112_164116-8ygce2e6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run davis-novel-drug-fold1
wandb: â­ï¸ View project at https://wandb.ai/tringuyen/LLMDTA
wandb: ğŸš€ View run at https://wandb.ai/tringuyen/LLMDTA/runs/8ygce2e6
Weights & Biases initialized: LLMDTA
Loading fold 1 data...
  Train: ./data/dta-5fold-dataset/davis/novel-drug/fold_1_train.csv
  Valid: ./data/dta-5fold-dataset/davis/novel-drug/fold_1_valid.csv
  Test:  ./data/dta-5fold-dataset/davis/novel-drug/fold_1_test.csv
Dataset loaded: 19448 train, 4862 valid, 5746 test samples
/home/minhtrin/.conda/envs/LLMDTA/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Traing Log at fold-1 epoch-1: mse-1.728639, rmse-1.314777, r2--0.195195
Valid at fold-1: mse-1.768183
Update best_mse, Valid at fold-1 epoch-1: mse-1.768183, rmse-1.32973, ci--1, r2--1.102417, pearson-0.493991, spearman-0.426681
Traing Log at fold-1 epoch-2: mse-0.684571, rmse-0.827388, r2--0.184923
Valid at fold-1: mse-1.165142
Update best_mse, Valid at fold-1 epoch-2: mse-1.165142, rmse-1.079417, ci--1, r2--0.385385, pearson-0.563833, spearman-0.489077
Traing Log at fold-1 epoch-3: mse-0.570061, rmse-0.755024, r2--0.092097
Valid at fold-1: mse-0.75253
Update best_mse, Valid at fold-1 epoch-3: mse-0.75253, rmse-0.867485, ci--1, r2-0.105221, pearson-0.603626, spearman-0.504534
Traing Log at fold-1 epoch-4: mse-0.489374, rmse-0.699553, r2-0.011013
Valid at fold-1: mse-0.478141
Update best_mse, Valid at fold-1 epoch-4: mse-0.478141, rmse-0.691477, ci--1, r2-0.431477, pearson-0.67155, spearman-0.557404
Traing Log at fold-1 epoch-5: mse-0.428806, rmse-0.654833, r2-0.105987
Valid at fold-1: mse-0.450153
Update best_mse, Valid at fold-1 epoch-5: mse-0.450153, rmse-0.670935, ci--1, r2-0.464755, pearson-0.751839, spearman-0.630248
Traing Log at fold-1 epoch-6: mse-0.384897, rmse-0.6204, r2-0.20568
Valid at fold-1: mse-0.374832
Update best_mse, Valid at fold-1 epoch-6: mse-0.374832, rmse-0.612235, ci--1, r2-0.554314, pearson-0.746529, spearman-0.63234
Traing Log at fold-1 epoch-7: mse-0.351584, rmse-0.592945, r2-0.274603
Valid at fold-1: mse-0.364539
Update best_mse, Valid at fold-1 epoch-7: mse-0.364539, rmse-0.603771, ci--1, r2-0.566553, pearson-0.760115, spearman-0.63726
Traing Log at fold-1 epoch-8: mse-0.335256, rmse-0.579013, r2-0.320202
Valid at fold-1: mse-0.324131
Update best_mse, Valid at fold-1 epoch-8: mse-0.324131, rmse-0.569325, ci--1, r2-0.6146, pearson-0.787379, spearman-0.680486
Traing Log at fold-1 epoch-9: mse-0.316503, rmse-0.562586, r2-0.360547
Valid at fold-1: mse-0.331719
Traing Log at fold-1 epoch-10: mse-0.297385, rmse-0.54533, r2-0.41345
Valid at fold-1: mse-0.313956
Update best_mse, Valid at fold-1 epoch-10: mse-0.313956, rmse-0.560318, ci--1, r2-0.626698, pearson-0.803203, spearman-0.678502
Traing Log at fold-1 epoch-11: mse-0.284799, rmse-0.533666, r2-0.450282
Valid at fold-1: mse-0.336676
Traing Log at fold-1 epoch-12: mse-0.270214, rmse-0.519821, r2-0.490425
Valid at fold-1: mse-0.307017
Update best_mse, Valid at fold-1 epoch-12: mse-0.307017, rmse-0.554091, ci--1, r2-0.634948, pearson-0.808073, spearman-0.67496
Traing Log at fold-1 epoch-13: mse-0.261241, rmse-0.511117, r2-0.511172
Valid at fold-1: mse-0.272834
Update best_mse, Valid at fold-1 epoch-13: mse-0.272834, rmse-0.522335, ci--1, r2-0.675593, pearson-0.8227, spearman-0.703216
Traing Log at fold-1 epoch-14: mse-0.250307, rmse-0.500306, r2-0.541882
Valid at fold-1: mse-0.292644
Traing Log at fold-1 epoch-15: mse-0.239215, rmse-0.489096, r2-0.569045
Valid at fold-1: mse-0.264386
Update best_mse, Valid at fold-1 epoch-15: mse-0.264386, rmse-0.514185, ci--1, r2-0.685637, pearson-0.829495, spearman-0.697295
Traing Log at fold-1 epoch-16: mse-0.229954, rmse-0.479535, r2-0.592125
Valid at fold-1: mse-0.276854
Traing Log at fold-1 epoch-17: mse-0.222318, rmse-0.471506, r2-0.609465
Valid at fold-1: mse-0.274015
Traing Log at fold-1 epoch-18: mse-0.218132, rmse-0.467046, r2-0.620081
Valid at fold-1: mse-0.259942
Update best_mse, Valid at fold-1 epoch-18: mse-0.259942, rmse-0.509845, ci--1, r2-0.690922, pearson-0.832988, spearman-0.707489
Traing Log at fold-1 epoch-19: mse-0.207636, rmse-0.455671, r2-0.644662
Valid at fold-1: mse-0.26246
Traing Log at fold-1 epoch-20: mse-0.201524, rmse-0.448914, r2-0.657483
Valid at fold-1: mse-0.265292
Traing Log at fold-1 epoch-21: mse-0.196853, rmse-0.443681, r2-0.668962
Valid at fold-1: mse-0.25189
Update best_mse, Valid at fold-1 epoch-21: mse-0.25189, rmse-0.501886, ci--1, r2-0.700496, pearson-0.842989, spearman-0.70425
Traing Log at fold-1 epoch-22: mse-0.193144, rmse-0.439482, r2-0.67769
Valid at fold-1: mse-0.259039
Traing Log at fold-1 epoch-23: mse-0.1872, rmse-0.432666, r2-0.690689
Valid at fold-1: mse-0.239945
Update best_mse, Valid at fold-1 epoch-23: mse-0.239945, rmse-0.489842, ci--1, r2-0.714699, pearson-0.848054, spearman-0.709511
Traing Log at fold-1 epoch-24: mse-0.182045, rmse-0.426668, r2-0.701478
Valid at fold-1: mse-0.269538
Traing Log at fold-1 epoch-25: mse-0.177256, rmse-0.421018, r2-0.712585
Valid at fold-1: mse-0.25059
Traing Log at fold-1 epoch-26: mse-0.171584, rmse-0.414227, r2-0.723295
Valid at fold-1: mse-0.241828
Traing Log at fold-1 epoch-27: mse-0.169002, rmse-0.411098, r2-0.729232
Valid at fold-1: mse-0.23484
Update best_mse, Valid at fold-1 epoch-27: mse-0.23484, rmse-0.484603, ci--1, r2-0.720769, pearson-0.850589, spearman-0.722294
Traing Log at fold-1 epoch-28: mse-0.16404, rmse-0.405018, r2-0.737807
Valid at fold-1: mse-0.253446
Traing Log at fold-1 epoch-29: mse-0.162112, rmse-0.402632, r2-0.743228
Valid at fold-1: mse-0.236295
Traing Log at fold-1 epoch-30: mse-0.158446, rmse-0.398052, r2-0.750015
Valid at fold-1: mse-0.228599
Update best_mse, Valid at fold-1 epoch-30: mse-0.228599, rmse-0.47812, ci--1, r2-0.72819, pearson-0.854109, spearman-0.715188
Traing Log at fold-1 epoch-31: mse-0.152834, rmse-0.390939, r2-0.759525
Valid at fold-1: mse-0.2349
Traing Log at fold-1 epoch-32: mse-0.150263, rmse-0.387637, r2-0.765959
Valid at fold-1: mse-0.23319
Traing Log at fold-1 epoch-33: mse-0.146188, rmse-0.382345, r2-0.773544
Valid at fold-1: mse-0.244923
Traing Log at fold-1 epoch-34: mse-0.147479, rmse-0.38403, r2-0.770639
Valid at fold-1: mse-0.241157
Traing Log at fold-1 epoch-35: mse-0.142646, rmse-0.377685, r2-0.780462
Valid at fold-1: mse-0.236265
Traing Log at fold-1 epoch-36: mse-0.140365, rmse-0.374653, r2-0.784564
Valid at fold-1: mse-0.235328
Traing Log at fold-1 epoch-37: mse-0.137332, rmse-0.370584, r2-0.790166
Valid at fold-1: mse-0.240506
Traing Log at fold-1 epoch-38: mse-0.134561, rmse-0.366825, r2-0.795216
Valid at fold-1: mse-0.238467
Traing Log at fold-1 epoch-39: mse-0.133922, rmse-0.365954, r2-0.795985
Valid at fold-1: mse-0.232622
Traing Log at fold-1 epoch-40: mse-0.130342, rmse-0.361028, r2-0.80322
Valid at fold-1: mse-0.233348
Traing Log at fold-1 epoch-41: mse-0.130426, rmse-0.361145, r2-0.802112
Valid at fold-1: mse-0.230154
Traing Log at fold-1 epoch-42: mse-0.125511, rmse-0.354275, r2-0.810908
Valid at fold-1: mse-0.236032
Traing Log at fold-1 epoch-43: mse-0.123249, rmse-0.351069, r2-0.8158
Valid at fold-1: mse-0.227855
Update best_mse, Valid at fold-1 epoch-43: mse-0.227855, rmse-0.477341, ci--1, r2-0.729074, pearson-0.854495, spearman-0.711966
Traing Log at fold-1 epoch-44: mse-0.124406, rmse-0.352712, r2-0.812609
Valid at fold-1: mse-0.226934
Update best_mse, Valid at fold-1 epoch-44: mse-0.226934, rmse-0.476376, ci--1, r2-0.730169, pearson-0.857249, spearman-0.710639
Traing Log at fold-1 epoch-45: mse-0.120832, rmse-0.34761, r2-0.819967
Valid at fold-1: mse-0.231452
Traing Log at fold-1 epoch-46: mse-0.118433, rmse-0.344141, r2-0.824421
Valid at fold-1: mse-0.214086
Update best_mse, Valid at fold-1 epoch-46: mse-0.214086, rmse-0.462695, ci--1, r2-0.745445, pearson-0.866694, spearman-0.714334
Traing Log at fold-1 epoch-47: mse-0.115851, rmse-0.340369, r2-0.8284
Valid at fold-1: mse-0.229239
Traing Log at fold-1 epoch-48: mse-0.114175, rmse-0.337899, r2-0.831564
Valid at fold-1: mse-0.232225
Traing Log at fold-1 epoch-49: mse-0.113075, rmse-0.336266, r2-0.832613
Valid at fold-1: mse-0.232711
Traing Log at fold-1 epoch-50: mse-0.112401, rmse-0.335263, r2-0.83517
Valid at fold-1: mse-0.217006
Traing Log at fold-1 epoch-51: mse-0.10987, rmse-0.331466, r2-0.838877
Valid at fold-1: mse-0.232079
Traing Log at fold-1 epoch-52: mse-0.109713, rmse-0.331229, r2-0.838856
Valid at fold-1: mse-0.245481
Traing Log at fold-1 epoch-53: mse-0.110669, rmse-0.332669, r2-0.837362
Valid at fold-1: mse-0.229561
Traing Log at fold-1 epoch-54: mse-0.106116, rmse-0.325754, r2-0.844779
Valid at fold-1: mse-0.23254
Traing Log at fold-1 epoch-55: mse-0.106345, rmse-0.326106, r2-0.845168
Valid at fold-1: mse-0.223847
Traing Log at fold-1 epoch-56: mse-0.104018, rmse-0.322518, r2-0.848294
Valid at fold-1: mse-0.224097
Traing Log at fold-1 epoch-57: mse-0.101505, rmse-0.318598, r2-0.853007
Valid at fold-1: mse-0.224041
Traing Log at fold-1 epoch-58: mse-0.101346, rmse-0.318349, r2-0.852994
Valid at fold-1: mse-0.222461
Traing Log at fold-1 epoch-59: mse-0.098869, rmse-0.314434, r2-0.857275
Valid at fold-1: mse-0.228
Traing Log at fold-1 epoch-60: mse-0.098112, rmse-0.313228, r2-0.858188
Valid at fold-1: mse-0.218247
Traing Log at fold-1 epoch-61: mse-0.096471, rmse-0.310598, r2-0.860909
Valid at fold-1: mse-0.220943
Traing Log at fold-1 epoch-62: mse-0.096522, rmse-0.31068, r2-0.860719
Valid at fold-1: mse-0.216024
Traing Log at fold-1 epoch-63: mse-0.097973, rmse-0.313006, r2-0.859075
Valid at fold-1: mse-0.217409
Traing Log at fold-1 epoch-64: mse-0.0941, rmse-0.306758, r2-0.865219
Valid at fold-1: mse-0.23269
Traing Log at fold-1 epoch-65: mse-0.09389, rmse-0.306416, r2-0.86554
Valid at fold-1: mse-0.233898
Traing Log at fold-1 epoch-66: mse-0.091458, rmse-0.30242, r2-0.868759
Valid at fold-1: mse-0.220961
Traing Log at fold-1 epoch-67: mse-0.090747, rmse-0.301242, r2-0.870517
Valid at fold-1: mse-0.218776
Traing stop at epoch-67, model save at-./savemodel/davis-novel-drug-fold1-Nov12_16-41-16.pth
Save log over at ./log/Nov12_16-41-16-davis-novel-drug-fold1.csv

============================================================
Testing fold 1 with best model...
============================================================
/home/minhtrin/.conda/envs/LLMDTA/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Test at fold-1, mse: 0.633429, rmse: 0.795882, ci: 0.719895, r2: 0.220743, pearson: 0.492226, spearman: 0.387852

Fold 1 results saved to: ./log/Test-davis-novel-drug-fold1-Nov12_16-41-16.csv
============================================================
Training fold 1 completed successfully!
============================================================
wandb: updating run metadata
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading config.yaml
wandb: uploading history steps 154-154, summary, console lines 170-175
wandb: 
wandb: Run history:
wandb:      best_valid/mse â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  best_valid/pearson â–â–‚â–ƒâ–„â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:       best_valid/r2 â–â–„â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:     best_valid/rmse â–ˆâ–†â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb: best_valid/spearman â–â–‚â–ƒâ–„â–†â–†â–†â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–ˆ
wandb:             test/ci â–
wandb:            test/mse â–
wandb:        test/pearson â–
wandb:             test/r2 â–
wandb:                 +13 ...
wandb: 
wandb: Run summary:
wandb:      best_valid/mse 0.21409
wandb:  best_valid/pearson 0.86669
wandb:       best_valid/r2 0.74545
wandb:     best_valid/rmse 0.4627
wandb: best_valid/spearman 0.71433
wandb:               epoch 67
wandb:       final_test_ci 0.71989
wandb:      final_test_mse 0.63343
wandb:  final_test_pearson 0.49223
wandb:       final_test_r2 0.22074
wandb:                 +19 ...
wandb: 
wandb: ğŸš€ View run davis-novel-drug-fold1 at: https://wandb.ai/tringuyen/LLMDTA/runs/8ygce2e6
wandb: â­ï¸ View project at: https://wandb.ai/tringuyen/LLMDTA
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251112_164116-8ygce2e6/logs
Weights & Biases run finished

Training for fold 1 completed successfully.
Python script exit code: 0
==========================================
End Time: Thu Nov 13 02:28:42 PM AEDT 2025
==========================================
