==========================================
Job ID: 2013062
Array Task ID: 4
Node: v100-f-01
Start Time: Thu Nov 13 07:13:43 AM AEDT 2025
==========================================
Activating conda environment...
Conda environment activated: LLMDTA
Checking GPU...
Thu Nov 13 07:13:44 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           On  |   00000000:18:00.0 Off |                    0 |
| N/A   33C    P0             42W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-SXM2-32GB           On  |   00000000:3B:00.0 Off |                    0 |
| N/A   28C    P0             42W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-SXM2-32GB           On  |   00000000:86:00.0 Off |                    0 |
| N/A   28C    P0             40W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-SXM2-32GB           On  |   00000000:AF:00.0 Off |                    0 |
| N/A   31C    P0             42W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Setting environment variables...

Starting training for fold 4...


============================================================
Starting training for Fold 4
Dataset: kiba, Running Set: novel-prot
Epochs: 200, Batch Size: 16
============================================================

Executing: python -u code/train.py --fold 4 --cuda 0 --dataset kiba --running_set novel-prot --epochs 200 --batch_size 16 --wandb_project LLMDTA
============================================================
Training Fold 4/4
Dataset: kiba-novel-prot
Device: cuda (CUDA_VISIBLE_DEVICES=0)
Pretrain-./data/kiba/kiba_drug_pretrain.pkl
Pretrain-./data/kiba/kiba_esm_pretrain.pkl
============================================================
wandb: Currently logged in as: tringuyen to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: setting up run kvn7pvfs
wandb: Tracking run with wandb version 0.23.0
wandb: Run data is saved locally in /vast/minhtrin/DTA/Temp/wandb/run-20251113_071406-kvn7pvfs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run kiba-novel-prot-fold4
wandb: â­ï¸ View project at https://wandb.ai/tringuyen/LLMDTA
wandb: ğŸš€ View run at https://wandb.ai/tringuyen/LLMDTA/runs/kvn7pvfs
Weights & Biases initialized: LLMDTA
Loading fold 4 data...
  Train: ./data/dta-5fold-dataset/kiba/novel-prot/fold_4_train.csv
  Valid: ./data/dta-5fold-dataset/kiba/novel-prot/fold_4_valid.csv
  Test:  ./data/dta-5fold-dataset/kiba/novel-prot/fold_4_test.csv
Dataset loaded: 78485 train, 19622 valid, 20147 test samples
/home/minhtrin/.conda/envs/LLMDTA/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Traing Log at fold-4 epoch-1: mse-2.293801, rmse-1.51453, r2--0.161528
Valid at fold-4: mse-0.437767
Update best_mse, Valid at fold-4 epoch-1: mse-0.437767, rmse-0.661639, ci--1, r2-0.344461, pearson-0.613016, spearman-0.633789
Traing Log at fold-4 epoch-2: mse-0.469579, rmse-0.685258, r2--0.340382
Valid at fold-4: mse-0.374898
Update best_mse, Valid at fold-4 epoch-2: mse-0.374898, rmse-0.612289, ci--1, r2-0.438605, pearson-0.679002, spearman-0.672459
Traing Log at fold-4 epoch-3: mse-0.392793, rmse-0.626732, r2--0.100824
Valid at fold-4: mse-0.374193
Update best_mse, Valid at fold-4 epoch-3: mse-0.374193, rmse-0.611713, ci--1, r2-0.43966, pearson-0.695406, spearman-0.691537
Traing Log at fold-4 epoch-4: mse-0.342609, rmse-0.585328, r2-0.102807
Valid at fold-4: mse-0.30872
Update best_mse, Valid at fold-4 epoch-4: mse-0.30872, rmse-0.555625, ci--1, r2-0.537704, pearson-0.737215, spearman-0.71118
Traing Log at fold-4 epoch-5: mse-0.323488, rmse-0.56876, r2-0.205686
Valid at fold-4: mse-0.27351
Update best_mse, Valid at fold-4 epoch-5: mse-0.27351, rmse-0.522982, ci--1, r2-0.590429, pearson-0.774532, spearman-0.75272
Traing Log at fold-4 epoch-6: mse-0.317905, rmse-0.563831, r2-0.266643
Valid at fold-4: mse-0.275738
Traing Log at fold-4 epoch-7: mse-0.283487, rmse-0.532435, r2-0.352469
Valid at fold-4: mse-0.266855
Update best_mse, Valid at fold-4 epoch-7: mse-0.266855, rmse-0.51658, ci--1, r2-0.600394, pearson-0.777714, spearman-0.757648
Traing Log at fold-4 epoch-8: mse-0.275653, rmse-0.525027, r2-0.37524
Valid at fold-4: mse-0.286048
Traing Log at fold-4 epoch-9: mse-0.261966, rmse-0.511826, r2-0.424103
Valid at fold-4: mse-0.262844
Update best_mse, Valid at fold-4 epoch-9: mse-0.262844, rmse-0.512683, ci--1, r2-0.606401, pearson-0.783204, spearman-0.759514
Traing Log at fold-4 epoch-10: mse-0.250094, rmse-0.500094, r2-0.463563
Valid at fold-4: mse-0.263212
Traing Log at fold-4 epoch-11: mse-0.239759, rmse-0.489652, r2-0.495568
Valid at fold-4: mse-0.279285
Traing Log at fold-4 epoch-12: mse-0.228339, rmse-0.477848, r2-0.530822
Valid at fold-4: mse-0.240876
Update best_mse, Valid at fold-4 epoch-12: mse-0.240876, rmse-0.490791, ci--1, r2-0.639297, pearson-0.806669, spearman-0.790435
Traing Log at fold-4 epoch-13: mse-0.220396, rmse-0.469464, r2-0.556122
Valid at fold-4: mse-0.25028
Traing Log at fold-4 epoch-14: mse-0.213729, rmse-0.462308, r2-0.573394
Valid at fold-4: mse-0.233079
Update best_mse, Valid at fold-4 epoch-14: mse-0.233079, rmse-0.482783, ci--1, r2-0.650973, pearson-0.816419, spearman-0.7932
Traing Log at fold-4 epoch-15: mse-0.206085, rmse-0.453966, r2-0.595794
Valid at fold-4: mse-0.223254
Update best_mse, Valid at fold-4 epoch-15: mse-0.223254, rmse-0.472498, ci--1, r2-0.665686, pearson-0.817688, spearman-0.801665
Traing Log at fold-4 epoch-16: mse-0.200585, rmse-0.447867, r2-0.61091
Valid at fold-4: mse-0.22135
Update best_mse, Valid at fold-4 epoch-16: mse-0.22135, rmse-0.470479, ci--1, r2-0.668536, pearson-0.824467, spearman-0.80828
Traing Log at fold-4 epoch-17: mse-0.194285, rmse-0.440777, r2-0.627631
Valid at fold-4: mse-0.223276
Traing Log at fold-4 epoch-18: mse-0.188847, rmse-0.434565, r2-0.640992
Valid at fold-4: mse-0.226829
Traing Log at fold-4 epoch-19: mse-0.183557, rmse-0.428435, r2-0.654532
Valid at fold-4: mse-0.212556
Update best_mse, Valid at fold-4 epoch-19: mse-0.212556, rmse-0.461038, ci--1, r2-0.681705, pearson-0.831005, spearman-0.812163
Traing Log at fold-4 epoch-20: mse-0.179344, rmse-0.42349, r2-0.665413
Valid at fold-4: mse-0.225958
Traing Log at fold-4 epoch-21: mse-0.174065, rmse-0.417211, r2-0.679281
Valid at fold-4: mse-0.207453
Update best_mse, Valid at fold-4 epoch-21: mse-0.207453, rmse-0.45547, ci--1, r2-0.689347, pearson-0.834408, spearman-0.810747
Traing Log at fold-4 epoch-22: mse-0.169812, rmse-0.412083, r2-0.688506
Valid at fold-4: mse-0.22228
Traing Log at fold-4 epoch-23: mse-0.166284, rmse-0.40778, r2-0.697435
Valid at fold-4: mse-0.20103
Update best_mse, Valid at fold-4 epoch-23: mse-0.20103, rmse-0.448363, ci--1, r2-0.698966, pearson-0.842727, spearman-0.820621
Traing Log at fold-4 epoch-24: mse-0.161331, rmse-0.40166, r2-0.708746
Valid at fold-4: mse-0.196658
Update best_mse, Valid at fold-4 epoch-24: mse-0.196658, rmse-0.443462, ci--1, r2-0.705512, pearson-0.844252, spearman-0.824211
Traing Log at fold-4 epoch-25: mse-0.157034, rmse-0.396275, r2-0.718685
Valid at fold-4: mse-0.201797
Traing Log at fold-4 epoch-26: mse-0.154805, rmse-0.393453, r2-0.724382
Valid at fold-4: mse-0.190659
Update best_mse, Valid at fold-4 epoch-26: mse-0.190659, rmse-0.436646, ci--1, r2-0.714495, pearson-0.849009, spearman-0.830696
Traing Log at fold-4 epoch-27: mse-0.149839, rmse-0.38709, r2-0.734832
Valid at fold-4: mse-0.198048
Traing Log at fold-4 epoch-28: mse-0.148593, rmse-0.385478, r2-0.737938
Valid at fold-4: mse-0.197887
Traing Log at fold-4 epoch-29: mse-0.143489, rmse-0.3788, r2-0.749112
Valid at fold-4: mse-0.194291
Traing Log at fold-4 epoch-30: mse-0.140635, rmse-0.375014, r2-0.75545
Valid at fold-4: mse-0.187111
Update best_mse, Valid at fold-4 epoch-30: mse-0.187111, rmse-0.432563, ci--1, r2-0.719808, pearson-0.850225, spearman-0.826919
Traing Log at fold-4 epoch-31: mse-0.138691, rmse-0.372412, r2-0.759278
Valid at fold-4: mse-0.183924
Update best_mse, Valid at fold-4 epoch-31: mse-0.183924, rmse-0.428864, ci--1, r2-0.72458, pearson-0.856138, spearman-0.836227
Traing Log at fold-4 epoch-32: mse-0.136343, rmse-0.369247, r2-0.764803
Valid at fold-4: mse-0.191199
Traing Log at fold-4 epoch-33: mse-0.133022, rmse-0.364722, r2-0.771366
Valid at fold-4: mse-0.182696
Update best_mse, Valid at fold-4 epoch-33: mse-0.182696, rmse-0.42743, ci--1, r2-0.726419, pearson-0.859053, spearman-0.835588
Traing Log at fold-4 epoch-34: mse-0.132446, rmse-0.363932, r2-0.773026
Valid at fold-4: mse-0.187203
Traing Log at fold-4 epoch-35: mse-0.128181, rmse-0.358024, r2-0.781591
Valid at fold-4: mse-0.18833
Traing Log at fold-4 epoch-36: mse-0.12727, rmse-0.35675, r2-0.784083
Valid at fold-4: mse-0.185872
Traing Log at fold-4 epoch-37: mse-0.123161, rmse-0.350943, r2-0.791771
Valid at fold-4: mse-0.192536
Traing Log at fold-4 epoch-38: mse-0.123519, rmse-0.351453, r2-0.791435
Valid at fold-4: mse-0.18811
Traing Log at fold-4 epoch-39: mse-0.118936, rmse-0.344871, r2-0.800656
Valid at fold-4: mse-0.186047
Traing Log at fold-4 epoch-40: mse-0.117674, rmse-0.343037, r2-0.80311
Valid at fold-4: mse-0.180713
Update best_mse, Valid at fold-4 epoch-40: mse-0.180713, rmse-0.425104, ci--1, r2-0.729388, pearson-0.859542, spearman-0.839299
Traing Log at fold-4 epoch-41: mse-0.115754, rmse-0.340227, r2-0.807094
Valid at fold-4: mse-0.179276
Update best_mse, Valid at fold-4 epoch-41: mse-0.179276, rmse-0.423411, ci--1, r2-0.73154, pearson-0.861116, spearman-0.840632
Traing Log at fold-4 epoch-42: mse-0.114771, rmse-0.338778, r2-0.809008
Valid at fold-4: mse-0.185806
Traing Log at fold-4 epoch-43: mse-0.111745, rmse-0.334283, r2-0.814539
Valid at fold-4: mse-0.182666
Traing Log at fold-4 epoch-44: mse-0.110887, rmse-0.332996, r2-0.816832
Valid at fold-4: mse-0.177398
Update best_mse, Valid at fold-4 epoch-44: mse-0.177398, rmse-0.421186, ci--1, r2-0.734353, pearson-0.862476, spearman-0.845085
Traing Log at fold-4 epoch-45: mse-0.109088, rmse-0.330284, r2-0.820023
Valid at fold-4: mse-0.175523
Update best_mse, Valid at fold-4 epoch-45: mse-0.175523, rmse-0.418955, ci--1, r2-0.73716, pearson-0.864125, spearman-0.84012
Traing Log at fold-4 epoch-46: mse-0.10705, rmse-0.327185, r2-0.824054
Valid at fold-4: mse-0.175226
Update best_mse, Valid at fold-4 epoch-46: mse-0.175226, rmse-0.4186, ci--1, r2-0.737606, pearson-0.864551, spearman-0.844164
Traing Log at fold-4 epoch-47: mse-0.105979, rmse-0.325544, r2-0.825996
Valid at fold-4: mse-0.178319
Traing Log at fold-4 epoch-48: mse-0.103075, rmse-0.321052, r2-0.831672
Valid at fold-4: mse-0.180135
Traing Log at fold-4 epoch-49: mse-0.102567, rmse-0.320261, r2-0.83291
Valid at fold-4: mse-0.180581
Traing Log at fold-4 epoch-50: mse-0.101403, rmse-0.318438, r2-0.834965
Valid at fold-4: mse-0.176288
Traing Log at fold-4 epoch-51: mse-0.099348, rmse-0.315196, r2-0.838782
Valid at fold-4: mse-0.173897
Update best_mse, Valid at fold-4 epoch-51: mse-0.173897, rmse-0.417009, ci--1, r2-0.739596, pearson-0.865479, spearman-0.845609
Traing Log at fold-4 epoch-52: mse-0.098985, rmse-0.314619, r2-0.839314
Valid at fold-4: mse-0.185477
Traing Log at fold-4 epoch-53: mse-0.096463, rmse-0.310585, r2-0.844037
Valid at fold-4: mse-0.175479
Traing Log at fold-4 epoch-54: mse-0.095893, rmse-0.309666, r2-0.845167
Valid at fold-4: mse-0.177916
Traing Log at fold-4 epoch-55: mse-0.094097, rmse-0.306752, r2-0.848689
Valid at fold-4: mse-0.168827
Update best_mse, Valid at fold-4 epoch-55: mse-0.168827, rmse-0.410886, ci--1, r2-0.747188, pearson-0.866635, spearman-0.850352
Traing Log at fold-4 epoch-56: mse-0.093019, rmse-0.30499, r2-0.850462
Valid at fold-4: mse-0.172156
Traing Log at fold-4 epoch-57: mse-0.091594, rmse-0.302645, r2-0.852892
Valid at fold-4: mse-0.18035
Traing Log at fold-4 epoch-58: mse-0.090671, rmse-0.301116, r2-0.854882
Valid at fold-4: mse-0.180563
Traing Log at fold-4 epoch-59: mse-0.088903, rmse-0.298165, r2-0.858227
Valid at fold-4: mse-0.168462
Update best_mse, Valid at fold-4 epoch-59: mse-0.168462, rmse-0.410441, ci--1, r2-0.747735, pearson-0.868221, spearman-0.847173
Traing Log at fold-4 epoch-60: mse-0.087922, rmse-0.296516, r2-0.859725
Valid at fold-4: mse-0.171649
Traing Log at fold-4 epoch-61: mse-0.086146, rmse-0.293507, r2-0.862831
Valid at fold-4: mse-0.181678
Traing Log at fold-4 epoch-62: mse-0.086087, rmse-0.293406, r2-0.863461
Valid at fold-4: mse-0.168443
Update best_mse, Valid at fold-4 epoch-62: mse-0.168443, rmse-0.410418, ci--1, r2-0.747763, pearson-0.870765, spearman-0.854089
Traing Log at fold-4 epoch-63: mse-0.083545, rmse-0.289041, r2-0.867546
Valid at fold-4: mse-0.175608
Traing Log at fold-4 epoch-64: mse-0.083013, rmse-0.28812, r2-0.868758
Valid at fold-4: mse-0.166034
Update best_mse, Valid at fold-4 epoch-64: mse-0.166034, rmse-0.407473, ci--1, r2-0.75137, pearson-0.872343, spearman-0.853714
Traing Log at fold-4 epoch-65: mse-0.081529, rmse-0.285533, r2-0.871326
Valid at fold-4: mse-0.182297
Traing Log at fold-4 epoch-66: mse-0.080685, rmse-0.284052, r2-0.872896
Valid at fold-4: mse-0.174248
Traing Log at fold-4 epoch-67: mse-0.078639, rmse-0.280427, r2-0.87625
Valid at fold-4: mse-0.168515
Traing Log at fold-4 epoch-68: mse-0.077864, rmse-0.279041, r2-0.877878
Valid at fold-4: mse-0.168857
Traing Log at fold-4 epoch-69: mse-0.077555, rmse-0.278486, r2-0.878345
Valid at fold-4: mse-0.168674
Traing Log at fold-4 epoch-70: mse-0.076854, rmse-0.277226, r2-0.879522
Valid at fold-4: mse-0.174753
Traing Log at fold-4 epoch-71: mse-0.074991, rmse-0.273844, r2-0.882838
Valid at fold-4: mse-0.164938
Update best_mse, Valid at fold-4 epoch-71: mse-0.164938, rmse-0.406126, ci--1, r2-0.753011, pearson-0.871642, spearman-0.852923
Traing Log at fold-4 epoch-72: mse-0.074071, rmse-0.27216, r2-0.884379
Valid at fold-4: mse-0.174099
Traing Log at fold-4 epoch-73: mse-0.073001, rmse-0.270187, r2-0.886389
Valid at fold-4: mse-0.162125
Update best_mse, Valid at fold-4 epoch-73: mse-0.162125, rmse-0.402648, ci--1, r2-0.757224, pearson-0.874054, spearman-0.858452
Traing Log at fold-4 epoch-74: mse-0.071274, rmse-0.266971, r2-0.889004
Valid at fold-4: mse-0.165249
Traing Log at fold-4 epoch-75: mse-0.071448, rmse-0.267297, r2-0.889013
Valid at fold-4: mse-0.173628
Traing Log at fold-4 epoch-76: mse-0.070631, rmse-0.265765, r2-0.890388
Valid at fold-4: mse-0.17073
Traing Log at fold-4 epoch-77: mse-0.068891, rmse-0.26247, r2-0.893468
Valid at fold-4: mse-0.159388
Update best_mse, Valid at fold-4 epoch-77: mse-0.159388, rmse-0.399234, ci--1, r2-0.761322, pearson-0.876027, spearman-0.859011
Traing Log at fold-4 epoch-78: mse-0.068384, rmse-0.261503, r2-0.894086
Valid at fold-4: mse-0.163899
Traing Log at fold-4 epoch-79: mse-0.067977, rmse-0.260723, r2-0.894944
Valid at fold-4: mse-0.168107
Traing Log at fold-4 epoch-80: mse-0.066556, rmse-0.257984, r2-0.897444
Valid at fold-4: mse-0.164368
Traing Log at fold-4 epoch-81: mse-0.065994, rmse-0.256892, r2-0.898191
Valid at fold-4: mse-0.171663
Traing Log at fold-4 epoch-82: mse-0.064995, rmse-0.254941, r2-0.900046
Valid at fold-4: mse-0.16424
Traing Log at fold-4 epoch-83: mse-0.063505, rmse-0.252002, r2-0.902299
Valid at fold-4: mse-0.167179
Traing Log at fold-4 epoch-84: mse-0.063366, rmse-0.251726, r2-0.902841
Valid at fold-4: mse-0.162334
Traing Log at fold-4 epoch-85: mse-0.062329, rmse-0.249658, r2-0.904434
Valid at fold-4: mse-0.164049
Traing Log at fold-4 epoch-86: mse-0.060823, rmse-0.246623, r2-0.90691
Valid at fold-4: mse-0.164145
Traing Log at fold-4 epoch-87: mse-0.060789, rmse-0.246554, r2-0.907075
Valid at fold-4: mse-0.169256
Traing Log at fold-4 epoch-88: mse-0.05987, rmse-0.244683, r2-0.908555
Valid at fold-4: mse-0.163543
Traing Log at fold-4 epoch-89: mse-0.059278, rmse-0.243471, r2-0.909429
Valid at fold-4: mse-0.165802
Traing Log at fold-4 epoch-90: mse-0.058145, rmse-0.241132, r2-0.911365
Valid at fold-4: mse-0.165628
Traing Log at fold-4 epoch-91: mse-0.057188, rmse-0.23914, r2-0.912984
Valid at fold-4: mse-0.16051
Traing Log at fold-4 epoch-92: mse-0.05768, rmse-0.240167, r2-0.91219
Valid at fold-4: mse-0.167548
Traing Log at fold-4 epoch-93: mse-0.056421, rmse-0.237531, r2-0.914247
Valid at fold-4: mse-0.162643
Traing Log at fold-4 epoch-94: mse-0.055419, rmse-0.235412, r2-0.915887
Valid at fold-4: mse-0.165292
Traing Log at fold-4 epoch-95: mse-0.05491, rmse-0.234328, r2-0.91674
Valid at fold-4: mse-0.160035
Traing Log at fold-4 epoch-96: mse-0.054025, rmse-0.232432, r2-0.91811
Valid at fold-4: mse-0.162182
Traing Log at fold-4 epoch-97: mse-0.052852, rmse-0.229896, r2-0.920124
Valid at fold-4: mse-0.164318
Traing Log at fold-4 epoch-98: mse-0.052145, rmse-0.228352, r2-0.921285
Valid at fold-4: mse-0.160087
Traing stop at epoch-98, model save at-./savemodel/kiba-novel-prot-fold4-Nov13_07-14-05.pth
Save log over at ./log/Nov13_07-14-05-kiba-novel-prot-fold4.csv

============================================================
Testing fold 4 with best model...
============================================================
/home/minhtrin/.conda/envs/LLMDTA/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Test at fold-4, mse: 0.285567, rmse: 0.534385, ci: 0.782023, r2: 0.588658, pearson: 0.773155, spearman: 0.706507

Fold 4 results saved to: ./log/Test-kiba-novel-prot-fold4-Nov13_07-14-05.csv
============================================================
Training fold 4 completed successfully!
============================================================
wandb: updating run metadata
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading config.yaml
wandb: uploading history steps 228-228, summary, console lines 244-249
wandb: 
wandb: Run history:
wandb:      best_valid/mse â–ˆâ–†â–†â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:  best_valid/pearson â–â–ƒâ–ƒâ–„â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:       best_valid/r2 â–â–ƒâ–ƒâ–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:     best_valid/rmse â–ˆâ–‡â–‡â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–
wandb: best_valid/spearman â–â–‚â–ƒâ–ƒâ–…â–…â–…â–†â–†â–†â–†â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:             test/ci â–
wandb:            test/mse â–
wandb:        test/pearson â–
wandb:             test/r2 â–
wandb:                 +13 ...
wandb: 
wandb: Run summary:
wandb:      best_valid/mse 0.15939
wandb:  best_valid/pearson 0.87603
wandb:       best_valid/r2 0.76132
wandb:     best_valid/rmse 0.39923
wandb: best_valid/spearman 0.85901
wandb:               epoch 98
wandb:       final_test_ci 0.78202
wandb:      final_test_mse 0.28557
wandb:  final_test_pearson 0.77316
wandb:       final_test_r2 0.58866
wandb:                 +19 ...
wandb: 
wandb: ğŸš€ View run kiba-novel-prot-fold4 at: https://wandb.ai/tringuyen/LLMDTA/runs/kvn7pvfs
wandb: â­ï¸ View project at: https://wandb.ai/tringuyen/LLMDTA
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251113_071406-kvn7pvfs/logs
Weights & Biases run finished

Training for fold 4 completed successfully.
Python script exit code: 0
==========================================
End Time: Fri Nov 14 01:10:28 AM AEDT 2025
==========================================
