==========================================
Job ID: 2012969
Array Task ID: 2
Node: v100-f-22
Start Time: Wed Nov 12 04:42:40 PM AEDT 2025
==========================================
Activating conda environment...
Conda environment activated: LLMDTA
Checking GPU...
Wed Nov 12 16:42:41 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           On  |   00000000:18:00.0 Off |                    0 |
| N/A   39C    P0             46W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Setting environment variables...

Starting training for fold 2...


============================================================
Starting training for Fold 2
Dataset: davis, Running Set: novel-prot
Epochs: 200, Batch Size: 16
============================================================

Executing: python -u code/train.py --fold 2 --cuda 0 --dataset davis --running_set novel-prot --epochs 200 --batch_size 16 --wandb_project LLMDTA
============================================================
Training Fold 2/4
Dataset: davis-novel-prot
Device: cuda (CUDA_VISIBLE_DEVICES=0)
Pretrain-./data/davis/davis_drug_pretrain.pkl
Pretrain-./data/davis/davis_esm_pretrain.pkl
============================================================
wandb: Currently logged in as: tringuyen to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: setting up run ypyvx8v4
wandb: Tracking run with wandb version 0.23.0
wandb: Run data is saved locally in /vast/minhtrin/DTA/Temp/wandb/run-20251112_164253-ypyvx8v4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run davis-novel-prot-fold2
wandb: â­ï¸ View project at https://wandb.ai/tringuyen/LLMDTA
wandb: ğŸš€ View run at https://wandb.ai/tringuyen/LLMDTA/runs/ypyvx8v4
Weights & Biases initialized: LLMDTA
Loading fold 2 data...
  Train: ./data/dta-5fold-dataset/davis/novel-prot/fold_2_train.csv
  Valid: ./data/dta-5fold-dataset/davis/novel-prot/fold_2_valid.csv
  Test:  ./data/dta-5fold-dataset/davis/novel-prot/fold_2_test.csv
Dataset loaded: 19257 train, 4815 valid, 5984 test samples
/home/minhtrin/.conda/envs/LLMDTA/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Traing Log at fold-2 epoch-1: mse-1.73358, rmse-1.316655, r2--0.212227
Valid at fold-2: mse-2.225564
Update best_mse, Valid at fold-2 epoch-1: mse-2.225564, rmse-1.491832, ci--1, r2--1.861095, pearson-0.281979, spearman-0.227072
Traing Log at fold-2 epoch-2: mse-0.72296, rmse-0.85027, r2--0.2672
Valid at fold-2: mse-0.87412
Update best_mse, Valid at fold-2 epoch-2: mse-0.87412, rmse-0.934944, ci--1, r2--0.123734, pearson-0.583464, spearman-0.481994
Traing Log at fold-2 epoch-3: mse-0.574102, rmse-0.757695, r2--0.105492
Valid at fold-2: mse-0.715408
Update best_mse, Valid at fold-2 epoch-3: mse-0.715408, rmse-0.845818, ci--1, r2-0.0803, pearson-0.636822, spearman-0.52836
Traing Log at fold-2 epoch-4: mse-0.49265, rmse-0.70189, r2--0.00392
Valid at fold-2: mse-0.569199
Update best_mse, Valid at fold-2 epoch-4: mse-0.569199, rmse-0.754453, ci--1, r2-0.268261, pearson-0.694507, spearman-0.553041
Traing Log at fold-2 epoch-5: mse-0.428169, rmse-0.654346, r2-0.110905
Valid at fold-2: mse-0.605113
Traing Log at fold-2 epoch-6: mse-0.388979, rmse-0.623682, r2-0.185079
Valid at fold-2: mse-0.420876
Update best_mse, Valid at fold-2 epoch-6: mse-0.420876, rmse-0.64875, ci--1, r2-0.458939, pearson-0.73579, spearman-0.599793
Traing Log at fold-2 epoch-7: mse-0.35471, rmse-0.595576, r2-0.263612
Valid at fold-2: mse-0.349863
Update best_mse, Valid at fold-2 epoch-7: mse-0.349863, rmse-0.591492, ci--1, r2-0.55023, pearson-0.760949, spearman-0.611202
Traing Log at fold-2 epoch-8: mse-0.324296, rmse-0.56947, r2-0.348373
Valid at fold-2: mse-0.325187
Update best_mse, Valid at fold-2 epoch-8: mse-0.325187, rmse-0.570251, ci--1, r2-0.581953, pearson-0.772002, spearman-0.638165
Traing Log at fold-2 epoch-9: mse-0.309396, rmse-0.556234, r2-0.385932
Valid at fold-2: mse-0.361937
Traing Log at fold-2 epoch-10: mse-0.294051, rmse-0.542265, r2-0.423013
Valid at fold-2: mse-0.322398
Update best_mse, Valid at fold-2 epoch-10: mse-0.322398, rmse-0.567801, ci--1, r2-0.585538, pearson-0.785648, spearman-0.643226
Traing Log at fold-2 epoch-11: mse-0.277521, rmse-0.526803, r2-0.470254
Valid at fold-2: mse-0.318077
Update best_mse, Valid at fold-2 epoch-11: mse-0.318077, rmse-0.563983, ci--1, r2-0.591093, pearson-0.778585, spearman-0.649139
Traing Log at fold-2 epoch-12: mse-0.272013, rmse-0.521548, r2-0.479926
Valid at fold-2: mse-0.285757
Update best_mse, Valid at fold-2 epoch-12: mse-0.285757, rmse-0.534563, ci--1, r2-0.632642, pearson-0.802135, spearman-0.656245
Traing Log at fold-2 epoch-13: mse-0.26056, rmse-0.510451, r2-0.511365
Valid at fold-2: mse-0.28378
Update best_mse, Valid at fold-2 epoch-13: mse-0.28378, rmse-0.53271, ci--1, r2-0.635183, pearson-0.806385, spearman-0.654893
Traing Log at fold-2 epoch-14: mse-0.251683, rmse-0.501681, r2-0.537461
Valid at fold-2: mse-0.304573
Traing Log at fold-2 epoch-15: mse-0.241503, rmse-0.49143, r2-0.558282
Valid at fold-2: mse-0.266533
Update best_mse, Valid at fold-2 epoch-15: mse-0.266533, rmse-0.516268, ci--1, r2-0.657356, pearson-0.812565, spearman-0.668159
Traing Log at fold-2 epoch-16: mse-0.232711, rmse-0.482401, r2-0.581034
Valid at fold-2: mse-0.295295
Traing Log at fold-2 epoch-17: mse-0.223209, rmse-0.47245, r2-0.604102
Valid at fold-2: mse-0.250958
Update best_mse, Valid at fold-2 epoch-17: mse-0.250958, rmse-0.500957, ci--1, r2-0.677379, pearson-0.824105, spearman-0.674432
Traing Log at fold-2 epoch-18: mse-0.216789, rmse-0.465606, r2-0.617963
Valid at fold-2: mse-0.268714
Traing Log at fold-2 epoch-19: mse-0.21468, rmse-0.463335, r2-0.626303
Valid at fold-2: mse-0.252962
Traing Log at fold-2 epoch-20: mse-0.201869, rmse-0.449298, r2-0.653998
Valid at fold-2: mse-0.244507
Update best_mse, Valid at fold-2 epoch-20: mse-0.244507, rmse-0.494477, ci--1, r2-0.685671, pearson-0.828911, spearman-0.680711
Traing Log at fold-2 epoch-21: mse-0.198407, rmse-0.445429, r2-0.662442
Valid at fold-2: mse-0.268077
Traing Log at fold-2 epoch-22: mse-0.190901, rmse-0.436923, r2-0.678734
Valid at fold-2: mse-0.255861
Traing Log at fold-2 epoch-23: mse-0.189192, rmse-0.434962, r2-0.681794
Valid at fold-2: mse-0.247483
Traing Log at fold-2 epoch-24: mse-0.180881, rmse-0.425301, r2-0.698839
Valid at fold-2: mse-0.254267
Traing Log at fold-2 epoch-25: mse-0.178108, rmse-0.422029, r2-0.707595
Valid at fold-2: mse-0.23409
Update best_mse, Valid at fold-2 epoch-25: mse-0.23409, rmse-0.483828, ci--1, r2-0.699064, pearson-0.837344, spearman-0.679085
Traing Log at fold-2 epoch-26: mse-0.173629, rmse-0.416689, r2-0.716166
Valid at fold-2: mse-0.253432
Traing Log at fold-2 epoch-27: mse-0.16793, rmse-0.409792, r2-0.727042
Valid at fold-2: mse-0.231474
Update best_mse, Valid at fold-2 epoch-27: mse-0.231474, rmse-0.481117, ci--1, r2-0.702427, pearson-0.8433, spearman-0.686669
Traing Log at fold-2 epoch-28: mse-0.164665, rmse-0.40579, r2-0.734182
Valid at fold-2: mse-0.240666
Traing Log at fold-2 epoch-29: mse-0.162509, rmse-0.403124, r2-0.739062
Valid at fold-2: mse-0.232992
Traing Log at fold-2 epoch-30: mse-0.155679, rmse-0.394561, r2-0.753077
Valid at fold-2: mse-0.24822
Traing Log at fold-2 epoch-31: mse-0.155702, rmse-0.39459, r2-0.751752
Valid at fold-2: mse-0.228879
Update best_mse, Valid at fold-2 epoch-31: mse-0.228879, rmse-0.478413, ci--1, r2-0.705762, pearson-0.841713, spearman-0.685477
Traing Log at fold-2 epoch-32: mse-0.151624, rmse-0.389389, r2-0.761177
Valid at fold-2: mse-0.262387
Traing Log at fold-2 epoch-33: mse-0.145012, rmse-0.380804, r2-0.772347
Valid at fold-2: mse-0.236477
Traing Log at fold-2 epoch-34: mse-0.141908, rmse-0.376706, r2-0.77815
Valid at fold-2: mse-0.239235
Traing Log at fold-2 epoch-35: mse-0.14425, rmse-0.379803, r2-0.774901
Valid at fold-2: mse-0.220729
Update best_mse, Valid at fold-2 epoch-35: mse-0.220729, rmse-0.469818, ci--1, r2-0.71624, pearson-0.847725, spearman-0.684344
Traing Log at fold-2 epoch-36: mse-0.138633, rmse-0.372334, r2-0.786202
Valid at fold-2: mse-0.218987
Update best_mse, Valid at fold-2 epoch-36: mse-0.218987, rmse-0.467961, ci--1, r2-0.718479, pearson-0.848689, spearman-0.682594
Traing Log at fold-2 epoch-37: mse-0.136348, rmse-0.369254, r2-0.788171
Valid at fold-2: mse-0.228045
Traing Log at fold-2 epoch-38: mse-0.132181, rmse-0.363567, r2-0.798212
Valid at fold-2: mse-0.268024
Traing Log at fold-2 epoch-39: mse-0.13129, rmse-0.36234, r2-0.797377
Valid at fold-2: mse-0.224931
Traing Log at fold-2 epoch-40: mse-0.1301, rmse-0.360693, r2-0.801746
Valid at fold-2: mse-0.232953
Traing Log at fold-2 epoch-41: mse-0.126918, rmse-0.356256, r2-0.806786
Valid at fold-2: mse-0.21856
Update best_mse, Valid at fold-2 epoch-41: mse-0.21856, rmse-0.467504, ci--1, r2-0.719029, pearson-0.848543, spearman-0.678561
Traing Log at fold-2 epoch-42: mse-0.122358, rmse-0.349797, r2-0.814591
Valid at fold-2: mse-0.228564
Traing Log at fold-2 epoch-43: mse-0.120937, rmse-0.34776, r2-0.817483
Valid at fold-2: mse-0.229666
Traing Log at fold-2 epoch-44: mse-0.118754, rmse-0.344608, r2-0.821733
Valid at fold-2: mse-0.221911
Traing Log at fold-2 epoch-45: mse-0.11733, rmse-0.342535, r2-0.823983
Valid at fold-2: mse-0.229625
Traing Log at fold-2 epoch-46: mse-0.116377, rmse-0.34114, r2-0.825134
Valid at fold-2: mse-0.226337
Traing Log at fold-2 epoch-47: mse-0.116606, rmse-0.341476, r2-0.825642
Valid at fold-2: mse-0.201298
Update best_mse, Valid at fold-2 epoch-47: mse-0.201298, rmse-0.448663, ci--1, r2-0.741219, pearson-0.863745, spearman-0.695969
Traing Log at fold-2 epoch-48: mse-0.11653, rmse-0.341365, r2-0.825977
Valid at fold-2: mse-0.230908
Traing Log at fold-2 epoch-49: mse-0.113348, rmse-0.336672, r2-0.830944
Valid at fold-2: mse-0.219376
Traing Log at fold-2 epoch-50: mse-0.112824, rmse-0.335893, r2-0.832603
Valid at fold-2: mse-0.231733
Traing Log at fold-2 epoch-51: mse-0.111209, rmse-0.333479, r2-0.834435
Valid at fold-2: mse-0.222322
Traing Log at fold-2 epoch-52: mse-0.108231, rmse-0.328984, r2-0.840029
Valid at fold-2: mse-0.216939
Traing Log at fold-2 epoch-53: mse-0.10425, rmse-0.322878, r2-0.846797
Valid at fold-2: mse-0.257629
Traing Log at fold-2 epoch-54: mse-0.105152, rmse-0.324272, r2-0.844421
Valid at fold-2: mse-0.212959
Traing Log at fold-2 epoch-55: mse-0.102961, rmse-0.320876, r2-0.849196
Valid at fold-2: mse-0.221609
Traing Log at fold-2 epoch-56: mse-0.103086, rmse-0.32107, r2-0.848294
Valid at fold-2: mse-0.224061
Traing Log at fold-2 epoch-57: mse-0.101206, rmse-0.318128, r2-0.851537
Valid at fold-2: mse-0.211996
Traing Log at fold-2 epoch-58: mse-0.098212, rmse-0.313389, r2-0.856735
Valid at fold-2: mse-0.214541
Traing Log at fold-2 epoch-59: mse-0.097552, rmse-0.312333, r2-0.858217
Valid at fold-2: mse-0.217691
Traing Log at fold-2 epoch-60: mse-0.098038, rmse-0.31311, r2-0.856922
Valid at fold-2: mse-0.214394
Traing Log at fold-2 epoch-61: mse-0.095048, rmse-0.308298, r2-0.861501
Valid at fold-2: mse-0.210999
Traing Log at fold-2 epoch-62: mse-0.094998, rmse-0.308217, r2-0.862171
Valid at fold-2: mse-0.220578
Traing Log at fold-2 epoch-63: mse-0.092255, rmse-0.303735, r2-0.866376
Valid at fold-2: mse-0.221942
Traing Log at fold-2 epoch-64: mse-0.094396, rmse-0.307239, r2-0.863215
Valid at fold-2: mse-0.217333
Traing Log at fold-2 epoch-65: mse-0.092095, rmse-0.303471, r2-0.866831
Valid at fold-2: mse-0.214487
Traing Log at fold-2 epoch-66: mse-0.093568, rmse-0.305889, r2-0.864868
Valid at fold-2: mse-0.225393
Traing Log at fold-2 epoch-67: mse-0.089677, rmse-0.299461, r2-0.871036
Valid at fold-2: mse-0.213074
Traing Log at fold-2 epoch-68: mse-0.088859, rmse-0.298092, r2-0.871294
Valid at fold-2: mse-0.214982
Traing stop at epoch-68, model save at-./savemodel/davis-novel-prot-fold2-Nov12_16-42-52.pth
Save log over at ./log/Nov12_16-42-52-davis-novel-prot-fold2.csv

============================================================
Testing fold 2 with best model...
============================================================
/home/minhtrin/.conda/envs/LLMDTA/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Test at fold-2, mse: 0.451294, rmse: 0.671784, ci: 0.81918, r2: 0.489481, pearson: 0.705074, spearman: 0.619485

Fold 2 results saved to: ./log/Test-davis-novel-prot-fold2-Nov12_16-42-52.csv
============================================================
Training fold 2 completed successfully!
============================================================
wandb: updating run metadata
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading history steps 157-157, summary, console lines 173-178
wandb: 
wandb: Run history:
wandb:      best_valid/mse â–ˆâ–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  best_valid/pearson â–â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:       best_valid/r2 â–â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:     best_valid/rmse â–ˆâ–„â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb: best_valid/spearman â–â–…â–…â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:             test/ci â–
wandb:            test/mse â–
wandb:        test/pearson â–
wandb:             test/r2 â–
wandb:                 +13 ...
wandb: 
wandb: Run summary:
wandb:      best_valid/mse 0.2013
wandb:  best_valid/pearson 0.86374
wandb:       best_valid/r2 0.74122
wandb:     best_valid/rmse 0.44866
wandb: best_valid/spearman 0.69597
wandb:               epoch 68
wandb:       final_test_ci 0.81918
wandb:      final_test_mse 0.45129
wandb:  final_test_pearson 0.70507
wandb:       final_test_r2 0.48948
wandb:                 +19 ...
wandb: 
wandb: ğŸš€ View run davis-novel-prot-fold2 at: https://wandb.ai/tringuyen/LLMDTA/runs/ypyvx8v4
wandb: â­ï¸ View project at: https://wandb.ai/tringuyen/LLMDTA
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251112_164253-ypyvx8v4/logs
Weights & Biases run finished

Training for fold 2 completed successfully.
Python script exit code: 0
==========================================
End Time: Wed Nov 12 10:31:09 PM AEDT 2025
==========================================
