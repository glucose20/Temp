==========================================
Job ID: 2012960
Array Task ID: 4
Node: v100-f-14
Start Time: Wed Nov 12 04:41:40 PM AEDT 2025
==========================================
Activating conda environment...
Conda environment activated: LLMDTA
Checking GPU...
Wed Nov 12 16:41:40 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           On  |   00000000:AF:00.0 Off |                    0 |
| N/A   38C    P0             43W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Setting environment variables...

Starting training for fold 4...


============================================================
Starting training for Fold 4
Dataset: davis, Running Set: warm
Epochs: 200, Batch Size: 16
============================================================

Executing: python -u code/train.py --fold 4 --cuda 0 --dataset davis --running_set warm --epochs 200 --batch_size 16 --wandb_project LLMDTA
============================================================
Training Fold 4/4
Dataset: davis-warm
Device: cuda (CUDA_VISIBLE_DEVICES=0)
Pretrain-./data/davis/davis_drug_pretrain.pkl
Pretrain-./data/davis/davis_esm_pretrain.pkl
============================================================
wandb: Currently logged in as: tringuyen to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: setting up run n1vw7dze
wandb: Tracking run with wandb version 0.23.0
wandb: Run data is saved locally in /vast/minhtrin/DTA/Temp/wandb/run-20251112_164146-n1vw7dze
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run davis-warm-fold4
wandb: â­ï¸ View project at https://wandb.ai/tringuyen/LLMDTA
wandb: ğŸš€ View run at https://wandb.ai/tringuyen/LLMDTA/runs/n1vw7dze
Weights & Biases initialized: LLMDTA
Loading fold 4 data...
  Train: ./data/dta-5fold-dataset/davis/warm/fold_4_train.csv
  Valid: ./data/dta-5fold-dataset/davis/warm/fold_4_valid.csv
  Test:  ./data/dta-5fold-dataset/davis/warm/fold_4_test.csv
Dataset loaded: 19235 train, 4809 valid, 6012 test samples
/home/minhtrin/.conda/envs/LLMDTA/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Traing Log at fold-4 epoch-1: mse-1.776204, rmse-1.332743, r2--0.217111
Valid at fold-4: mse-1.044638
Update best_mse, Valid at fold-4 epoch-1: mse-1.044638, rmse-1.022076, ci--1, r2--0.326108, pearson-0.414853, spearman-0.380765
Traing Log at fold-4 epoch-2: mse-0.76203, rmse-0.872943, r2--0.300777
Valid at fold-4: mse-0.812321
Update best_mse, Valid at fold-4 epoch-2: mse-0.812321, rmse-0.901289, ci--1, r2--0.031195, pearson-0.610055, spearman-0.536959
Traing Log at fold-4 epoch-3: mse-0.590847, rmse-0.768666, r2--0.139433
Valid at fold-4: mse-1.519311
Traing Log at fold-4 epoch-4: mse-0.518818, rmse-0.72029, r2--0.046033
Valid at fold-4: mse-0.605695
Update best_mse, Valid at fold-4 epoch-4: mse-0.605695, rmse-0.778264, ci--1, r2-0.231105, pearson-0.634657, spearman-0.538039
Traing Log at fold-4 epoch-5: mse-0.446732, rmse-0.668381, r2-0.081431
Valid at fold-4: mse-0.497607
Update best_mse, Valid at fold-4 epoch-5: mse-0.497607, rmse-0.705413, ci--1, r2-0.368317, pearson-0.646871, spearman-0.541689
Traing Log at fold-4 epoch-6: mse-0.40221, rmse-0.6342, r2-0.152706
Valid at fold-4: mse-0.539149
Traing Log at fold-4 epoch-7: mse-0.374915, rmse-0.612303, r2-0.221305
Valid at fold-4: mse-0.427587
Update best_mse, Valid at fold-4 epoch-7: mse-0.427587, rmse-0.653901, ci--1, r2-0.457203, pearson-0.68081, spearman-0.549506
Traing Log at fold-4 epoch-8: mse-0.353583, rmse-0.594628, r2-0.267638
Valid at fold-4: mse-0.374017
Update best_mse, Valid at fold-4 epoch-8: mse-0.374017, rmse-0.611569, ci--1, r2-0.525207, pearson-0.743198, spearman-0.601738
Traing Log at fold-4 epoch-9: mse-0.325706, rmse-0.570707, r2-0.348175
Valid at fold-4: mse-0.359214
Update best_mse, Valid at fold-4 epoch-9: mse-0.359214, rmse-0.599345, ci--1, r2-0.543998, pearson-0.754622, spearman-0.628553
Traing Log at fold-4 epoch-10: mse-0.313636, rmse-0.560032, r2-0.377159
Valid at fold-4: mse-0.310494
Update best_mse, Valid at fold-4 epoch-10: mse-0.310494, rmse-0.55722, ci--1, r2-0.605846, pearson-0.781281, spearman-0.648131
Traing Log at fold-4 epoch-11: mse-0.293064, rmse-0.541354, r2-0.433724
Valid at fold-4: mse-0.286722
Update best_mse, Valid at fold-4 epoch-11: mse-0.286722, rmse-0.535464, ci--1, r2-0.636024, pearson-0.7977, spearman-0.657955
Traing Log at fold-4 epoch-12: mse-0.281648, rmse-0.530705, r2-0.471692
Valid at fold-4: mse-0.308493
Traing Log at fold-4 epoch-13: mse-0.268447, rmse-0.518119, r2-0.500661
Valid at fold-4: mse-0.333263
Traing Log at fold-4 epoch-14: mse-0.258515, rmse-0.508444, r2-0.526225
Valid at fold-4: mse-0.302083
Traing Log at fold-4 epoch-15: mse-0.24404, rmse-0.494005, r2-0.566176
Valid at fold-4: mse-0.293887
Traing Log at fold-4 epoch-16: mse-0.244197, rmse-0.494163, r2-0.563731
Valid at fold-4: mse-0.283105
Update best_mse, Valid at fold-4 epoch-16: mse-0.283105, rmse-0.532076, ci--1, r2-0.640615, pearson-0.804327, spearman-0.666135
Traing Log at fold-4 epoch-17: mse-0.232307, rmse-0.481983, r2-0.591789
Valid at fold-4: mse-0.28235
Update best_mse, Valid at fold-4 epoch-17: mse-0.28235, rmse-0.531366, ci--1, r2-0.641573, pearson-0.802339, spearman-0.646623
Traing Log at fold-4 epoch-18: mse-0.223452, rmse-0.472707, r2-0.61276
Valid at fold-4: mse-0.279578
Update best_mse, Valid at fold-4 epoch-18: mse-0.279578, rmse-0.528752, ci--1, r2-0.645092, pearson-0.803729, spearman-0.645155
Traing Log at fold-4 epoch-19: mse-0.220756, rmse-0.469847, r2-0.619834
Valid at fold-4: mse-0.266886
Update best_mse, Valid at fold-4 epoch-19: mse-0.266886, rmse-0.51661, ci--1, r2-0.661204, pearson-0.814133, spearman-0.663452
Traing Log at fold-4 epoch-20: mse-0.210056, rmse-0.458318, r2-0.645393
Valid at fold-4: mse-0.248097
Update best_mse, Valid at fold-4 epoch-20: mse-0.248097, rmse-0.498094, ci--1, r2-0.685055, pearson-0.827867, spearman-0.67234
Traing Log at fold-4 epoch-21: mse-0.208755, rmse-0.456898, r2-0.647778
Valid at fold-4: mse-0.255015
Traing Log at fold-4 epoch-22: mse-0.1983, rmse-0.445309, r2-0.669634
Valid at fold-4: mse-0.268097
Traing Log at fold-4 epoch-23: mse-0.194675, rmse-0.441219, r2-0.679333
Valid at fold-4: mse-0.255304
Traing Log at fold-4 epoch-24: mse-0.191815, rmse-0.437967, r2-0.685536
Valid at fold-4: mse-0.26729
Traing Log at fold-4 epoch-25: mse-0.182638, rmse-0.427362, r2-0.704165
Valid at fold-4: mse-0.253608
Traing Log at fold-4 epoch-26: mse-0.180128, rmse-0.424415, r2-0.709219
Valid at fold-4: mse-0.245059
Update best_mse, Valid at fold-4 epoch-26: mse-0.245059, rmse-0.495034, ci--1, r2-0.688912, pearson-0.830792, spearman-0.682282
Traing Log at fold-4 epoch-27: mse-0.173789, rmse-0.41688, r2-0.72221
Valid at fold-4: mse-0.254765
Traing Log at fold-4 epoch-28: mse-0.173884, rmse-0.416993, r2-0.721147
Valid at fold-4: mse-0.262692
Traing Log at fold-4 epoch-29: mse-0.166998, rmse-0.408654, r2-0.736717
Valid at fold-4: mse-0.240804
Update best_mse, Valid at fold-4 epoch-29: mse-0.240804, rmse-0.490718, ci--1, r2-0.694313, pearson-0.837154, spearman-0.678935
Traing Log at fold-4 epoch-30: mse-0.1635, rmse-0.404351, r2-0.743309
Valid at fold-4: mse-0.24683
Traing Log at fold-4 epoch-31: mse-0.158741, rmse-0.398423, r2-0.751662
Valid at fold-4: mse-0.247189
Traing Log at fold-4 epoch-32: mse-0.155835, rmse-0.39476, r2-0.757454
Valid at fold-4: mse-0.247883
Traing Log at fold-4 epoch-33: mse-0.156575, rmse-0.395695, r2-0.758442
Valid at fold-4: mse-0.256221
Traing Log at fold-4 epoch-34: mse-0.153687, rmse-0.392029, r2-0.761951
Valid at fold-4: mse-0.247382
Traing Log at fold-4 epoch-35: mse-0.144461, rmse-0.380081, r2-0.779424
Valid at fold-4: mse-0.235969
Update best_mse, Valid at fold-4 epoch-35: mse-0.235969, rmse-0.485766, ci--1, r2-0.700451, pearson-0.837879, spearman-0.6721
Traing Log at fold-4 epoch-36: mse-0.14621, rmse-0.382374, r2-0.775812
Valid at fold-4: mse-0.239836
Traing Log at fold-4 epoch-37: mse-0.141578, rmse-0.376269, r2-0.783912
Valid at fold-4: mse-0.235871
Update best_mse, Valid at fold-4 epoch-37: mse-0.235871, rmse-0.485665, ci--1, r2-0.700576, pearson-0.84069, spearman-0.683925
Traing Log at fold-4 epoch-38: mse-0.142497, rmse-0.377487, r2-0.783911
Valid at fold-4: mse-0.236164
Traing Log at fold-4 epoch-39: mse-0.139498, rmse-0.373494, r2-0.788467
Valid at fold-4: mse-0.237956
Traing Log at fold-4 epoch-40: mse-0.134206, rmse-0.366342, r2-0.799045
Valid at fold-4: mse-0.251424
Traing Log at fold-4 epoch-41: mse-0.130362, rmse-0.361057, r2-0.805802
Valid at fold-4: mse-0.25369
Traing Log at fold-4 epoch-42: mse-0.131003, rmse-0.361943, r2-0.803738
Valid at fold-4: mse-0.240116
Traing Log at fold-4 epoch-43: mse-0.127849, rmse-0.35756, r2-0.809436
Valid at fold-4: mse-0.238636
Traing Log at fold-4 epoch-44: mse-0.123583, rmse-0.351544, r2-0.817578
Valid at fold-4: mse-0.231745
Update best_mse, Valid at fold-4 epoch-44: mse-0.231745, rmse-0.481399, ci--1, r2-0.705813, pearson-0.841744, spearman-0.676608
Traing Log at fold-4 epoch-45: mse-0.124315, rmse-0.352583, r2-0.816105
Valid at fold-4: mse-0.242592
Traing Log at fold-4 epoch-46: mse-0.121845, rmse-0.349064, r2-0.820037
Valid at fold-4: mse-0.239753
Traing Log at fold-4 epoch-47: mse-0.12081, rmse-0.347578, r2-0.821946
Valid at fold-4: mse-0.241589
Traing Log at fold-4 epoch-48: mse-0.119287, rmse-0.345379, r2-0.825128
Valid at fold-4: mse-0.224016
Update best_mse, Valid at fold-4 epoch-48: mse-0.224016, rmse-0.473303, ci--1, r2-0.715625, pearson-0.846459, spearman-0.687642
Traing Log at fold-4 epoch-49: mse-0.115938, rmse-0.340497, r2-0.830122
Valid at fold-4: mse-0.232763
Traing Log at fold-4 epoch-50: mse-0.117274, rmse-0.342453, r2-0.828123
Valid at fold-4: mse-0.232765
Traing Log at fold-4 epoch-51: mse-0.112601, rmse-0.33556, r2-0.836607
Valid at fold-4: mse-0.232984
Traing Log at fold-4 epoch-52: mse-0.112273, rmse-0.335071, r2-0.83696
Valid at fold-4: mse-0.232013
Traing Log at fold-4 epoch-53: mse-0.112858, rmse-0.335944, r2-0.835424
Valid at fold-4: mse-0.2306
Traing Log at fold-4 epoch-54: mse-0.108449, rmse-0.329317, r2-0.843428
Valid at fold-4: mse-0.239065
Traing Log at fold-4 epoch-55: mse-0.109339, rmse-0.330664, r2-0.841082
Valid at fold-4: mse-0.227838
Traing Log at fold-4 epoch-56: mse-0.109452, rmse-0.330835, r2-0.841656
Valid at fold-4: mse-0.231992
Traing Log at fold-4 epoch-57: mse-0.104912, rmse-0.323901, r2-0.848806
Valid at fold-4: mse-0.228752
Traing Log at fold-4 epoch-58: mse-0.106083, rmse-0.325704, r2-0.847285
Valid at fold-4: mse-0.237281
Traing Log at fold-4 epoch-59: mse-0.103693, rmse-0.322013, r2-0.850947
Valid at fold-4: mse-0.231318
Traing Log at fold-4 epoch-60: mse-0.102427, rmse-0.320042, r2-0.852899
Valid at fold-4: mse-0.239483
Traing Log at fold-4 epoch-61: mse-0.100521, rmse-0.31705, r2-0.855848
Valid at fold-4: mse-0.226725
Traing Log at fold-4 epoch-62: mse-0.098689, rmse-0.314148, r2-0.859617
Valid at fold-4: mse-0.243712
Traing Log at fold-4 epoch-63: mse-0.097737, rmse-0.31263, r2-0.860607
Valid at fold-4: mse-0.227848
Traing Log at fold-4 epoch-64: mse-0.099206, rmse-0.31497, r2-0.85843
Valid at fold-4: mse-0.224071
Traing Log at fold-4 epoch-65: mse-0.097516, rmse-0.312276, r2-0.861249
Valid at fold-4: mse-0.219826
Update best_mse, Valid at fold-4 epoch-65: mse-0.219826, rmse-0.468857, ci--1, r2-0.720943, pearson-0.850499, spearman-0.685007
Traing Log at fold-4 epoch-66: mse-0.094873, rmse-0.308015, r2-0.865189
Valid at fold-4: mse-0.224856
Traing Log at fold-4 epoch-67: mse-0.095602, rmse-0.309196, r2-0.864494
Valid at fold-4: mse-0.225577
Traing Log at fold-4 epoch-68: mse-0.095036, rmse-0.30828, r2-0.86496
Valid at fold-4: mse-0.228419
Traing Log at fold-4 epoch-69: mse-0.092715, rmse-0.304492, r2-0.868976
Valid at fold-4: mse-0.22784
Traing Log at fold-4 epoch-70: mse-0.0914, rmse-0.302324, r2-0.870396
Valid at fold-4: mse-0.228496
Traing Log at fold-4 epoch-71: mse-0.091056, rmse-0.301754, r2-0.871542
Valid at fold-4: mse-0.224497
Traing Log at fold-4 epoch-72: mse-0.089312, rmse-0.298851, r2-0.874384
Valid at fold-4: mse-0.239527
Traing Log at fold-4 epoch-73: mse-0.091045, rmse-0.301737, r2-0.871349
Valid at fold-4: mse-0.226629
Traing Log at fold-4 epoch-74: mse-0.088596, rmse-0.297651, r2-0.875419
Valid at fold-4: mse-0.220465
Traing Log at fold-4 epoch-75: mse-0.086936, rmse-0.294849, r2-0.878159
Valid at fold-4: mse-0.225492
Traing Log at fold-4 epoch-76: mse-0.086872, rmse-0.29474, r2-0.878195
Valid at fold-4: mse-0.224232
Traing Log at fold-4 epoch-77: mse-0.086044, rmse-0.293333, r2-0.879115
Valid at fold-4: mse-0.229145
Traing Log at fold-4 epoch-78: mse-0.083029, rmse-0.288147, r2-0.883815
Valid at fold-4: mse-0.225358
Traing Log at fold-4 epoch-79: mse-0.085888, rmse-0.293066, r2-0.879468
Valid at fold-4: mse-0.226246
Traing Log at fold-4 epoch-80: mse-0.083735, rmse-0.289369, r2-0.88325
Valid at fold-4: mse-0.22779
Traing Log at fold-4 epoch-81: mse-0.082465, rmse-0.287167, r2-0.88514
Valid at fold-4: mse-0.223143
Traing Log at fold-4 epoch-82: mse-0.082481, rmse-0.287195, r2-0.884858
Valid at fold-4: mse-0.233383
Traing Log at fold-4 epoch-83: mse-0.08332, rmse-0.288652, r2-0.883684
Valid at fold-4: mse-0.225027
Traing Log at fold-4 epoch-84: mse-0.081563, rmse-0.285593, r2-0.886369
Valid at fold-4: mse-0.220658
Traing Log at fold-4 epoch-85: mse-0.079807, rmse-0.282501, r2-0.889061
Valid at fold-4: mse-0.222372
Traing Log at fold-4 epoch-86: mse-0.080137, rmse-0.283084, r2-0.888637
Valid at fold-4: mse-0.231475
Traing stop at epoch-86, model save at-./savemodel/davis-warm-fold4-Nov12_16-41-45.pth
Save log over at ./log/Nov12_16-41-45-davis-warm-fold4.csv

============================================================
Testing fold 4 with best model...
============================================================
/home/minhtrin/.conda/envs/LLMDTA/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Test at fold-4, mse: 0.245421, rmse: 0.4954, ci: 0.883235, r2: 0.704211, pearson: 0.842094, spearman: 0.687954

Fold 4 results saved to: ./log/Test-davis-warm-fold4-Nov12_16-41-45.csv
============================================================
Training fold 4 completed successfully!
============================================================
wandb: updating run metadata
wandb: uploading wandb-summary.json; uploading output.log
wandb: uploading history steps 193-193, summary, console lines 209-214
wandb: 
wandb: Run history:
wandb:      best_valid/mse â–ˆâ–†â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:  best_valid/pearson â–â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:       best_valid/r2 â–â–ƒâ–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:     best_valid/rmse â–ˆâ–†â–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–
wandb: best_valid/spearman â–â–…â–…â–…â–…â–†â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               epoch â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb:             test/ci â–
wandb:            test/mse â–
wandb:        test/pearson â–
wandb:             test/r2 â–
wandb:                 +13 ...
wandb: 
wandb: Run summary:
wandb:      best_valid/mse 0.21983
wandb:  best_valid/pearson 0.8505
wandb:       best_valid/r2 0.72094
wandb:     best_valid/rmse 0.46886
wandb: best_valid/spearman 0.68501
wandb:               epoch 86
wandb:       final_test_ci 0.88323
wandb:      final_test_mse 0.24542
wandb:  final_test_pearson 0.84209
wandb:       final_test_r2 0.70421
wandb:                 +19 ...
wandb: 
wandb: ğŸš€ View run davis-warm-fold4 at: https://wandb.ai/tringuyen/LLMDTA/runs/n1vw7dze
wandb: â­ï¸ View project at: https://wandb.ai/tringuyen/LLMDTA
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251112_164146-n1vw7dze/logs
Weights & Biases run finished

Training for fold 4 completed successfully.
Python script exit code: 0
==========================================
End Time: Wed Nov 12 08:35:25 PM AEDT 2025
==========================================
