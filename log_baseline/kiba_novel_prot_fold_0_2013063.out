==========================================
Job ID: 2013063
Array Task ID: 0
Node: v100-f-04
Start Time: Thu Nov 13 07:13:42 AM AEDT 2025
==========================================
Activating conda environment...
Conda environment activated: LLMDTA
Checking GPU...
Thu Nov 13 07:13:43 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           On  |   00000000:18:00.0 Off |                    0 |
| N/A   33C    P0             41W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-SXM2-32GB           On  |   00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             41W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-SXM2-32GB           On  |   00000000:86:00.0 Off |                    0 |
| N/A   29C    P0             42W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-SXM2-32GB           On  |   00000000:AF:00.0 Off |                    0 |
| N/A   32C    P0             44W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Setting environment variables...

Starting training for fold 0...


============================================================
Starting training for Fold 0
Dataset: kiba, Running Set: novel-prot
Epochs: 200, Batch Size: 16
============================================================

Executing: python -u code/train.py --fold 0 --cuda 0 --dataset kiba --running_set novel-prot --epochs 200 --batch_size 16 --wandb_project LLMDTA
============================================================
Training Fold 0/4
Dataset: kiba-novel-prot
Device: cuda (CUDA_VISIBLE_DEVICES=0)
Pretrain-./data/kiba/kiba_drug_pretrain.pkl
Pretrain-./data/kiba/kiba_esm_pretrain.pkl
============================================================
wandb: Currently logged in as: tringuyen to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: setting up run f0w6m9he
wandb: Tracking run with wandb version 0.23.0
wandb: Run data is saved locally in /vast/minhtrin/DTA/Temp/wandb/run-20251113_071350-f0w6m9he
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run kiba-novel-prot-fold0
wandb: â­ï¸ View project at https://wandb.ai/tringuyen/LLMDTA
wandb: ğŸš€ View run at https://wandb.ai/tringuyen/LLMDTA/runs/f0w6m9he
Weights & Biases initialized: LLMDTA
Loading fold 0 data...
  Train: ./data/dta-5fold-dataset/kiba/novel-prot/fold_0_train.csv
  Valid: ./data/dta-5fold-dataset/kiba/novel-prot/fold_0_valid.csv
  Test:  ./data/dta-5fold-dataset/kiba/novel-prot/fold_0_test.csv
Dataset loaded: 74504 train, 18626 valid, 25124 test samples
/home/minhtrin/.conda/envs/LLMDTA/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Traing Log at fold-0 epoch-1: mse-2.392473, rmse-1.546762, r2--0.165912
Valid at fold-0: mse-0.48195
Update best_mse, Valid at fold-0 epoch-1: mse-0.48195, rmse-0.694226, ci--1, r2-0.305139, pearson-0.56039, spearman-0.57738
Traing Log at fold-0 epoch-2: mse-0.497889, rmse-0.705613, r2--0.4041
Valid at fold-0: mse-0.478566
Update best_mse, Valid at fold-0 epoch-2: mse-0.478566, rmse-0.691785, ci--1, r2-0.310019, pearson-0.61112, spearman-0.591077
Traing Log at fold-0 epoch-3: mse-0.406806, rmse-0.637813, r2--0.228859
Valid at fold-0: mse-0.347725
Update best_mse, Valid at fold-0 epoch-3: mse-0.347725, rmse-0.589682, ci--1, r2-0.498661, pearson-0.71504, spearman-0.710036
Traing Log at fold-0 epoch-4: mse-0.35829, rmse-0.598573, r2-0.015497
Valid at fold-0: mse-0.324317
Update best_mse, Valid at fold-0 epoch-4: mse-0.324317, rmse-0.569488, ci--1, r2-0.532411, pearson-0.746356, spearman-0.740191
Traing Log at fold-0 epoch-5: mse-0.336283, rmse-0.579899, r2-0.134202
Valid at fold-0: mse-0.315755
Update best_mse, Valid at fold-0 epoch-5: mse-0.315755, rmse-0.561921, ci--1, r2-0.544755, pearson-0.759887, spearman-0.752897
Traing Log at fold-0 epoch-6: mse-0.307181, rmse-0.554239, r2-0.244863
Valid at fold-0: mse-0.295944
Update best_mse, Valid at fold-0 epoch-6: mse-0.295944, rmse-0.544007, ci--1, r2-0.573318, pearson-0.761559, spearman-0.747175
Traing Log at fold-0 epoch-7: mse-0.299656, rmse-0.547408, r2-0.28603
Valid at fold-0: mse-0.265534
Update best_mse, Valid at fold-0 epoch-7: mse-0.265534, rmse-0.5153, ci--1, r2-0.617162, pearson-0.785614, spearman-0.763542
Traing Log at fold-0 epoch-8: mse-0.275141, rmse-0.524539, r2-0.369803
Valid at fold-0: mse-0.265831
Traing Log at fold-0 epoch-9: mse-0.268, rmse-0.517687, r2-0.397855
Valid at fold-0: mse-0.248898
Update best_mse, Valid at fold-0 epoch-9: mse-0.248898, rmse-0.498897, ci--1, r2-0.641147, pearson-0.802012, spearman-0.78029
Traing Log at fold-0 epoch-10: mse-0.258433, rmse-0.508363, r2-0.429477
Valid at fold-0: mse-0.242376
Update best_mse, Valid at fold-0 epoch-10: mse-0.242376, rmse-0.492317, ci--1, r2-0.65055, pearson-0.807112, spearman-0.794318
Traing Log at fold-0 epoch-11: mse-0.2482, rmse-0.498196, r2-0.464651
Valid at fold-0: mse-0.237988
Update best_mse, Valid at fold-0 epoch-11: mse-0.237988, rmse-0.48784, ci--1, r2-0.656877, pearson-0.811399, spearman-0.790288
Traing Log at fold-0 epoch-12: mse-0.236917, rmse-0.486741, r2-0.499282
Valid at fold-0: mse-0.24357
Traing Log at fold-0 epoch-13: mse-0.226862, rmse-0.4763, r2-0.530682
Valid at fold-0: mse-0.225395
Update best_mse, Valid at fold-0 epoch-13: mse-0.225395, rmse-0.474757, ci--1, r2-0.675033, pearson-0.82199, spearman-0.802651
Traing Log at fold-0 epoch-14: mse-0.220066, rmse-0.469112, r2-0.551929
Valid at fold-0: mse-0.215411
Update best_mse, Valid at fold-0 epoch-14: mse-0.215411, rmse-0.464124, ci--1, r2-0.689427, pearson-0.831009, spearman-0.809096
Traing Log at fold-0 epoch-15: mse-0.213315, rmse-0.46186, r2-0.570258
Valid at fold-0: mse-0.219775
Traing Log at fold-0 epoch-16: mse-0.206102, rmse-0.453985, r2-0.591877
Valid at fold-0: mse-0.219638
Traing Log at fold-0 epoch-17: mse-0.201004, rmse-0.448335, r2-0.606288
Valid at fold-0: mse-0.215824
Traing Log at fold-0 epoch-18: mse-0.19513, rmse-0.441735, r2-0.62122
Valid at fold-0: mse-0.211593
Update best_mse, Valid at fold-0 epoch-18: mse-0.211593, rmse-0.459992, ci--1, r2-0.694932, pearson-0.83514, spearman-0.819796
Traing Log at fold-0 epoch-19: mse-0.191282, rmse-0.437358, r2-0.631731
Valid at fold-0: mse-0.201268
Update best_mse, Valid at fold-0 epoch-19: mse-0.201268, rmse-0.448629, ci--1, r2-0.709819, pearson-0.845495, spearman-0.825557
Traing Log at fold-0 epoch-20: mse-0.184229, rmse-0.429219, r2-0.650013
Valid at fold-0: mse-0.202657
Traing Log at fold-0 epoch-21: mse-0.179049, rmse-0.423142, r2-0.662983
Valid at fold-0: mse-0.203066
Traing Log at fold-0 epoch-22: mse-0.175738, rmse-0.419211, r2-0.670172
Valid at fold-0: mse-0.206
Traing Log at fold-0 epoch-23: mse-0.171628, rmse-0.41428, r2-0.682166
Valid at fold-0: mse-0.199105
Update best_mse, Valid at fold-0 epoch-23: mse-0.199105, rmse-0.446212, ci--1, r2-0.712937, pearson-0.847005, spearman-0.818624
Traing Log at fold-0 epoch-24: mse-0.167724, rmse-0.409541, r2-0.69121
Valid at fold-0: mse-0.200989
Traing Log at fold-0 epoch-25: mse-0.16386, rmse-0.404796, r2-0.700022
Valid at fold-0: mse-0.195225
Update best_mse, Valid at fold-0 epoch-25: mse-0.195225, rmse-0.441843, ci--1, r2-0.718531, pearson-0.851704, spearman-0.82803
Traing Log at fold-0 epoch-26: mse-0.160382, rmse-0.400478, r2-0.708474
Valid at fold-0: mse-0.192046
Update best_mse, Valid at fold-0 epoch-26: mse-0.192046, rmse-0.43823, ci--1, r2-0.723115, pearson-0.853552, spearman-0.831311
Traing Log at fold-0 epoch-27: mse-0.156587, rmse-0.395711, r2-0.717394
Valid at fold-0: mse-0.189938
Update best_mse, Valid at fold-0 epoch-27: mse-0.189938, rmse-0.435819, ci--1, r2-0.726153, pearson-0.857449, spearman-0.832213
Traing Log at fold-0 epoch-28: mse-0.153035, rmse-0.391196, r2-0.725741
Valid at fold-0: mse-0.182255
Update best_mse, Valid at fold-0 epoch-28: mse-0.182255, rmse-0.426914, ci--1, r2-0.73723, pearson-0.859817, spearman-0.833202
Traing Log at fold-0 epoch-29: mse-0.149081, rmse-0.38611, r2-0.734439
Valid at fold-0: mse-0.188161
Traing Log at fold-0 epoch-30: mse-0.146888, rmse-0.383259, r2-0.7392
Valid at fold-0: mse-0.185001
Traing Log at fold-0 epoch-31: mse-0.14331, rmse-0.378563, r2-0.747104
Valid at fold-0: mse-0.186997
Traing Log at fold-0 epoch-32: mse-0.140305, rmse-0.374573, r2-0.754363
Valid at fold-0: mse-0.176578
Update best_mse, Valid at fold-0 epoch-32: mse-0.176578, rmse-0.420212, ci--1, r2-0.745416, pearson-0.867334, spearman-0.845733
Traing Log at fold-0 epoch-33: mse-0.138354, rmse-0.37196, r2-0.758006
Valid at fold-0: mse-0.172259
Update best_mse, Valid at fold-0 epoch-33: mse-0.172259, rmse-0.415041, ci--1, r2-0.751643, pearson-0.868416, spearman-0.845608
Traing Log at fold-0 epoch-34: mse-0.136, rmse-0.368782, r2-0.763382
Valid at fold-0: mse-0.180747
Traing Log at fold-0 epoch-35: mse-0.132961, rmse-0.364639, r2-0.769692
Valid at fold-0: mse-0.179688
Traing Log at fold-0 epoch-36: mse-0.130142, rmse-0.360752, r2-0.775579
Valid at fold-0: mse-0.181516
Traing Log at fold-0 epoch-37: mse-0.127986, rmse-0.357751, r2-0.780027
Valid at fold-0: mse-0.177123
Traing Log at fold-0 epoch-38: mse-0.125229, rmse-0.353877, r2-0.785716
Valid at fold-0: mse-0.183545
Traing Log at fold-0 epoch-39: mse-0.123608, rmse-0.351579, r2-0.789426
Valid at fold-0: mse-0.176832
Traing Log at fold-0 epoch-40: mse-0.120993, rmse-0.347841, r2-0.794698
Valid at fold-0: mse-0.178158
Traing Log at fold-0 epoch-41: mse-0.118319, rmse-0.343975, r2-0.799749
Valid at fold-0: mse-0.182023
Traing Log at fold-0 epoch-42: mse-0.117597, rmse-0.342924, r2-0.801646
Valid at fold-0: mse-0.18238
Traing Log at fold-0 epoch-43: mse-0.114347, rmse-0.338152, r2-0.807952
Valid at fold-0: mse-0.181977
Traing Log at fold-0 epoch-44: mse-0.113002, rmse-0.336158, r2-0.811335
Valid at fold-0: mse-0.167481
Update best_mse, Valid at fold-0 epoch-44: mse-0.167481, rmse-0.409244, ci--1, r2-0.758531, pearson-0.874733, spearman-0.84957
Traing Log at fold-0 epoch-45: mse-0.111287, rmse-0.333596, r2-0.814311
Valid at fold-0: mse-0.175587
Traing Log at fold-0 epoch-46: mse-0.10934, rmse-0.330666, r2-0.817965
Valid at fold-0: mse-0.175392
Traing Log at fold-0 epoch-47: mse-0.108149, rmse-0.32886, r2-0.820108
Valid at fold-0: mse-0.170609
Traing Log at fold-0 epoch-48: mse-0.104815, rmse-0.323751, r2-0.826927
Valid at fold-0: mse-0.171968
Traing Log at fold-0 epoch-49: mse-0.104591, rmse-0.323406, r2-0.827527
Valid at fold-0: mse-0.164697
Update best_mse, Valid at fold-0 epoch-49: mse-0.164697, rmse-0.405829, ci--1, r2-0.762545, pearson-0.875449, spearman-0.851905
Traing Log at fold-0 epoch-50: mse-0.102566, rmse-0.320259, r2-0.831447
Valid at fold-0: mse-0.164258
Update best_mse, Valid at fold-0 epoch-50: mse-0.164258, rmse-0.405287, ci--1, r2-0.763179, pearson-0.875454, spearman-0.851382
Traing Log at fold-0 epoch-51: mse-0.10208, rmse-0.3195, r2-0.831939
Valid at fold-0: mse-0.167958
Traing Log at fold-0 epoch-52: mse-0.09934, rmse-0.315183, r2-0.837757
Valid at fold-0: mse-0.173948
Traing Log at fold-0 epoch-53: mse-0.097483, rmse-0.312223, r2-0.840754
Valid at fold-0: mse-0.163365
Update best_mse, Valid at fold-0 epoch-53: mse-0.163365, rmse-0.404185, ci--1, r2-0.764465, pearson-0.879475, spearman-0.856469
Traing Log at fold-0 epoch-54: mse-0.096805, rmse-0.311135, r2-0.842
Valid at fold-0: mse-0.162732
Update best_mse, Valid at fold-0 epoch-54: mse-0.162732, rmse-0.4034, ci--1, r2-0.765378, pearson-0.87645, spearman-0.857051
Traing Log at fold-0 epoch-55: mse-0.095762, rmse-0.309455, r2-0.844426
Valid at fold-0: mse-0.163187
Traing Log at fold-0 epoch-56: mse-0.092966, rmse-0.304903, r2-0.849292
Valid at fold-0: mse-0.167769
Traing Log at fold-0 epoch-57: mse-0.092503, rmse-0.304143, r2-0.850354
Valid at fold-0: mse-0.163851
Traing Log at fold-0 epoch-58: mse-0.091099, rmse-0.301826, r2-0.852867
Valid at fold-0: mse-0.161944
Update best_mse, Valid at fold-0 epoch-58: mse-0.161944, rmse-0.402423, ci--1, r2-0.766514, pearson-0.878839, spearman-0.857298
Traing Log at fold-0 epoch-59: mse-0.089492, rmse-0.299151, r2-0.855841
Valid at fold-0: mse-0.16295
Traing Log at fold-0 epoch-60: mse-0.08818, rmse-0.296951, r2-0.858302
Valid at fold-0: mse-0.172183
Traing Log at fold-0 epoch-61: mse-0.087078, rmse-0.29509, r2-0.860087
Valid at fold-0: mse-0.170037
Traing Log at fold-0 epoch-62: mse-0.08576, rmse-0.292849, r2-0.862489
Valid at fold-0: mse-0.168547
Traing Log at fold-0 epoch-63: mse-0.084998, rmse-0.291544, r2-0.864258
Valid at fold-0: mse-0.161707
Update best_mse, Valid at fold-0 epoch-63: mse-0.161707, rmse-0.402129, ci--1, r2-0.766855, pearson-0.880048, spearman-0.856091
Traing Log at fold-0 epoch-64: mse-0.083382, rmse-0.288759, r2-0.866886
Valid at fold-0: mse-0.161443
Update best_mse, Valid at fold-0 epoch-64: mse-0.161443, rmse-0.4018, ci--1, r2-0.767237, pearson-0.877897, spearman-0.85689
Traing Log at fold-0 epoch-65: mse-0.082598, rmse-0.287399, r2-0.868488
Valid at fold-0: mse-0.163621
Traing Log at fold-0 epoch-66: mse-0.08056, rmse-0.283831, r2-0.872107
Valid at fold-0: mse-0.158859
Update best_mse, Valid at fold-0 epoch-66: mse-0.158859, rmse-0.398571, ci--1, r2-0.770963, pearson-0.880252, spearman-0.857466
Traing Log at fold-0 epoch-67: mse-0.079491, rmse-0.281941, r2-0.873905
Valid at fold-0: mse-0.163509
Traing Log at fold-0 epoch-68: mse-0.07906, rmse-0.281177, r2-0.874635
Valid at fold-0: mse-0.159242
Traing Log at fold-0 epoch-69: mse-0.077448, rmse-0.278294, r2-0.877524
Valid at fold-0: mse-0.156938
Update best_mse, Valid at fold-0 epoch-69: mse-0.156938, rmse-0.396154, ci--1, r2-0.773731, pearson-0.884114, spearman-0.861891
Traing Log at fold-0 epoch-70: mse-0.076454, rmse-0.276503, r2-0.879458
Valid at fold-0: mse-0.159438
Traing Log at fold-0 epoch-71: mse-0.075749, rmse-0.275225, r2-0.880409
Valid at fold-0: mse-0.162913
Traing Log at fold-0 epoch-72: mse-0.073878, rmse-0.271806, r2-0.883939
Valid at fold-0: mse-0.158912
Traing Log at fold-0 epoch-73: mse-0.072924, rmse-0.270044, r2-0.885506
Valid at fold-0: mse-0.164531
Traing Log at fold-0 epoch-74: mse-0.072482, rmse-0.269225, r2-0.886388
Valid at fold-0: mse-0.161534
Traing Log at fold-0 epoch-75: mse-0.070734, rmse-0.26596, r2-0.889283
Valid at fold-0: mse-0.158757
Traing Log at fold-0 epoch-76: mse-0.070486, rmse-0.265492, r2-0.889792
Valid at fold-0: mse-0.159394
Traing Log at fold-0 epoch-77: mse-0.070207, rmse-0.264967, r2-0.890249
Valid at fold-0: mse-0.15249
Update best_mse, Valid at fold-0 epoch-77: mse-0.15249, rmse-0.3905, ci--1, r2-0.780144, pearson-0.884367, spearman-0.860841
Traing Log at fold-0 epoch-78: mse-0.068127, rmse-0.261012, r2-0.893751
Valid at fold-0: mse-0.15708
Traing Log at fold-0 epoch-79: mse-0.066915, rmse-0.25868, r2-0.895912
Valid at fold-0: mse-0.157783
Traing Log at fold-0 epoch-80: mse-0.066442, rmse-0.257763, r2-0.89681
Valid at fold-0: mse-0.160277
Traing Log at fold-0 epoch-81: mse-0.065811, rmse-0.256536, r2-0.897733
Valid at fold-0: mse-0.158473
Traing Log at fold-0 epoch-82: mse-0.064853, rmse-0.254662, r2-0.899454
Valid at fold-0: mse-0.15727
Traing Log at fold-0 epoch-83: mse-0.063952, rmse-0.252888, r2-0.90097
Valid at fold-0: mse-0.153021
Traing Log at fold-0 epoch-84: mse-0.062104, rmse-0.249207, r2-0.904152
Valid at fold-0: mse-0.153892
Traing Log at fold-0 epoch-85: mse-0.062623, rmse-0.250245, r2-0.903208
Valid at fold-0: mse-0.153277
Traing Log at fold-0 epoch-86: mse-0.060784, rmse-0.246544, r2-0.906243
Valid at fold-0: mse-0.157129
Traing Log at fold-0 epoch-87: mse-0.060696, rmse-0.246365, r2-0.906514
Valid at fold-0: mse-0.154648
Traing Log at fold-0 epoch-88: mse-0.059687, rmse-0.244309, r2-0.908134
Valid at fold-0: mse-0.157726
Traing Log at fold-0 epoch-89: mse-0.059061, rmse-0.243025, r2-0.909194
Valid at fold-0: mse-0.154384
Traing Log at fold-0 epoch-90: mse-0.058231, rmse-0.241312, r2-0.910622
Valid at fold-0: mse-0.157669
Traing Log at fold-0 epoch-91: mse-0.057833, rmse-0.240486, r2-0.911167
Valid at fold-0: mse-0.155892
Traing Log at fold-0 epoch-92: mse-0.05683, rmse-0.238389, r2-0.912872
Valid at fold-0: mse-0.153579
Traing Log at fold-0 epoch-93: mse-0.05649, rmse-0.237676, r2-0.913426
Valid at fold-0: mse-0.152655
Traing Log at fold-0 epoch-94: mse-0.055772, rmse-0.23616, r2-0.914804
Valid at fold-0: mse-0.155831
Traing Log at fold-0 epoch-95: mse-0.054915, rmse-0.234341, r2-0.916078
Valid at fold-0: mse-0.153979
Traing Log at fold-0 epoch-96: mse-0.054147, rmse-0.232694, r2-0.917421
Valid at fold-0: mse-0.150904
Update best_mse, Valid at fold-0 epoch-96: mse-0.150904, rmse-0.388463, ci--1, r2-0.782432, pearson-0.885497, spearman-0.867166
Traing Log at fold-0 epoch-97: mse-0.053269, rmse-0.2308, r2-0.918722
Valid at fold-0: mse-0.153793
Traing Log at fold-0 epoch-98: mse-0.05234, rmse-0.228779, r2-0.920349
Valid at fold-0: mse-0.151875
Traing Log at fold-0 epoch-99: mse-0.05227, rmse-0.228627, r2-0.920381
Valid at fold-0: mse-0.151425
Traing Log at fold-0 epoch-100: mse-0.050989, rmse-0.225808, r2-0.922539
Valid at fold-0: mse-0.151612
Traing Log at fold-0 epoch-101: mse-0.05047, rmse-0.224656, r2-0.9234
Valid at fold-0: mse-0.155733
Traing Log at fold-0 epoch-102: mse-0.050045, rmse-0.223708, r2-0.924068
Valid at fold-0: mse-0.151823
Traing Log at fold-0 epoch-103: mse-0.049492, rmse-0.222469, r2-0.924995
Valid at fold-0: mse-0.154929
Traing Log at fold-0 epoch-104: mse-0.048981, rmse-0.221316, r2-0.925889
Valid at fold-0: mse-0.150242
Update best_mse, Valid at fold-0 epoch-104: mse-0.150242, rmse-0.38761, ci--1, r2-0.783386, pearson-0.88982, spearman-0.867914
Traing Log at fold-0 epoch-105: mse-0.048566, rmse-0.220378, r2-0.926442
Valid at fold-0: mse-0.152093
Traing Log at fold-0 epoch-106: mse-0.047998, rmse-0.219085, r2-0.927418
Valid at fold-0: mse-0.152787
Traing Log at fold-0 epoch-107: mse-0.047336, rmse-0.217569, r2-0.928463
Valid at fold-0: mse-0.152334
Traing Log at fold-0 epoch-108: mse-0.047049, rmse-0.216907, r2-0.928985
Valid at fold-0: mse-0.151535
Traing Log at fold-0 epoch-109: mse-0.04633, rmse-0.215243, r2-0.930105
Valid at fold-0: mse-0.150363
Traing Log at fold-0 epoch-110: mse-0.045442, rmse-0.213171, r2-0.931517
Valid at fold-0: mse-0.149427
Update best_mse, Valid at fold-0 epoch-110: mse-0.149427, rmse-0.386558, ci--1, r2-0.784561, pearson-0.886688, spearman-0.86729
Traing Log at fold-0 epoch-111: mse-0.045293, rmse-0.212822, r2-0.931768
Valid at fold-0: mse-0.150713
Traing Log at fold-0 epoch-112: mse-0.044245, rmse-0.210344, r2-0.93347
Valid at fold-0: mse-0.149794
Traing Log at fold-0 epoch-113: mse-0.044031, rmse-0.209836, r2-0.933752
Valid at fold-0: mse-0.149191
Update best_mse, Valid at fold-0 epoch-113: mse-0.149191, rmse-0.386253, ci--1, r2-0.784901, pearson-0.887185, spearman-0.867674
Traing Log at fold-0 epoch-114: mse-0.044262, rmse-0.210386, r2-0.933421
Valid at fold-0: mse-0.153676
Traing Log at fold-0 epoch-115: mse-0.042889, rmse-0.207096, r2-0.935615
Valid at fold-0: mse-0.150079
Traing Log at fold-0 epoch-116: mse-0.04236, rmse-0.205816, r2-0.936489
Valid at fold-0: mse-0.147964
Update best_mse, Valid at fold-0 epoch-116: mse-0.147964, rmse-0.384661, ci--1, r2-0.78667, pearson-0.889241, spearman-0.868201
Traing Log at fold-0 epoch-117: mse-0.041701, rmse-0.204208, r2-0.937489
Valid at fold-0: mse-0.149845
Traing Log at fold-0 epoch-118: mse-0.041989, rmse-0.204912, r2-0.937024
Valid at fold-0: mse-0.148279
Traing Log at fold-0 epoch-119: mse-0.041005, rmse-0.202497, r2-0.938557
Valid at fold-0: mse-0.150642
Traing Log at fold-0 epoch-120: mse-0.040793, rmse-0.201973, r2-0.938942
Valid at fold-0: mse-0.150879
Traing Log at fold-0 epoch-121: mse-0.040305, rmse-0.200761, r2-0.939713
Valid at fold-0: mse-0.148818
Traing Log at fold-0 epoch-122: mse-0.039599, rmse-0.198994, r2-0.940812
Valid at fold-0: mse-0.150672
Traing Log at fold-0 epoch-123: mse-0.039639, rmse-0.199095, r2-0.940779
Valid at fold-0: mse-0.152496
Traing Log at fold-0 epoch-124: mse-0.0393, rmse-0.198242, r2-0.941301
Valid at fold-0: mse-0.150293
Traing Log at fold-0 epoch-125: mse-0.039192, rmse-0.197971, r2-0.941466
Valid at fold-0: mse-0.1473
Update best_mse, Valid at fold-0 epoch-125: mse-0.1473, rmse-0.383797, ci--1, r2-0.787627, pearson-0.889306, spearman-0.867489
Traing Log at fold-0 epoch-126: mse-0.038115, rmse-0.19523, r2-0.943096
Valid at fold-0: mse-0.149068
Traing Log at fold-0 epoch-127: mse-0.038373, rmse-0.195891, r2-0.942806
Valid at fold-0: mse-0.150232
Traing Log at fold-0 epoch-128: mse-0.037755, rmse-0.194306, r2-0.94378
Valid at fold-0: mse-0.146031
Update best_mse, Valid at fold-0 epoch-128: mse-0.146031, rmse-0.38214, ci--1, r2-0.789457, pearson-0.889361, spearman-0.869778
Traing Log at fold-0 epoch-129: mse-0.037082, rmse-0.192566, r2-0.944771
Valid at fold-0: mse-0.147163
Traing Log at fold-0 epoch-130: mse-0.036719, rmse-0.191622, r2-0.945336
Valid at fold-0: mse-0.150747
Traing Log at fold-0 epoch-131: mse-0.03684, rmse-0.191937, r2-0.945122
Valid at fold-0: mse-0.149287
Traing Log at fold-0 epoch-132: mse-0.03608, rmse-0.189947, r2-0.946389
Valid at fold-0: mse-0.148666
Traing Log at fold-0 epoch-133: mse-0.036199, rmse-0.190262, r2-0.946179
Valid at fold-0: mse-0.143675
Update best_mse, Valid at fold-0 epoch-133: mse-0.143675, rmse-0.379045, ci--1, r2-0.792853, pearson-0.891629, spearman-0.87352
Traing Log at fold-0 epoch-134: mse-0.035538, rmse-0.188515, r2-0.947187
Valid at fold-0: mse-0.149441
Traing Log at fold-0 epoch-135: mse-0.03461, rmse-0.186038, r2-0.948627
Valid at fold-0: mse-0.149198
Traing Log at fold-0 epoch-136: mse-0.034905, rmse-0.186828, r2-0.948188
Valid at fold-0: mse-0.14786
Traing Log at fold-0 epoch-137: mse-0.034446, rmse-0.185595, r2-0.948891
Valid at fold-0: mse-0.149269
Traing Log at fold-0 epoch-138: mse-0.033944, rmse-0.184239, r2-0.949705
Valid at fold-0: mse-0.148071
Traing Log at fold-0 epoch-139: mse-0.034066, rmse-0.18457, r2-0.949459
Valid at fold-0: mse-0.148369
Traing Log at fold-0 epoch-140: mse-0.033866, rmse-0.184028, r2-0.949798
Valid at fold-0: mse-0.149713
Traing Log at fold-0 epoch-141: mse-0.033254, rmse-0.182356, r2-0.950749
Valid at fold-0: mse-0.149359
Traing Log at fold-0 epoch-142: mse-0.033114, rmse-0.181973, r2-0.950961
Valid at fold-0: mse-0.147555
Traing Log at fold-0 epoch-143: mse-0.032462, rmse-0.180173, r2-0.952
Valid at fold-0: mse-0.148298
Traing Log at fold-0 epoch-144: mse-0.032102, rmse-0.17917, r2-0.952504
Valid at fold-0: mse-0.149667
Traing Log at fold-0 epoch-145: mse-0.032039, rmse-0.178995, r2-0.952633
Valid at fold-0: mse-0.147705
Traing Log at fold-0 epoch-146: mse-0.031528, rmse-0.177561, r2-0.953395
Valid at fold-0: mse-0.146104
Traing Log at fold-0 epoch-147: mse-0.0322, rmse-0.179442, r2-0.952418
Valid at fold-0: mse-0.14767
Traing Log at fold-0 epoch-148: mse-0.031592, rmse-0.177742, r2-0.953291
Valid at fold-0: mse-0.148294
Traing Log at fold-0 epoch-149: mse-0.031161, rmse-0.176525, r2-0.953983
Valid at fold-0: mse-0.147743
Traing Log at fold-0 epoch-150: mse-0.030956, rmse-0.175943, r2-0.954297
Valid at fold-0: mse-0.148884
Traing Log at fold-0 epoch-151: mse-0.030738, rmse-0.175323, r2-0.954641
Valid at fold-0: mse-0.14582
Traing Log at fold-0 epoch-152: mse-0.03013, rmse-0.173579, r2-0.955573
Valid at fold-0: mse-0.147109
Traing Log at fold-0 epoch-153: mse-0.030226, rmse-0.173858, r2-0.955421
Valid at fold-0: mse-0.148515
Traing Log at fold-0 epoch-154: mse-0.029598, rmse-0.172041, r2-0.956365
Valid at fold-0: mse-0.147953
Traing stop at epoch-154, model save at-./savemodel/kiba-novel-prot-fold0-Nov13_07-13-50.pth
Save log over at ./log/Nov13_07-13-50-kiba-novel-prot-fold0.csv

============================================================
Testing fold 0 with best model...
============================================================
/home/minhtrin/.conda/envs/LLMDTA/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Test at fold-0, mse: 0.29474, rmse: 0.5429, ci: 0.792889, r2: 0.574308, pearson: 0.768443, spearman: 0.729797

Fold 0 results saved to: ./log/Test-kiba-novel-prot-fold0-Nov13_07-13-50.csv
============================================================
Training fold 0 completed successfully!
============================================================
wandb: updating run metadata
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading history steps 348-348, summary, console lines 364-369
wandb: 
wandb: Run history:
wandb:      best_valid/mse â–ˆâ–ˆâ–…â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  best_valid/pearson â–â–‚â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:       best_valid/r2 â–â–â–„â–„â–„â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:     best_valid/rmse â–ˆâ–ˆâ–†â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb: best_valid/spearman â–â–â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:             test/ci â–
wandb:            test/mse â–
wandb:        test/pearson â–
wandb:             test/r2 â–
wandb:                 +13 ...
wandb: 
wandb: Run summary:
wandb:      best_valid/mse 0.14367
wandb:  best_valid/pearson 0.89163
wandb:       best_valid/r2 0.79285
wandb:     best_valid/rmse 0.37905
wandb: best_valid/spearman 0.87352
wandb:               epoch 154
wandb:       final_test_ci 0.79289
wandb:      final_test_mse 0.29474
wandb:  final_test_pearson 0.76844
wandb:       final_test_r2 0.57431
wandb:                 +19 ...
wandb: 
wandb: ğŸš€ View run kiba-novel-prot-fold0 at: https://wandb.ai/tringuyen/LLMDTA/runs/f0w6m9he
wandb: â­ï¸ View project at: https://wandb.ai/tringuyen/LLMDTA
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251113_071350-f0w6m9he/logs
Weights & Biases run finished

Training for fold 0 completed successfully.
Python script exit code: 0
==========================================
End Time: Fri Nov 14 10:59:36 AM AEDT 2025
==========================================
