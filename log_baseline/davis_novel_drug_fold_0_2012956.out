==========================================
Job ID: 2012956
Array Task ID: 0
Node: v100l-f-03
Start Time: Wed Nov 12 04:41:09 PM AEDT 2025
==========================================
Activating conda environment...
Conda environment activated: LLMDTA
Checking GPU...
Wed Nov 12 16:41:10 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           On  |   00000000:85:00.0 Off |                    0 |
| N/A   34C    P0             42W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Setting environment variables...

Starting training for fold 0...


============================================================
Starting training for Fold 0
Dataset: davis, Running Set: novel-drug
Epochs: 200, Batch Size: 16
============================================================

Executing: python -u code/train.py --fold 0 --cuda 0 --dataset davis --running_set novel-drug --epochs 200 --batch_size 16 --wandb_project LLMDTA
============================================================
Training Fold 0/4
Dataset: davis-novel-drug
Device: cuda (CUDA_VISIBLE_DEVICES=0)
Pretrain-./data/davis/davis_drug_pretrain.pkl
Pretrain-./data/davis/davis_esm_pretrain.pkl
============================================================
wandb: Currently logged in as: tringuyen to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: setting up run dq2g7qok
wandb: Tracking run with wandb version 0.23.0
wandb: Run data is saved locally in /vast/minhtrin/DTA/Temp/wandb/run-20251112_164115-dq2g7qok
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run davis-novel-drug-fold0
wandb: â­ï¸ View project at https://wandb.ai/tringuyen/LLMDTA
wandb: ğŸš€ View run at https://wandb.ai/tringuyen/LLMDTA/runs/dq2g7qok
Weights & Biases initialized: LLMDTA
Loading fold 0 data...
  Train: ./data/dta-5fold-dataset/davis/novel-drug/fold_0_train.csv
  Valid: ./data/dta-5fold-dataset/davis/novel-drug/fold_0_valid.csv
  Test:  ./data/dta-5fold-dataset/davis/novel-drug/fold_0_test.csv
Dataset loaded: 19448 train, 4862 valid, 5746 test samples
/home/minhtrin/.conda/envs/LLMDTA/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Traing Log at fold-0 epoch-1: mse-1.822175, rmse-1.349879, r2--0.207384
Valid at fold-0: mse-1.290418
Update best_mse, Valid at fold-0 epoch-1: mse-1.290418, rmse-1.135966, ci--1, r2--0.504781, pearson-0.448586, spearman-0.415714
Traing Log at fold-0 epoch-2: mse-0.743516, rmse-0.862274, r2--0.181399
Valid at fold-0: mse-1.649488
Traing Log at fold-0 epoch-3: mse-0.590933, rmse-0.768722, r2--0.073371
Valid at fold-0: mse-0.863525
Update best_mse, Valid at fold-0 epoch-3: mse-0.863525, rmse-0.929261, ci--1, r2--0.006973, pearson-0.611986, spearman-0.549705
Traing Log at fold-0 epoch-4: mse-0.503904, rmse-0.709862, r2-0.048327
Valid at fold-0: mse-0.648721
Update best_mse, Valid at fold-0 epoch-4: mse-0.648721, rmse-0.805432, ci--1, r2-0.243515, pearson-0.675982, spearman-0.57434
Traing Log at fold-0 epoch-5: mse-0.447834, rmse-0.669204, r2-0.135729
Valid at fold-0: mse-0.436672
Update best_mse, Valid at fold-0 epoch-5: mse-0.436672, rmse-0.660812, ci--1, r2-0.490788, pearson-0.709423, spearman-0.592735
Traing Log at fold-0 epoch-6: mse-0.398969, rmse-0.63164, r2-0.222943
Valid at fold-0: mse-0.39262
Update best_mse, Valid at fold-0 epoch-6: mse-0.39262, rmse-0.626594, ci--1, r2-0.542158, pearson-0.763066, spearman-0.662575
Traing Log at fold-0 epoch-7: mse-0.363541, rmse-0.602944, r2-0.296651
Valid at fold-0: mse-0.408439
Traing Log at fold-0 epoch-8: mse-0.346097, rmse-0.5883, r2-0.33447
Valid at fold-0: mse-0.357397
Update best_mse, Valid at fold-0 epoch-8: mse-0.357397, rmse-0.597827, ci--1, r2-0.583232, pearson-0.775586, spearman-0.668381
Traing Log at fold-0 epoch-9: mse-0.326307, rmse-0.571233, r2-0.38602
Valid at fold-0: mse-0.350498
Update best_mse, Valid at fold-0 epoch-9: mse-0.350498, rmse-0.592029, ci--1, r2-0.591277, pearson-0.782327, spearman-0.655763
Traing Log at fold-0 epoch-10: mse-0.30763, rmse-0.554644, r2-0.424619
Valid at fold-0: mse-0.310827
Update best_mse, Valid at fold-0 epoch-10: mse-0.310827, rmse-0.557518, ci--1, r2-0.637539, pearson-0.803071, spearman-0.700579
Traing Log at fold-0 epoch-11: mse-0.290846, rmse-0.539302, r2-0.472208
Valid at fold-0: mse-0.282889
Update best_mse, Valid at fold-0 epoch-11: mse-0.282889, rmse-0.531873, ci--1, r2-0.670118, pearson-0.822437, spearman-0.700563
Traing Log at fold-0 epoch-12: mse-0.277832, rmse-0.527098, r2-0.502949
Valid at fold-0: mse-0.300589
Traing Log at fold-0 epoch-13: mse-0.269569, rmse-0.519201, r2-0.526844
Valid at fold-0: mse-0.28051
Update best_mse, Valid at fold-0 epoch-13: mse-0.28051, rmse-0.529632, ci--1, r2-0.672893, pearson-0.823386, spearman-0.695044
Traing Log at fold-0 epoch-14: mse-0.256165, rmse-0.506128, r2-0.555695
Valid at fold-0: mse-0.281875
Traing Log at fold-0 epoch-15: mse-0.247544, rmse-0.497537, r2-0.575289
Valid at fold-0: mse-0.293411
Traing Log at fold-0 epoch-16: mse-0.239468, rmse-0.489355, r2-0.596862
Valid at fold-0: mse-0.293602
Traing Log at fold-0 epoch-17: mse-0.228924, rmse-0.47846, r2-0.619199
Valid at fold-0: mse-0.259366
Update best_mse, Valid at fold-0 epoch-17: mse-0.259366, rmse-0.50928, ci--1, r2-0.697549, pearson-0.835258, spearman-0.713807
Traing Log at fold-0 epoch-18: mse-0.22582, rmse-0.475205, r2-0.62584
Valid at fold-0: mse-0.269725
Traing Log at fold-0 epoch-19: mse-0.217189, rmse-0.466036, r2-0.644209
Valid at fold-0: mse-0.27163
Traing Log at fold-0 epoch-20: mse-0.209757, rmse-0.457992, r2-0.660227
Valid at fold-0: mse-0.249381
Update best_mse, Valid at fold-0 epoch-20: mse-0.249381, rmse-0.499381, ci--1, r2-0.709192, pearson-0.842745, spearman-0.705463
Traing Log at fold-0 epoch-21: mse-0.201249, rmse-0.448608, r2-0.678753
Valid at fold-0: mse-0.239845
Update best_mse, Valid at fold-0 epoch-21: mse-0.239845, rmse-0.48974, ci--1, r2-0.720312, pearson-0.850764, spearman-0.721794
Traing Log at fold-0 epoch-22: mse-0.196997, rmse-0.443843, r2-0.687593
Valid at fold-0: mse-0.252103
Traing Log at fold-0 epoch-23: mse-0.192429, rmse-0.438667, r2-0.696584
Valid at fold-0: mse-0.244869
Traing Log at fold-0 epoch-24: mse-0.191561, rmse-0.437677, r2-0.699168
Valid at fold-0: mse-0.243071
Traing Log at fold-0 epoch-25: mse-0.18451, rmse-0.429546, r2-0.71202
Valid at fold-0: mse-0.229449
Update best_mse, Valid at fold-0 epoch-25: mse-0.229449, rmse-0.479008, ci--1, r2-0.732436, pearson-0.855883, spearman-0.720721
Traing Log at fold-0 epoch-26: mse-0.174673, rmse-0.417939, r2-0.731799
Valid at fold-0: mse-0.249519
Traing Log at fold-0 epoch-27: mse-0.172565, rmse-0.415409, r2-0.737137
Valid at fold-0: mse-0.234036
Traing Log at fold-0 epoch-28: mse-0.171535, rmse-0.414168, r2-0.739353
Valid at fold-0: mse-0.236212
Traing Log at fold-0 epoch-29: mse-0.167423, rmse-0.409173, r2-0.746202
Valid at fold-0: mse-0.228528
Update best_mse, Valid at fold-0 epoch-29: mse-0.228528, rmse-0.478046, ci--1, r2-0.733509, pearson-0.858803, spearman-0.721315
Traing Log at fold-0 epoch-30: mse-0.161939, rmse-0.402417, r2-0.756308
Valid at fold-0: mse-0.244847
Traing Log at fold-0 epoch-31: mse-0.159248, rmse-0.399059, r2-0.76169
Valid at fold-0: mse-0.227924
Update best_mse, Valid at fold-0 epoch-31: mse-0.227924, rmse-0.477414, ci--1, r2-0.734213, pearson-0.857799, spearman-0.714936
Traing Log at fold-0 epoch-32: mse-0.158858, rmse-0.39857, r2-0.763482
Valid at fold-0: mse-0.231936
Traing Log at fold-0 epoch-33: mse-0.154827, rmse-0.39348, r2-0.770105
Valid at fold-0: mse-0.238312
Traing Log at fold-0 epoch-34: mse-0.150653, rmse-0.38814, r2-0.7761
Valid at fold-0: mse-0.232432
Traing Log at fold-0 epoch-35: mse-0.148374, rmse-0.385193, r2-0.781564
Valid at fold-0: mse-0.222097
Update best_mse, Valid at fold-0 epoch-35: mse-0.222097, rmse-0.471272, ci--1, r2-0.741009, pearson-0.862329, spearman-0.720044
Traing Log at fold-0 epoch-36: mse-0.14524, rmse-0.381104, r2-0.786642
Valid at fold-0: mse-0.225437
Traing Log at fold-0 epoch-37: mse-0.14114, rmse-0.375686, r2-0.793838
Valid at fold-0: mse-0.217246
Update best_mse, Valid at fold-0 epoch-37: mse-0.217246, rmse-0.466096, ci--1, r2-0.746666, pearson-0.865577, spearman-0.722564
Traing Log at fold-0 epoch-38: mse-0.140619, rmse-0.374992, r2-0.795846
Valid at fold-0: mse-0.225494
Traing Log at fold-0 epoch-39: mse-0.138893, rmse-0.372684, r2-0.796899
Valid at fold-0: mse-0.223175
Traing Log at fold-0 epoch-40: mse-0.136252, rmse-0.369124, r2-0.803096
Valid at fold-0: mse-0.230146
Traing Log at fold-0 epoch-41: mse-0.132587, rmse-0.364125, r2-0.809515
Valid at fold-0: mse-0.230266
Traing Log at fold-0 epoch-42: mse-0.132158, rmse-0.363535, r2-0.80867
Valid at fold-0: mse-0.237168
Traing Log at fold-0 epoch-43: mse-0.129968, rmse-0.360511, r2-0.814354
Valid at fold-0: mse-0.220394
Traing Log at fold-0 epoch-44: mse-0.126871, rmse-0.356189, r2-0.818539
Valid at fold-0: mse-0.236282
Traing Log at fold-0 epoch-45: mse-0.12435, rmse-0.352633, r2-0.82236
Valid at fold-0: mse-0.22859
Traing Log at fold-0 epoch-46: mse-0.126428, rmse-0.355567, r2-0.819795
Valid at fold-0: mse-0.220193
Traing Log at fold-0 epoch-47: mse-0.125535, rmse-0.35431, r2-0.820924
Valid at fold-0: mse-0.227988
Traing Log at fold-0 epoch-48: mse-0.118165, rmse-0.343751, r2-0.832607
Valid at fold-0: mse-0.214842
Update best_mse, Valid at fold-0 epoch-48: mse-0.214842, rmse-0.463511, ci--1, r2-0.749469, pearson-0.867219, spearman-0.7179
Traing Log at fold-0 epoch-49: mse-0.119999, rmse-0.346409, r2-0.830258
Valid at fold-0: mse-0.229045
Traing Log at fold-0 epoch-50: mse-0.119362, rmse-0.345488, r2-0.831062
Valid at fold-0: mse-0.220541
Traing Log at fold-0 epoch-51: mse-0.117418, rmse-0.342663, r2-0.835007
Valid at fold-0: mse-0.217493
Traing Log at fold-0 epoch-52: mse-0.11567, rmse-0.340103, r2-0.836878
Valid at fold-0: mse-0.236623
Traing Log at fold-0 epoch-53: mse-0.113307, rmse-0.336612, r2-0.841072
Valid at fold-0: mse-0.216137
Traing Log at fold-0 epoch-54: mse-0.112898, rmse-0.336003, r2-0.841915
Valid at fold-0: mse-0.213018
Update best_mse, Valid at fold-0 epoch-54: mse-0.213018, rmse-0.461539, ci--1, r2-0.751595, pearson-0.869141, spearman-0.715676
Traing Log at fold-0 epoch-55: mse-0.112689, rmse-0.335692, r2-0.842012
Valid at fold-0: mse-0.214653
Traing Log at fold-0 epoch-56: mse-0.109193, rmse-0.330443, r2-0.847865
Valid at fold-0: mse-0.229982
Traing Log at fold-0 epoch-57: mse-0.107734, rmse-0.328228, r2-0.849917
Valid at fold-0: mse-0.223574
Traing Log at fold-0 epoch-58: mse-0.105732, rmse-0.325165, r2-0.852928
Valid at fold-0: mse-0.223749
Traing Log at fold-0 epoch-59: mse-0.108244, rmse-0.329005, r2-0.849709
Valid at fold-0: mse-0.224464
Traing Log at fold-0 epoch-60: mse-0.104435, rmse-0.323164, r2-0.855006
Valid at fold-0: mse-0.208558
Update best_mse, Valid at fold-0 epoch-60: mse-0.208558, rmse-0.456682, ci--1, r2-0.756796, pearson-0.871098, spearman-0.725085
Traing Log at fold-0 epoch-61: mse-0.104795, rmse-0.323721, r2-0.854796
Valid at fold-0: mse-0.22467
Traing Log at fold-0 epoch-62: mse-0.104553, rmse-0.323347, r2-0.855066
Valid at fold-0: mse-0.216022
Traing Log at fold-0 epoch-63: mse-0.103344, rmse-0.321472, r2-0.857072
Valid at fold-0: mse-0.214306
Traing Log at fold-0 epoch-64: mse-0.100897, rmse-0.317643, r2-0.860856
Valid at fold-0: mse-0.205822
Update best_mse, Valid at fold-0 epoch-64: mse-0.205822, rmse-0.453676, ci--1, r2-0.759987, pearson-0.872638, spearman-0.728573
Traing Log at fold-0 epoch-65: mse-0.098419, rmse-0.313717, r2-0.865093
Valid at fold-0: mse-0.22341
Traing Log at fold-0 epoch-66: mse-0.098796, rmse-0.314318, r2-0.863736
Valid at fold-0: mse-0.216004
Traing Log at fold-0 epoch-67: mse-0.099879, rmse-0.316037, r2-0.862719
Valid at fold-0: mse-0.217133
Traing Log at fold-0 epoch-68: mse-0.097042, rmse-0.311516, r2-0.867222
Valid at fold-0: mse-0.208795
Traing Log at fold-0 epoch-69: mse-0.096645, rmse-0.310878, r2-0.867305
Valid at fold-0: mse-0.215676
Traing Log at fold-0 epoch-70: mse-0.093173, rmse-0.305243, r2-0.873089
Valid at fold-0: mse-0.223832
Traing Log at fold-0 epoch-71: mse-0.092597, rmse-0.304298, r2-0.87339
Valid at fold-0: mse-0.21229
Traing Log at fold-0 epoch-72: mse-0.092996, rmse-0.304953, r2-0.873307
Valid at fold-0: mse-0.210066
Traing Log at fold-0 epoch-73: mse-0.094263, rmse-0.307022, r2-0.871191
Valid at fold-0: mse-0.214964
Traing Log at fold-0 epoch-74: mse-0.091827, rmse-0.30303, r2-0.874844
Valid at fold-0: mse-0.222718
Traing Log at fold-0 epoch-75: mse-0.090684, rmse-0.301137, r2-0.876568
Valid at fold-0: mse-0.219167
Traing Log at fold-0 epoch-76: mse-0.091211, rmse-0.302011, r2-0.876193
Valid at fold-0: mse-0.219999
Traing Log at fold-0 epoch-77: mse-0.089558, rmse-0.299262, r2-0.878238
Valid at fold-0: mse-0.217194
Traing Log at fold-0 epoch-78: mse-0.089725, rmse-0.299541, r2-0.878448
Valid at fold-0: mse-0.214445
Traing Log at fold-0 epoch-79: mse-0.090427, rmse-0.300711, r2-0.877029
Valid at fold-0: mse-0.218773
Traing Log at fold-0 epoch-80: mse-0.085983, rmse-0.293228, r2-0.883577
Valid at fold-0: mse-0.216849
Traing Log at fold-0 epoch-81: mse-0.087383, rmse-0.295607, r2-0.881884
Valid at fold-0: mse-0.208527
Traing Log at fold-0 epoch-82: mse-0.087408, rmse-0.295649, r2-0.88197
Valid at fold-0: mse-0.211236
Traing Log at fold-0 epoch-83: mse-0.083861, rmse-0.289588, r2-0.886534
Valid at fold-0: mse-0.218864
Traing Log at fold-0 epoch-84: mse-0.083694, rmse-0.2893, r2-0.887094
Valid at fold-0: mse-0.209937
Traing Log at fold-0 epoch-85: mse-0.084942, rmse-0.291448, r2-0.885384
Valid at fold-0: mse-0.220295
Traing stop at epoch-85, model save at-./savemodel/davis-novel-drug-fold0-Nov12_16-41-15.pth
Save log over at ./log/Nov12_16-41-15-davis-novel-drug-fold0.csv

============================================================
Testing fold 0 with best model...
============================================================
/home/minhtrin/.conda/envs/LLMDTA/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Test at fold-0, mse: 0.631866, rmse: 0.7949, ci: 0.663934, r2: 0.063495, pearson: 0.444615, spearman: 0.295378

Fold 0 results saved to: ./log/Test-davis-novel-drug-fold0-Nov12_16-41-15.csv
============================================================
Training fold 0 completed successfully!
============================================================
wandb: updating run metadata
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading history steps 192-192, summary, console lines 208-213
wandb: 
wandb: Run history:
wandb:      best_valid/mse â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  best_valid/pearson â–â–„â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:       best_valid/r2 â–â–„â–…â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:     best_valid/rmse â–ˆâ–†â–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb: best_valid/spearman â–â–„â–…â–…â–‡â–‡â–†â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             test/ci â–
wandb:            test/mse â–
wandb:        test/pearson â–
wandb:             test/r2 â–
wandb:                 +13 ...
wandb: 
wandb: Run summary:
wandb:      best_valid/mse 0.20582
wandb:  best_valid/pearson 0.87264
wandb:       best_valid/r2 0.75999
wandb:     best_valid/rmse 0.45368
wandb: best_valid/spearman 0.72857
wandb:               epoch 85
wandb:       final_test_ci 0.66393
wandb:      final_test_mse 0.63187
wandb:  final_test_pearson 0.44461
wandb:       final_test_r2 0.06349
wandb:                 +19 ...
wandb: 
wandb: ğŸš€ View run davis-novel-drug-fold0 at: https://wandb.ai/tringuyen/LLMDTA/runs/dq2g7qok
wandb: â­ï¸ View project at: https://wandb.ai/tringuyen/LLMDTA
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251112_164115-dq2g7qok/logs
Weights & Biases run finished

Training for fold 0 completed successfully.
Python script exit code: 0
==========================================
End Time: Wed Nov 12 08:14:24 PM AEDT 2025
==========================================
