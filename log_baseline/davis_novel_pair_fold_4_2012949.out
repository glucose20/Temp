==========================================
Job ID: 2012949
Array Task ID: 4
Node: v100l-f-03
Start Time: Wed Nov 12 04:40:39 PM AEDT 2025
==========================================
Activating conda environment...
Conda environment activated: LLMDTA
Checking GPU...
Wed Nov 12 16:40:40 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           On  |   00000000:06:00.0 Off |                    0 |
| N/A   33C    P0             43W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Setting environment variables...

Starting training for fold 4...


============================================================
Starting training for Fold 4
Dataset: davis, Running Set: novel-pair
Epochs: 200, Batch Size: 16
============================================================

Executing: python -u code/train.py --fold 4 --cuda 0 --dataset davis --running_set novel-pair --epochs 200 --batch_size 16 --wandb_project LLMDTA
============================================================
Training Fold 4/4
Dataset: davis-novel-pair
Device: cuda (CUDA_VISIBLE_DEVICES=0)
Pretrain-./data/davis/davis_drug_pretrain.pkl
Pretrain-./data/davis/davis_esm_pretrain.pkl
============================================================
wandb: Currently logged in as: tringuyen to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: setting up run gp1f2h6g
wandb: Tracking run with wandb version 0.23.0
wandb: Run data is saved locally in /vast/minhtrin/DTA/Temp/wandb/run-20251112_164047-gp1f2h6g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run davis-novel-pair-fold4
wandb: â­ï¸ View project at https://wandb.ai/tringuyen/LLMDTA
wandb: ğŸš€ View run at https://wandb.ai/tringuyen/LLMDTA/runs/gp1f2h6g
Weights & Biases initialized: LLMDTA
Loading fold 4 data...
  Train: ./data/dta-5fold-dataset/davis/novel-pair/fold_4_train.csv
  Valid: ./data/dta-5fold-dataset/davis/novel-pair/fold_4_valid.csv
  Test:  ./data/dta-5fold-dataset/davis/novel-pair/fold_4_test.csv
Dataset loaded: 10865 train, 14412 valid, 4779 test samples
/home/minhtrin/.conda/envs/LLMDTA/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Traing Log at fold-4 epoch-1: mse-2.391418, rmse-1.546421, r2--0.258312
Valid at fold-4: mse-2.44422
Update best_mse, Valid at fold-4 epoch-1: mse-2.44422, rmse-1.5634, ci--1, r2--2.076958, pearson-0.301027, spearman-0.296393
Traing Log at fold-4 epoch-2: mse-0.889347, rmse-0.943052, r2--0.374906
Valid at fold-4: mse-1.36612
Update best_mse, Valid at fold-4 epoch-2: mse-1.36612, rmse-1.168811, ci--1, r2--0.719769, pearson-0.379782, spearman-0.352487
Traing Log at fold-4 epoch-3: mse-0.718597, rmse-0.847701, r2--0.310067
Valid at fold-4: mse-1.897117
Traing Log at fold-4 epoch-4: mse-0.621987, rmse-0.788661, r2--0.219875
Valid at fold-4: mse-1.070243
Update best_mse, Valid at fold-4 epoch-4: mse-1.070243, rmse-1.034526, ci--1, r2--0.347298, pearson-0.454382, spearman-0.375545
Traing Log at fold-4 epoch-5: mse-0.534106, rmse-0.730826, r2--0.087975
Valid at fold-4: mse-1.098619
Traing Log at fold-4 epoch-6: mse-0.486517, rmse-0.697508, r2-0.010424
Valid at fold-4: mse-0.819978
Update best_mse, Valid at fold-4 epoch-6: mse-0.819978, rmse-0.905527, ci--1, r2--0.032247, pearson-0.487614, spearman-0.415814
Traing Log at fold-4 epoch-7: mse-0.440796, rmse-0.663925, r2-0.078367
Valid at fold-4: mse-0.667989
Update best_mse, Valid at fold-4 epoch-7: mse-0.667989, rmse-0.817306, ci--1, r2-0.159087, pearson-0.534156, spearman-0.435184
Traing Log at fold-4 epoch-8: mse-0.405935, rmse-0.637131, r2-0.147683
Valid at fold-4: mse-0.678398
Traing Log at fold-4 epoch-9: mse-0.381331, rmse-0.61752, r2-0.202253
Valid at fold-4: mse-0.656193
Update best_mse, Valid at fold-4 epoch-9: mse-0.656193, rmse-0.810057, ci--1, r2-0.173938, pearson-0.574839, spearman-0.480131
Traing Log at fold-4 epoch-10: mse-0.344717, rmse-0.587126, r2-0.295956
Valid at fold-4: mse-0.5681
Update best_mse, Valid at fold-4 epoch-10: mse-0.5681, rmse-0.753724, ci--1, r2-0.284835, pearson-0.561659, spearman-0.471264
Traing Log at fold-4 epoch-11: mse-0.335485, rmse-0.579211, r2-0.313748
Valid at fold-4: mse-0.540824
Update best_mse, Valid at fold-4 epoch-11: mse-0.540824, rmse-0.735407, ci--1, r2-0.319172, pearson-0.57668, spearman-0.482347
Traing Log at fold-4 epoch-12: mse-0.32569, rmse-0.570692, r2-0.331669
Valid at fold-4: mse-0.562588
Traing Log at fold-4 epoch-13: mse-0.297935, rmse-0.545835, r2-0.407537
Valid at fold-4: mse-0.509814
Update best_mse, Valid at fold-4 epoch-13: mse-0.509814, rmse-0.714012, ci--1, r2-0.35821, pearson-0.612715, spearman-0.520513
Traing Log at fold-4 epoch-14: mse-0.29017, rmse-0.538674, r2-0.427245
Valid at fold-4: mse-0.555522
Traing Log at fold-4 epoch-15: mse-0.275949, rmse-0.525309, r2-0.457696
Valid at fold-4: mse-0.528599
Traing Log at fold-4 epoch-16: mse-0.271429, rmse-0.520989, r2-0.475868
Valid at fold-4: mse-0.47787
Update best_mse, Valid at fold-4 epoch-16: mse-0.47787, rmse-0.691281, ci--1, r2-0.398423, pearson-0.638192, spearman-0.540155
Traing Log at fold-4 epoch-17: mse-0.258566, rmse-0.508494, r2-0.504972
Valid at fold-4: mse-0.526533
Traing Log at fold-4 epoch-18: mse-0.249014, rmse-0.499013, r2-0.526685
Valid at fold-4: mse-0.502417
Traing Log at fold-4 epoch-19: mse-0.234094, rmse-0.483833, r2-0.568066
Valid at fold-4: mse-0.490575
Traing Log at fold-4 epoch-20: mse-0.22725, rmse-0.476708, r2-0.582853
Valid at fold-4: mse-0.514094
Traing Log at fold-4 epoch-21: mse-0.221617, rmse-0.470762, r2-0.59885
Valid at fold-4: mse-0.464368
Update best_mse, Valid at fold-4 epoch-21: mse-0.464368, rmse-0.681446, ci--1, r2-0.41542, pearson-0.654106, spearman-0.552381
Traing Log at fold-4 epoch-22: mse-0.215343, rmse-0.46405, r2-0.609576
Valid at fold-4: mse-0.470517
Traing Log at fold-4 epoch-23: mse-0.205117, rmse-0.452898, r2-0.635175
Valid at fold-4: mse-0.484306
Traing Log at fold-4 epoch-24: mse-0.198674, rmse-0.445728, r2-0.649989
Valid at fold-4: mse-0.473634
Traing Log at fold-4 epoch-25: mse-0.196654, rmse-0.443457, r2-0.655398
Valid at fold-4: mse-0.469025
Traing Log at fold-4 epoch-26: mse-0.18581, rmse-0.431057, r2-0.680578
Valid at fold-4: mse-0.451156
Update best_mse, Valid at fold-4 epoch-26: mse-0.451156, rmse-0.671681, ci--1, r2-0.432053, pearson-0.660985, spearman-0.541326
Traing Log at fold-4 epoch-27: mse-0.180277, rmse-0.42459, r2-0.689825
Valid at fold-4: mse-0.44991
Update best_mse, Valid at fold-4 epoch-27: mse-0.44991, rmse-0.670754, ci--1, r2-0.433621, pearson-0.669903, spearman-0.552648
Traing Log at fold-4 epoch-28: mse-0.177746, rmse-0.421599, r2-0.700352
Valid at fold-4: mse-0.475288
Traing Log at fold-4 epoch-29: mse-0.171735, rmse-0.41441, r2-0.707895
Valid at fold-4: mse-0.446184
Update best_mse, Valid at fold-4 epoch-29: mse-0.446184, rmse-0.66797, ci--1, r2-0.438312, pearson-0.670301, spearman-0.549772
Traing Log at fold-4 epoch-30: mse-0.167949, rmse-0.409815, r2-0.72105
Valid at fold-4: mse-0.454602
Traing Log at fold-4 epoch-31: mse-0.161088, rmse-0.401358, r2-0.730164
Valid at fold-4: mse-0.456764
Traing Log at fold-4 epoch-32: mse-0.160755, rmse-0.400943, r2-0.733893
Valid at fold-4: mse-0.456597
Traing Log at fold-4 epoch-33: mse-0.157402, rmse-0.39674, r2-0.740168
Valid at fold-4: mse-0.450482
Traing Log at fold-4 epoch-34: mse-0.150565, rmse-0.388027, r2-0.754837
Valid at fold-4: mse-0.434554
Update best_mse, Valid at fold-4 epoch-34: mse-0.434554, rmse-0.659207, ci--1, r2-0.452952, pearson-0.683102, spearman-0.57148
Traing Log at fold-4 epoch-35: mse-0.149097, rmse-0.386131, r2-0.758659
Valid at fold-4: mse-0.431437
Update best_mse, Valid at fold-4 epoch-35: mse-0.431437, rmse-0.656838, ci--1, r2-0.456877, pearson-0.684131, spearman-0.55615
Traing Log at fold-4 epoch-36: mse-0.142522, rmse-0.37752, r2-0.769224
Valid at fold-4: mse-0.445409
Traing Log at fold-4 epoch-37: mse-0.143723, rmse-0.379109, r2-0.76851
Valid at fold-4: mse-0.424633
Update best_mse, Valid at fold-4 epoch-37: mse-0.424633, rmse-0.651638, ci--1, r2-0.465442, pearson-0.685845, spearman-0.554717
Traing Log at fold-4 epoch-38: mse-0.136402, rmse-0.369326, r2-0.783024
Valid at fold-4: mse-0.442358
Traing Log at fold-4 epoch-39: mse-0.135355, rmse-0.367907, r2-0.783333
Valid at fold-4: mse-0.436307
Traing Log at fold-4 epoch-40: mse-0.132119, rmse-0.363482, r2-0.790654
Valid at fold-4: mse-0.438345
Traing Log at fold-4 epoch-41: mse-0.126461, rmse-0.355614, r2-0.801065
Valid at fold-4: mse-0.438128
Traing Log at fold-4 epoch-42: mse-0.123803, rmse-0.351857, r2-0.806328
Valid at fold-4: mse-0.456813
Traing Log at fold-4 epoch-43: mse-0.119575, rmse-0.345796, r2-0.814029
Valid at fold-4: mse-0.427396
Traing Log at fold-4 epoch-44: mse-0.122981, rmse-0.350687, r2-0.806963
Valid at fold-4: mse-0.428773
Traing Log at fold-4 epoch-45: mse-0.121469, rmse-0.348524, r2-0.811218
Valid at fold-4: mse-0.431231
Traing Log at fold-4 epoch-46: mse-0.119772, rmse-0.346081, r2-0.815115
Valid at fold-4: mse-0.431824
Traing Log at fold-4 epoch-47: mse-0.116227, rmse-0.34092, r2-0.819556
Valid at fold-4: mse-0.438451
Traing Log at fold-4 epoch-48: mse-0.115008, rmse-0.339128, r2-0.823379
Valid at fold-4: mse-0.435454
Traing Log at fold-4 epoch-49: mse-0.115442, rmse-0.339767, r2-0.821529
Valid at fold-4: mse-0.432711
Traing Log at fold-4 epoch-50: mse-0.112419, rmse-0.335289, r2-0.827262
Valid at fold-4: mse-0.434563
Traing Log at fold-4 epoch-51: mse-0.107229, rmse-0.327458, r2-0.835473
Valid at fold-4: mse-0.425458
Traing Log at fold-4 epoch-52: mse-0.108, rmse-0.328633, r2-0.835902
Valid at fold-4: mse-0.423286
Update best_mse, Valid at fold-4 epoch-52: mse-0.423286, rmse-0.650604, ci--1, r2-0.467137, pearson-0.697018, spearman-0.588916
Traing Log at fold-4 epoch-53: mse-0.107687, rmse-0.328157, r2-0.835683
Valid at fold-4: mse-0.441126
Traing Log at fold-4 epoch-54: mse-0.106685, rmse-0.326626, r2-0.838427
Valid at fold-4: mse-0.434214
Traing Log at fold-4 epoch-55: mse-0.103494, rmse-0.321704, r2-0.842141
Valid at fold-4: mse-0.438201
Traing Log at fold-4 epoch-56: mse-0.097582, rmse-0.312381, r2-0.853395
Valid at fold-4: mse-0.409112
Update best_mse, Valid at fold-4 epoch-56: mse-0.409112, rmse-0.639618, ci--1, r2-0.484981, pearson-0.701329, spearman-0.57825
Traing Log at fold-4 epoch-57: mse-0.097274, rmse-0.311887, r2-0.853995
Valid at fold-4: mse-0.429124
Traing Log at fold-4 epoch-58: mse-0.100441, rmse-0.316925, r2-0.847549
Valid at fold-4: mse-0.423635
Traing Log at fold-4 epoch-59: mse-0.09581, rmse-0.309531, r2-0.856239
Valid at fold-4: mse-0.431226
Traing Log at fold-4 epoch-60: mse-0.097545, rmse-0.312321, r2-0.854454
Valid at fold-4: mse-0.414564
Traing Log at fold-4 epoch-61: mse-0.091728, rmse-0.302866, r2-0.863062
Valid at fold-4: mse-0.419774
Traing Log at fold-4 epoch-62: mse-0.097972, rmse-0.313005, r2-0.852435
Valid at fold-4: mse-0.435855
Traing Log at fold-4 epoch-63: mse-0.09385, rmse-0.306349, r2-0.860526
Valid at fold-4: mse-0.43136
Traing Log at fold-4 epoch-64: mse-0.090295, rmse-0.300491, r2-0.865406
Valid at fold-4: mse-0.438327
Traing Log at fold-4 epoch-65: mse-0.090288, rmse-0.300479, r2-0.865861
Valid at fold-4: mse-0.418383
Traing Log at fold-4 epoch-66: mse-0.087709, rmse-0.296158, r2-0.86988
Valid at fold-4: mse-0.41665
Traing Log at fold-4 epoch-67: mse-0.08758, rmse-0.295939, r2-0.870532
Valid at fold-4: mse-0.412115
Traing Log at fold-4 epoch-68: mse-0.084498, rmse-0.290685, r2-0.875049
Valid at fold-4: mse-0.432046
Traing Log at fold-4 epoch-69: mse-0.085742, rmse-0.292818, r2-0.873248
Valid at fold-4: mse-0.418359
Traing Log at fold-4 epoch-70: mse-0.086715, rmse-0.294475, r2-0.871882
Valid at fold-4: mse-0.400157
Update best_mse, Valid at fold-4 epoch-70: mse-0.400157, rmse-0.63258, ci--1, r2-0.496254, pearson-0.71108, spearman-0.588664
Traing Log at fold-4 epoch-71: mse-0.082504, rmse-0.287236, r2-0.878692
Valid at fold-4: mse-0.407908
Traing Log at fold-4 epoch-72: mse-0.079797, rmse-0.282484, r2-0.882529
Valid at fold-4: mse-0.417684
Traing Log at fold-4 epoch-73: mse-0.083248, rmse-0.288527, r2-0.877754
Valid at fold-4: mse-0.416652
Traing Log at fold-4 epoch-74: mse-0.079194, rmse-0.281414, r2-0.883965
Valid at fold-4: mse-0.413498
Traing Log at fold-4 epoch-75: mse-0.078917, rmse-0.280921, r2-0.884766
Valid at fold-4: mse-0.411222
Traing Log at fold-4 epoch-76: mse-0.080538, rmse-0.283791, r2-0.881747
Valid at fold-4: mse-0.404421
Traing Log at fold-4 epoch-77: mse-0.077034, rmse-0.277551, r2-0.887189
Valid at fold-4: mse-0.413127
Traing Log at fold-4 epoch-78: mse-0.07669, rmse-0.27693, r2-0.888112
Valid at fold-4: mse-0.428084
Traing Log at fold-4 epoch-79: mse-0.07615, rmse-0.275953, r2-0.889265
Valid at fold-4: mse-0.418192
Traing Log at fold-4 epoch-80: mse-0.077183, rmse-0.277819, r2-0.887457
Valid at fold-4: mse-0.405235
Traing Log at fold-4 epoch-81: mse-0.07669, rmse-0.27693, r2-0.888508
Valid at fold-4: mse-0.404037
Traing Log at fold-4 epoch-82: mse-0.074946, rmse-0.273763, r2-0.890626
Valid at fold-4: mse-0.399419
Update best_mse, Valid at fold-4 epoch-82: mse-0.399419, rmse-0.631996, ci--1, r2-0.497183, pearson-0.715896, spearman-0.599666
Traing Log at fold-4 epoch-83: mse-0.073182, rmse-0.270521, r2-0.89367
Valid at fold-4: mse-0.41432
Traing Log at fold-4 epoch-84: mse-0.073055, rmse-0.270288, r2-0.894393
Valid at fold-4: mse-0.400939
Traing Log at fold-4 epoch-85: mse-0.070807, rmse-0.266095, r2-0.897149
Valid at fold-4: mse-0.40972
Traing Log at fold-4 epoch-86: mse-0.068434, rmse-0.2616, r2-0.901466
Valid at fold-4: mse-0.422009
Traing Log at fold-4 epoch-87: mse-0.072731, rmse-0.269686, r2-0.894573
Valid at fold-4: mse-0.405435
Traing Log at fold-4 epoch-88: mse-0.070569, rmse-0.265649, r2-0.897779
Valid at fold-4: mse-0.422309
Traing Log at fold-4 epoch-89: mse-0.070857, rmse-0.26619, r2-0.897315
Valid at fold-4: mse-0.408056
Traing Log at fold-4 epoch-90: mse-0.068098, rmse-0.260956, r2-0.902076
Valid at fold-4: mse-0.41586
Traing Log at fold-4 epoch-91: mse-0.068572, rmse-0.261862, r2-0.901501
Valid at fold-4: mse-0.421542
Traing Log at fold-4 epoch-92: mse-0.065703, rmse-0.256326, r2-0.905418
Valid at fold-4: mse-0.40291
Traing Log at fold-4 epoch-93: mse-0.068657, rmse-0.262025, r2-0.900794
Valid at fold-4: mse-0.410572
Traing Log at fold-4 epoch-94: mse-0.065406, rmse-0.255746, r2-0.906004
Valid at fold-4: mse-0.407817
Traing Log at fold-4 epoch-95: mse-0.067653, rmse-0.260103, r2-0.901963
Valid at fold-4: mse-0.405865
Traing Log at fold-4 epoch-96: mse-0.065585, rmse-0.256095, r2-0.906312
Valid at fold-4: mse-0.407601
Traing Log at fold-4 epoch-97: mse-0.066498, rmse-0.257872, r2-0.904639
Valid at fold-4: mse-0.401943
Traing Log at fold-4 epoch-98: mse-0.062678, rmse-0.250356, r2-0.910312
Valid at fold-4: mse-0.412696
Traing Log at fold-4 epoch-99: mse-0.065207, rmse-0.255357, r2-0.906301
Valid at fold-4: mse-0.402531
Traing Log at fold-4 epoch-100: mse-0.062952, rmse-0.250902, r2-0.909519
Valid at fold-4: mse-0.432665
Traing Log at fold-4 epoch-101: mse-0.063334, rmse-0.251662, r2-0.909872
Valid at fold-4: mse-0.407046
Traing Log at fold-4 epoch-102: mse-0.062991, rmse-0.250981, r2-0.909769
Valid at fold-4: mse-0.399432
Traing Log at fold-4 epoch-103: mse-0.061756, rmse-0.248508, r2-0.911597
Valid at fold-4: mse-0.407261
Traing stop at epoch-103, model save at-./savemodel/davis-novel-pair-fold4-Nov12_16-40-47.pth
Save log over at ./log/Nov12_16-40-47-davis-novel-pair-fold4.csv

============================================================
Testing fold 4 with best model...
============================================================
/home/minhtrin/.conda/envs/LLMDTA/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Test at fold-4, mse: 0.770575, rmse: 0.877824, ci: 0.655988, r2: 0.084743, pearson: 0.365625, spearman: 0.299794

Fold 4 results saved to: ./log/Test-davis-novel-pair-fold4-Nov12_16-40-47.csv
============================================================
Training fold 4 completed successfully!
============================================================
wandb: updating run metadata
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading history steps 227-227, summary, console lines 243-248
wandb: 
wandb: Run history:
wandb:      best_valid/mse â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  best_valid/pearson â–â–‚â–„â–„â–…â–†â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:       best_valid/r2 â–â–…â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:     best_valid/rmse â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb: best_valid/spearman â–â–‚â–ƒâ–„â–„â–…â–…â–…â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:               epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             test/ci â–
wandb:            test/mse â–
wandb:        test/pearson â–
wandb:             test/r2 â–
wandb:                 +13 ...
wandb: 
wandb: Run summary:
wandb:      best_valid/mse 0.39942
wandb:  best_valid/pearson 0.7159
wandb:       best_valid/r2 0.49718
wandb:     best_valid/rmse 0.632
wandb: best_valid/spearman 0.59967
wandb:               epoch 103
wandb:       final_test_ci 0.65599
wandb:      final_test_mse 0.77058
wandb:  final_test_pearson 0.36562
wandb:       final_test_r2 0.08474
wandb:                 +19 ...
wandb: 
wandb: ğŸš€ View run davis-novel-pair-fold4 at: https://wandb.ai/tringuyen/LLMDTA/runs/gp1f2h6g
wandb: â­ï¸ View project at: https://wandb.ai/tringuyen/LLMDTA
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251112_164047-gp1f2h6g/logs
Weights & Biases run finished

Training for fold 4 completed successfully.
Python script exit code: 0
==========================================
End Time: Wed Nov 12 08:58:15 PM AEDT 2025
==========================================
