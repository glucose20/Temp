==========================================
Job ID: 2012961
Array Task ID: 0
Node: v100l-f-06
Start Time: Wed Nov 12 04:41:39 PM AEDT 2025
==========================================
Activating conda environment...
Conda environment activated: LLMDTA
Checking GPU...
Wed Nov 12 16:41:40 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           On  |   00000000:06:00.0 Off |                    0 |
| N/A   32C    P0             43W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Setting environment variables...

Starting training for fold 0...


============================================================
Starting training for Fold 0
Dataset: davis, Running Set: warm
Epochs: 200, Batch Size: 16
============================================================

Executing: python -u code/train.py --fold 0 --cuda 0 --dataset davis --running_set warm --epochs 200 --batch_size 16 --wandb_project LLMDTA
============================================================
Training Fold 0/4
Dataset: davis-warm
Device: cuda (CUDA_VISIBLE_DEVICES=0)
Pretrain-./data/davis/davis_drug_pretrain.pkl
Pretrain-./data/davis/davis_esm_pretrain.pkl
============================================================
wandb: Currently logged in as: tringuyen to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: setting up run nckibc5y
wandb: Tracking run with wandb version 0.23.0
wandb: Run data is saved locally in /vast/minhtrin/DTA/Temp/wandb/run-20251112_164147-nckibc5y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run davis-warm-fold0
wandb: â­ï¸ View project at https://wandb.ai/tringuyen/LLMDTA
wandb: ğŸš€ View run at https://wandb.ai/tringuyen/LLMDTA/runs/nckibc5y
Weights & Biases initialized: LLMDTA
Loading fold 0 data...
  Train: ./data/dta-5fold-dataset/davis/warm/fold_0_train.csv
  Valid: ./data/dta-5fold-dataset/davis/warm/fold_0_valid.csv
  Test:  ./data/dta-5fold-dataset/davis/warm/fold_0_test.csv
Dataset loaded: 19236 train, 4809 valid, 6011 test samples
/home/minhtrin/.conda/envs/LLMDTA/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Traing Log at fold-0 epoch-1: mse-1.820789, rmse-1.349366, r2--0.236778
Valid at fold-0: mse-0.901132
Update best_mse, Valid at fold-0 epoch-1: mse-0.901132, rmse-0.949279, ci--1, r2--0.141324, pearson-0.517064, spearman-0.48458
Traing Log at fold-0 epoch-2: mse-0.767007, rmse-0.875789, r2--0.274336
Valid at fold-0: mse-1.365137
Traing Log at fold-0 epoch-3: mse-0.598291, rmse-0.773492, r2--0.166379
Valid at fold-0: mse-1.17968
Traing Log at fold-0 epoch-4: mse-0.531506, rmse-0.729044, r2--0.053995
Valid at fold-0: mse-0.773238
Update best_mse, Valid at fold-0 epoch-4: mse-0.773238, rmse-0.87934, ci--1, r2-0.020659, pearson-0.670947, spearman-0.554231
Traing Log at fold-0 epoch-5: mse-0.46551, rmse-0.682283, r2-0.044903
Valid at fold-0: mse-0.528215
Update best_mse, Valid at fold-0 epoch-5: mse-0.528215, rmse-0.726784, ci--1, r2-0.330992, pearson-0.701821, spearman-0.562316
Traing Log at fold-0 epoch-6: mse-0.423202, rmse-0.65054, r2-0.124209
Valid at fold-0: mse-0.398494
Update best_mse, Valid at fold-0 epoch-6: mse-0.398494, rmse-0.631264, ci--1, r2-0.495289, pearson-0.708775, spearman-0.559863
Traing Log at fold-0 epoch-7: mse-0.394843, rmse-0.628365, r2-0.176281
Valid at fold-0: mse-0.432459
Traing Log at fold-0 epoch-8: mse-0.366511, rmse-0.605401, r2-0.250209
Valid at fold-0: mse-0.333886
Update best_mse, Valid at fold-0 epoch-8: mse-0.333886, rmse-0.577829, ci--1, r2-0.577118, pearson-0.761215, spearman-0.617734
Traing Log at fold-0 epoch-9: mse-0.340264, rmse-0.583322, r2-0.313747
Valid at fold-0: mse-0.328678
Update best_mse, Valid at fold-0 epoch-9: mse-0.328678, rmse-0.573304, ci--1, r2-0.583715, pearson-0.771073, spearman-0.627332
Traing Log at fold-0 epoch-10: mse-0.322667, rmse-0.568037, r2-0.367638
Valid at fold-0: mse-0.324405
Update best_mse, Valid at fold-0 epoch-10: mse-0.324405, rmse-0.569565, ci--1, r2-0.589127, pearson-0.769126, spearman-0.638893
Traing Log at fold-0 epoch-11: mse-0.309208, rmse-0.556065, r2-0.406296
Valid at fold-0: mse-0.30274
Update best_mse, Valid at fold-0 epoch-11: mse-0.30274, rmse-0.550219, ci--1, r2-0.616566, pearson-0.788146, spearman-0.642575
Traing Log at fold-0 epoch-12: mse-0.292115, rmse-0.540477, r2-0.451396
Valid at fold-0: mse-0.29841
Update best_mse, Valid at fold-0 epoch-12: mse-0.29841, rmse-0.546269, ci--1, r2-0.62205, pearson-0.794601, spearman-0.654984
Traing Log at fold-0 epoch-13: mse-0.279534, rmse-0.528709, r2-0.485491
Valid at fold-0: mse-0.283729
Update best_mse, Valid at fold-0 epoch-13: mse-0.283729, rmse-0.532662, ci--1, r2-0.640645, pearson-0.800472, spearman-0.645491
Traing Log at fold-0 epoch-14: mse-0.267686, rmse-0.517384, r2-0.51967
Valid at fold-0: mse-0.293609
Traing Log at fold-0 epoch-15: mse-0.25511, rmse-0.505084, r2-0.545175
Valid at fold-0: mse-0.284568
Traing Log at fold-0 epoch-16: mse-0.247423, rmse-0.497416, r2-0.570844
Valid at fold-0: mse-0.265669
Update best_mse, Valid at fold-0 epoch-16: mse-0.265669, rmse-0.515431, ci--1, r2-0.663518, pearson-0.815675, spearman-0.660936
Traing Log at fold-0 epoch-17: mse-0.239637, rmse-0.489528, r2-0.584529
Valid at fold-0: mse-0.27315
Traing Log at fold-0 epoch-18: mse-0.23104, rmse-0.480667, r2-0.606889
Valid at fold-0: mse-0.261723
Update best_mse, Valid at fold-0 epoch-18: mse-0.261723, rmse-0.511589, ci--1, r2-0.668516, pearson-0.818704, spearman-0.665149
Traing Log at fold-0 epoch-19: mse-0.221995, rmse-0.471164, r2-0.62707
Valid at fold-0: mse-0.246689
Update best_mse, Valid at fold-0 epoch-19: mse-0.246689, rmse-0.496678, ci--1, r2-0.687557, pearson-0.831435, spearman-0.667867
Traing Log at fold-0 epoch-20: mse-0.221045, rmse-0.470154, r2-0.629877
Valid at fold-0: mse-0.266087
Traing Log at fold-0 epoch-21: mse-0.216161, rmse-0.464931, r2-0.640733
Valid at fold-0: mse-0.256938
Traing Log at fold-0 epoch-22: mse-0.203074, rmse-0.450637, r2-0.668407
Valid at fold-0: mse-0.255104
Traing Log at fold-0 epoch-23: mse-0.200853, rmse-0.448167, r2-0.673396
Valid at fold-0: mse-0.269073
Traing Log at fold-0 epoch-24: mse-0.194472, rmse-0.44099, r2-0.687242
Valid at fold-0: mse-0.255232
Traing Log at fold-0 epoch-25: mse-0.191335, rmse-0.437418, r2-0.690809
Valid at fold-0: mse-0.247652
Traing Log at fold-0 epoch-26: mse-0.184167, rmse-0.429147, r2-0.708564
Valid at fold-0: mse-0.245539
Update best_mse, Valid at fold-0 epoch-26: mse-0.245539, rmse-0.495519, ci--1, r2-0.689014, pearson-0.833946, spearman-0.683172
Traing Log at fold-0 epoch-27: mse-0.177567, rmse-0.421387, r2-0.72273
Valid at fold-0: mse-0.246007
Traing Log at fold-0 epoch-28: mse-0.176766, rmse-0.420436, r2-0.723348
Valid at fold-0: mse-0.264524
Traing Log at fold-0 epoch-29: mse-0.171029, rmse-0.413557, r2-0.734036
Valid at fold-0: mse-0.236791
Update best_mse, Valid at fold-0 epoch-29: mse-0.236791, rmse-0.486612, ci--1, r2-0.700093, pearson-0.841485, spearman-0.67635
Traing Log at fold-0 epoch-30: mse-0.170457, rmse-0.412865, r2-0.736266
Valid at fold-0: mse-0.243751
Traing Log at fold-0 epoch-31: mse-0.164505, rmse-0.405592, r2-0.747181
Valid at fold-0: mse-0.254629
Traing Log at fold-0 epoch-32: mse-0.160206, rmse-0.400257, r2-0.755397
Valid at fold-0: mse-0.246335
Traing Log at fold-0 epoch-33: mse-0.160794, rmse-0.400991, r2-0.754242
Valid at fold-0: mse-0.240995
Traing Log at fold-0 epoch-34: mse-0.156945, rmse-0.396163, r2-0.76229
Valid at fold-0: mse-0.242525
Traing Log at fold-0 epoch-35: mse-0.155121, rmse-0.393854, r2-0.765188
Valid at fold-0: mse-0.229245
Update best_mse, Valid at fold-0 epoch-35: mse-0.229245, rmse-0.478795, ci--1, r2-0.709651, pearson-0.843448, spearman-0.669848
Traing Log at fold-0 epoch-36: mse-0.14961, rmse-0.386794, r2-0.775166
Valid at fold-0: mse-0.235483
Traing Log at fold-0 epoch-37: mse-0.145963, rmse-0.382051, r2-0.781803
Valid at fold-0: mse-0.24192
Traing Log at fold-0 epoch-38: mse-0.145591, rmse-0.381564, r2-0.782231
Valid at fold-0: mse-0.239711
Traing Log at fold-0 epoch-39: mse-0.143883, rmse-0.379319, r2-0.786039
Valid at fold-0: mse-0.230357
Traing Log at fold-0 epoch-40: mse-0.13969, rmse-0.373751, r2-0.793284
Valid at fold-0: mse-0.230048
Traing Log at fold-0 epoch-41: mse-0.137041, rmse-0.37019, r2-0.797826
Valid at fold-0: mse-0.233655
Traing Log at fold-0 epoch-42: mse-0.137084, rmse-0.370249, r2-0.798339
Valid at fold-0: mse-0.227676
Update best_mse, Valid at fold-0 epoch-42: mse-0.227676, rmse-0.477154, ci--1, r2-0.711638, pearson-0.848572, spearman-0.676646
Traing Log at fold-0 epoch-43: mse-0.132855, rmse-0.364493, r2-0.804151
Valid at fold-0: mse-0.226985
Update best_mse, Valid at fold-0 epoch-43: mse-0.226985, rmse-0.476429, ci--1, r2-0.712514, pearson-0.850826, spearman-0.68961
Traing Log at fold-0 epoch-44: mse-0.13297, rmse-0.364651, r2-0.806179
Valid at fold-0: mse-0.228375
Traing Log at fold-0 epoch-45: mse-0.12863, rmse-0.358651, r2-0.812077
Valid at fold-0: mse-0.235193
Traing Log at fold-0 epoch-46: mse-0.129625, rmse-0.360035, r2-0.812113
Valid at fold-0: mse-0.234172
Traing Log at fold-0 epoch-47: mse-0.123828, rmse-0.351892, r2-0.820106
Valid at fold-0: mse-0.238845
Traing Log at fold-0 epoch-48: mse-0.127216, rmse-0.356673, r2-0.815224
Valid at fold-0: mse-0.235968
Traing Log at fold-0 epoch-49: mse-0.121499, rmse-0.348568, r2-0.825162
Valid at fold-0: mse-0.230233
Traing Log at fold-0 epoch-50: mse-0.120176, rmse-0.346664, r2-0.826276
Valid at fold-0: mse-0.230691
Traing Log at fold-0 epoch-51: mse-0.120015, rmse-0.346433, r2-0.826984
Valid at fold-0: mse-0.236271
Traing Log at fold-0 epoch-52: mse-0.117648, rmse-0.342998, r2-0.832646
Valid at fold-0: mse-0.227966
Traing Log at fold-0 epoch-53: mse-0.117161, rmse-0.342288, r2-0.83198
Valid at fold-0: mse-0.226439
Update best_mse, Valid at fold-0 epoch-53: mse-0.226439, rmse-0.475856, ci--1, r2-0.713205, pearson-0.847579, spearman-0.675919
Traing Log at fold-0 epoch-54: mse-0.116445, rmse-0.341241, r2-0.8329
Valid at fold-0: mse-0.227812
Traing Log at fold-0 epoch-55: mse-0.113786, rmse-0.337321, r2-0.837541
Valid at fold-0: mse-0.230449
Traing Log at fold-0 epoch-56: mse-0.111, rmse-0.333166, r2-0.843054
Valid at fold-0: mse-0.22977
Traing Log at fold-0 epoch-57: mse-0.113619, rmse-0.337074, r2-0.837676
Valid at fold-0: mse-0.234599
Traing Log at fold-0 epoch-58: mse-0.110498, rmse-0.332412, r2-0.843081
Valid at fold-0: mse-0.226743
Traing Log at fold-0 epoch-59: mse-0.108024, rmse-0.328671, r2-0.847225
Valid at fold-0: mse-0.224617
Update best_mse, Valid at fold-0 epoch-59: mse-0.224617, rmse-0.473938, ci--1, r2-0.715512, pearson-0.848673, spearman-0.680845
Traing Log at fold-0 epoch-60: mse-0.109595, rmse-0.331051, r2-0.84467
Valid at fold-0: mse-0.233968
Traing Log at fold-0 epoch-61: mse-0.105314, rmse-0.324522, r2-0.851821
Valid at fold-0: mse-0.232777
Traing Log at fold-0 epoch-62: mse-0.107259, rmse-0.327503, r2-0.848327
Valid at fold-0: mse-0.228217
Traing Log at fold-0 epoch-63: mse-0.105236, rmse-0.324401, r2-0.851619
Valid at fold-0: mse-0.228054
Traing Log at fold-0 epoch-64: mse-0.103523, rmse-0.32175, r2-0.854314
Valid at fold-0: mse-0.228949
Traing Log at fold-0 epoch-65: mse-0.102675, rmse-0.32043, r2-0.855719
Valid at fold-0: mse-0.230967
Traing Log at fold-0 epoch-66: mse-0.101778, rmse-0.319026, r2-0.857413
Valid at fold-0: mse-0.224786
Traing Log at fold-0 epoch-67: mse-0.099254, rmse-0.315046, r2-0.860865
Valid at fold-0: mse-0.226587
Traing Log at fold-0 epoch-68: mse-0.101041, rmse-0.317869, r2-0.859318
Valid at fold-0: mse-0.234917
Traing Log at fold-0 epoch-69: mse-0.099102, rmse-0.314804, r2-0.861343
Valid at fold-0: mse-0.220081
Update best_mse, Valid at fold-0 epoch-69: mse-0.220081, rmse-0.469128, ci--1, r2-0.721258, pearson-0.852694, spearman-0.679697
Traing Log at fold-0 epoch-70: mse-0.099055, rmse-0.31473, r2-0.861753
Valid at fold-0: mse-0.219638
Update best_mse, Valid at fold-0 epoch-70: mse-0.219638, rmse-0.468656, ci--1, r2-0.721818, pearson-0.851227, spearman-0.678372
Traing Log at fold-0 epoch-71: mse-0.09976, rmse-0.315848, r2-0.860754
Valid at fold-0: mse-0.219646
Traing Log at fold-0 epoch-72: mse-0.095135, rmse-0.308439, r2-0.867672
Valid at fold-0: mse-0.220208
Traing Log at fold-0 epoch-73: mse-0.093694, rmse-0.306095, r2-0.87013
Valid at fold-0: mse-0.226459
Traing Log at fold-0 epoch-74: mse-0.094522, rmse-0.307444, r2-0.868546
Valid at fold-0: mse-0.220345
Traing Log at fold-0 epoch-75: mse-0.09332, rmse-0.305483, r2-0.870613
Valid at fold-0: mse-0.228792
Traing Log at fold-0 epoch-76: mse-0.093422, rmse-0.30565, r2-0.869987
Valid at fold-0: mse-0.222186
Traing Log at fold-0 epoch-77: mse-0.092452, rmse-0.30406, r2-0.872689
Valid at fold-0: mse-0.219008
Update best_mse, Valid at fold-0 epoch-77: mse-0.219008, rmse-0.467983, ci--1, r2-0.722616, pearson-0.853052, spearman-0.682154
Traing Log at fold-0 epoch-78: mse-0.090966, rmse-0.301606, r2-0.874231
Valid at fold-0: mse-0.244154
Traing Log at fold-0 epoch-79: mse-0.090328, rmse-0.300545, r2-0.875385
Valid at fold-0: mse-0.229139
Traing Log at fold-0 epoch-80: mse-0.091951, rmse-0.303235, r2-0.872995
Valid at fold-0: mse-0.229677
Traing Log at fold-0 epoch-81: mse-0.088603, rmse-0.297663, r2-0.87747
Valid at fold-0: mse-0.215516
Update best_mse, Valid at fold-0 epoch-81: mse-0.215516, rmse-0.464237, ci--1, r2-0.72704, pearson-0.855603, spearman-0.680606
Traing Log at fold-0 epoch-82: mse-0.089351, rmse-0.298917, r2-0.877411
Valid at fold-0: mse-0.22504
Traing Log at fold-0 epoch-83: mse-0.087398, rmse-0.295631, r2-0.879315
Valid at fold-0: mse-0.215618
Traing Log at fold-0 epoch-84: mse-0.08819, rmse-0.296968, r2-0.878449
Valid at fold-0: mse-0.216948
Traing Log at fold-0 epoch-85: mse-0.087759, rmse-0.296242, r2-0.879669
Valid at fold-0: mse-0.222594
Traing Log at fold-0 epoch-86: mse-0.085181, rmse-0.291858, r2-0.883286
Valid at fold-0: mse-0.219389
Traing Log at fold-0 epoch-87: mse-0.083785, rmse-0.289457, r2-0.885162
Valid at fold-0: mse-0.23388
Traing Log at fold-0 epoch-88: mse-0.084719, rmse-0.291065, r2-0.88417
Valid at fold-0: mse-0.214373
Update best_mse, Valid at fold-0 epoch-88: mse-0.214373, rmse-0.463004, ci--1, r2-0.728487, pearson-0.854158, spearman-0.681216
Traing Log at fold-0 epoch-89: mse-0.085683, rmse-0.292717, r2-0.88231
Valid at fold-0: mse-0.224557
Traing Log at fold-0 epoch-90: mse-0.083882, rmse-0.289625, r2-0.885267
Valid at fold-0: mse-0.226567
Traing Log at fold-0 epoch-91: mse-0.082232, rmse-0.286761, r2-0.888057
Valid at fold-0: mse-0.229888
Traing Log at fold-0 epoch-92: mse-0.082505, rmse-0.287238, r2-0.886938
Valid at fold-0: mse-0.232522
Traing Log at fold-0 epoch-93: mse-0.081998, rmse-0.286353, r2-0.88779
Valid at fold-0: mse-0.215154
Traing Log at fold-0 epoch-94: mse-0.08253, rmse-0.28728, r2-0.88777
Valid at fold-0: mse-0.237902
Traing Log at fold-0 epoch-95: mse-0.082939, rmse-0.287992, r2-0.886498
Valid at fold-0: mse-0.220402
Traing Log at fold-0 epoch-96: mse-0.078317, rmse-0.279851, r2-0.893448
Valid at fold-0: mse-0.224586
Traing Log at fold-0 epoch-97: mse-0.081545, rmse-0.28556, r2-0.888625
Valid at fold-0: mse-0.223236
Traing Log at fold-0 epoch-98: mse-0.078048, rmse-0.279371, r2-0.893966
Valid at fold-0: mse-0.222323
Traing Log at fold-0 epoch-99: mse-0.080041, rmse-0.282914, r2-0.891644
Valid at fold-0: mse-0.228838
Traing Log at fold-0 epoch-100: mse-0.077965, rmse-0.279222, r2-0.893799
Valid at fold-0: mse-0.219641
Traing Log at fold-0 epoch-101: mse-0.079078, rmse-0.281208, r2-0.892646
Valid at fold-0: mse-0.219445
Traing Log at fold-0 epoch-102: mse-0.077242, rmse-0.277925, r2-0.895283
Valid at fold-0: mse-0.233469
Traing Log at fold-0 epoch-103: mse-0.079326, rmse-0.281648, r2-0.891813
Valid at fold-0: mse-0.226369
Traing Log at fold-0 epoch-104: mse-0.075606, rmse-0.274965, r2-0.8971
Valid at fold-0: mse-0.225322
Traing Log at fold-0 epoch-105: mse-0.077708, rmse-0.278762, r2-0.895051
Valid at fold-0: mse-0.223398
Traing Log at fold-0 epoch-106: mse-0.077474, rmse-0.278341, r2-0.895024
Valid at fold-0: mse-0.22823
Traing Log at fold-0 epoch-107: mse-0.076105, rmse-0.275871, r2-0.896647
Valid at fold-0: mse-0.225147
Traing Log at fold-0 epoch-108: mse-0.075469, rmse-0.274716, r2-0.89803
Valid at fold-0: mse-0.224536
Traing Log at fold-0 epoch-109: mse-0.074511, rmse-0.272968, r2-0.899096
Valid at fold-0: mse-0.217202
Traing stop at epoch-109, model save at-./savemodel/davis-warm-fold0-Nov12_16-41-46.pth
Save log over at ./log/Nov12_16-41-46-davis-warm-fold0.csv

============================================================
Testing fold 0 with best model...
============================================================
/home/minhtrin/.conda/envs/LLMDTA/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Test at fold-0, mse: 0.21014, rmse: 0.45841, ci: 0.895584, r2: 0.730677, pearson: 0.855819, spearman: 0.689445

Fold 0 results saved to: ./log/Test-davis-warm-fold0-Nov12_16-41-46.csv
============================================================
Training fold 0 completed successfully!
============================================================
wandb: updating run metadata
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading output.log; uploading config.yaml
wandb: uploading history steps 243-243, summary, console lines 259-264
wandb: 
wandb: Run history:
wandb:      best_valid/mse â–ˆâ–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  best_valid/pearson â–â–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:       best_valid/r2 â–â–‚â–…â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:     best_valid/rmse â–ˆâ–‡â–…â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: best_valid/spearman â–â–ƒâ–„â–„â–†â–†â–†â–†â–‡â–†â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:             test/ci â–
wandb:            test/mse â–
wandb:        test/pearson â–
wandb:             test/r2 â–
wandb:                 +13 ...
wandb: 
wandb: Run summary:
wandb:      best_valid/mse 0.21437
wandb:  best_valid/pearson 0.85416
wandb:       best_valid/r2 0.72849
wandb:     best_valid/rmse 0.463
wandb: best_valid/spearman 0.68122
wandb:               epoch 109
wandb:       final_test_ci 0.89558
wandb:      final_test_mse 0.21014
wandb:  final_test_pearson 0.85582
wandb:       final_test_r2 0.73068
wandb:                 +19 ...
wandb: 
wandb: ğŸš€ View run davis-warm-fold0 at: https://wandb.ai/tringuyen/LLMDTA/runs/nckibc5y
wandb: â­ï¸ View project at: https://wandb.ai/tringuyen/LLMDTA
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251112_164147-nckibc5y/logs
Weights & Biases run finished

Training for fold 0 completed successfully.
Python script exit code: 0
==========================================
End Time: Wed Nov 12 09:11:11 PM AEDT 2025
==========================================
